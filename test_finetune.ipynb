{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-13 19:11:14.780845: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-13 19:11:14.780954: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-13 19:11:14.824436: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-13 19:11:14.917050: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-13 19:11:16.208486: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "sys.path.append('.')\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "import draccus\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import tqdm\n",
    "import wandb\n",
    "from accelerate import PartialState\n",
    "from peft import LoraConfig, PeftModel, get_peft_model, prepare_model_for_kbit_training\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModelForVision2Seq, AutoProcessor, BitsAndBytesConfig\n",
    "from transformers.modeling_outputs import CausalLMOutputWithPast\n",
    "\n",
    "from finetune.finetune_config import FinetuneConfig\n",
    "from vla.base_prompter import PurePromptBuilder\n",
    "from vla.utils import PaddedCollatorForActionPrediction\n",
    "from vla.action_tokenizer import ActionTokenizer, RLbenchActionTokenizer\n",
    "from vla.dataset import save_dataset_statistics, RLbenchDataset\n",
    "import numpy as np\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = FinetuneConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning OpenVLA Model `/media/lawrence/Work/checkpoints/openvla-7b` on `test1`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "/home/lawrence/anaconda3/lib/python3.11/site-packages/transformers/quantizers/auto.py:159: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n",
      "  warnings.warn(warning_msg)\n",
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Fine-tuning OpenVLA Model `{cfg.vla_path}` on `{cfg.dataset_name}`\")\n",
    "\n",
    "# [Validate] Ensure GPU Available & Set Device / Distributed Context\n",
    "assert torch.cuda.is_available(), \"Fine-tuning assumes at least one GPU is available!\"\n",
    "distributed_state = PartialState()\n",
    "torch.cuda.set_device(device_id := distributed_state.local_process_index)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Configure Unique Experiment ID & Log Directory\n",
    "exp_id = (\n",
    "    f\"{cfg.vla_path.split('/')[-1]}+{cfg.dataset_name}\"\n",
    "    f\"+b{cfg.batch_size * cfg.grad_accumulation_steps}\"\n",
    "    f\"+lr-{cfg.learning_rate}\"\n",
    ")\n",
    "if cfg.use_lora:\n",
    "    exp_id += f\"+lora-r{cfg.lora_rank}+dropout-{cfg.lora_dropout}\"\n",
    "if cfg.use_quantization:\n",
    "    exp_id += \"+q-4bit\"\n",
    "\n",
    "# Start =>> Build Directories\n",
    "run_dir, adapter_dir = cfg.run_root_dir / exp_id, cfg.adapter_dir / exp_id\n",
    "os.makedirs(run_dir, exist_ok=True)\n",
    "\n",
    "# Quantization Config =>> only if LoRA fine-tuning\n",
    "quantization_config = None\n",
    "if cfg.use_quantization:\n",
    "    assert cfg.use_lora, \"Quantized training only supported for LoRA fine-tuning!\"\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True, bnb_4bit_compute_dtype=torch.bfloat16, bnb_4bit_quant_type=\"nf4\", #llm_int8_skip_modules = ['projector'],\n",
    "    )\n",
    "\n",
    "# Load OpenVLA Processor and Model using HF AutoClasses\n",
    "processor = AutoProcessor.from_pretrained(cfg.vla_path, trust_remote_code=True)\n",
    "vla = AutoModelForVision2Seq.from_pretrained(\n",
    "    cfg.vla_path_q,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    attn_implementation=\"sdpa\",\n",
    "    quantization_config=quantization_config,\n",
    "    low_cpu_mem_usage=True,\n",
    "    trust_remote_code=True,\n",
    "    device_map = \"auto\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 13,853,536 || all params: 7,555,090,720 || trainable%: 0.1834\n"
     ]
    }
   ],
   "source": [
    "# Device Placement =>> note that BitsAndBytes automatically handles for quantized training\n",
    "if cfg.use_quantization:\n",
    "    vla = prepare_model_for_kbit_training(vla)\n",
    "else:\n",
    "    vla = vla.to(device_id)\n",
    "\n",
    "# [LoRA] Wrap Model w/ PEFT `LoraConfig` =>> by default we set `target_modules=all-linear`\n",
    "if cfg.use_lora:\n",
    "    lora_config = LoraConfig(\n",
    "        r=cfg.lora_rank,\n",
    "        lora_alpha=min(cfg.lora_rank, 16),\n",
    "        lora_dropout=cfg.lora_dropout,\n",
    "        target_modules=\"all-linear\",\n",
    "        init_lora_weights=\"gaussian\",\n",
    "    )\n",
    "    vla = get_peft_model(vla, lora_config)\n",
    "    vla.print_trainable_parameters()\n",
    "\n",
    "# # Wrap VLA in PyTorch DDP Wrapper for Multi-GPU Training\n",
    "# vla = DDP(vla, device_ids=[device_id], find_unused_parameters=True, gradient_as_bucket_view=True)\n",
    "\n",
    "# Create Optimizer =>> note that we default to a simple constant learning rate!\n",
    "trainable_params = [param for param in vla.parameters() if param.requires_grad]\n",
    "optimizer = AdamW(trainable_params, lr=cfg.learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Action Tokenizer\n",
    "action_tokenizer = RLbenchActionTokenizer(processor.tokenizer)\n",
    "\n",
    "train_dataset = RLbenchDataset(\n",
    "    cfg.train_data_path,\n",
    "    action_tokenizer,\n",
    "    processor.tokenizer,\n",
    "    image_transform=processor.image_processor.apply_transform,\n",
    "    prompt_builder_fn=PurePromptBuilder,\n",
    ")\n",
    "\n",
    "test_dataset = RLbenchDataset(\n",
    "    cfg.test_data_path,\n",
    "    action_tokenizer,\n",
    "    processor.tokenizer,\n",
    "    image_transform=processor.image_processor.apply_transform,\n",
    "    prompt_builder_fn=PurePromptBuilder,\n",
    ")\n",
    "\n",
    "valid_dataset = RLbenchDataset(\n",
    "    cfg.valid_data_path,\n",
    "    action_tokenizer,\n",
    "    processor.tokenizer,\n",
    "    image_transform=processor.image_processor.apply_transform,\n",
    "    prompt_builder_fn=PurePromptBuilder,\n",
    ")\n",
    "\n",
    "# [Important] Save Dataset Statistics =>> used to de-normalize actions for inference!\n",
    "if distributed_state.is_main_process:\n",
    "    save_dataset_statistics(train_dataset.dataset_statistics, run_dir)\n",
    "\n",
    "# Create Collator and DataLoader\n",
    "collator = PaddedCollatorForActionPrediction(\n",
    "    processor.tokenizer.model_max_length, processor.tokenizer.pad_token_id, padding_side=\"right\"\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=cfg.batch_size,\n",
    "    shuffle=True,\n",
    "    sampler=None,\n",
    "    collate_fn=collator,\n",
    "    num_workers=0,  # Important =>> Set to 0 if using RLDS; TFDS rolls its own parallelism!\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=cfg.batch_size,\n",
    "    shuffle=True,\n",
    "    sampler=None,\n",
    "    collate_fn=collator,\n",
    "    num_workers=0,  # Important =>> Set to 0 if using RLDS; TFDS rolls its own parallelism!\n",
    ")\n",
    "\n",
    "valid_dataloader = DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=cfg.batch_size,\n",
    "    shuffle=True,\n",
    "    sampler=None,\n",
    "    collate_fn=collator,\n",
    "    num_workers=0,  # Important =>> Set to 0 if using RLDS; TFDS rolls its own parallelism!\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_idx, batch = next(enumerate(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "/home/lawrence/anaconda3/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "vla.train()\n",
    "optimizer.zero_grad()\n",
    "\n",
    "step_idx, batch = next(enumerate(train_dataloader))\n",
    "with torch.autocast(\"cuda\", dtype=torch.bfloat16):\n",
    "    output: CausalLMOutputWithPast = vla(\n",
    "        input_ids=batch[\"input_ids\"].to(device_id),\n",
    "        attention_mask=batch[\"attention_mask\"].to(device_id),\n",
    "        pixel_values=batch[\"pixel_values\"].to(torch.bfloat16).to(device_id),\n",
    "        labels=batch[\"labels\"],\n",
    "    )\n",
    "    loss = output.loss\n",
    "\n",
    "# Backward!\n",
    "# loss.backward()\n",
    "\n",
    "# Compute Accuracy and L1 Loss for Logging\n",
    "action_logits = output.logits[:, vla.vision_backbone.featurizer.patch_embed.num_patches : -1]#TODO: why num_patches:-1?\n",
    "action_preds = action_logits.argmax(dim=2)\n",
    "action_gt = batch[\"labels\"][:, 1:].to(action_preds.device)\n",
    "mask = action_gt > action_tokenizer.action_token_begin_idx\n",
    "\n",
    "# masked_logits = action_logits[:,mask,action_tokenizer.action_token_begin_idx:]\n",
    "masked_logits = action_logits[mask][:,action_tokenizer.action_token_begin_idx+1:processor.tokenizer.vocab_size].view(2,7,-1)\n",
    "\n",
    "\n",
    "\n",
    "# Compute Accuracy\n",
    "correct_preds = (action_preds == action_gt) & mask\n",
    "action_accuracy = correct_preds.sum().float() / mask.sum().float()\n",
    "\n",
    "# # Compute L1 Loss on Predicted (Continuous) Actions\n",
    "# continuous_actions_pred = torch.tensor(\n",
    "#     action_tokenizer.decode_token_ids_to_actions(action_preds[mask].cpu().numpy())\n",
    "# )\n",
    "# continuous_actions_gt = torch.tensor(\n",
    "#     action_tokenizer.decode_token_ids_to_actions(action_gt[mask].cpu().numpy())\n",
    "# )\n",
    "# action_l1_loss = torch.nn.functional.l1_loss(continuous_actions_pred, continuous_actions_gt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 86, 137, 177, 146, 161, 119, 255,  77, 173, 104, 198,  49, 127, 255],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_tokenizer.tokenizer.vocab_size - action_preds[mask] - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7, 352])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(352 - action_logits[0][mask[0],action_tokenizer.action_token_begin_idx:processor.tokenizer.vocab_size-1]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-11.8125, -11.7500, -12.9375,  ...,  -3.2500,  -4.4375,  -0.0311],\n",
       "         [-10.6875,  -9.1875,  -9.4375,  ...,   0.0542,  -0.7891,   1.5078],\n",
       "         [ -8.0625,  -7.2500,  -8.6875,  ...,  -4.7812,  -5.4375,  -2.1406],\n",
       "         ...,\n",
       "         [ -9.3125,  -5.8125,  -5.5000,  ...,   6.1562,   2.8594,  16.7500],\n",
       "         [ -6.6875,  -6.0312,  -6.8438,  ...,   4.7812,   5.9688,   3.5312],\n",
       "         [ -7.4062,  -7.6250,  -8.5625,  ...,   4.4062,   4.9375,   8.1875]],\n",
       "\n",
       "        [[ -9.8750, -13.4375,  -9.0000,  ...,  -3.6719,  -1.3359,   6.1250],\n",
       "         [-10.1875,  -9.3125, -11.0000,  ...,  -3.5156,  -5.2188,  -1.6172],\n",
       "         [ -5.2812,  -3.1250,  -4.5938,  ...,  -2.8906,   0.8789,   2.7031],\n",
       "         ...,\n",
       "         [ -3.7969,  -4.0938,  -1.2188,  ...,   8.8750,   5.2188,  11.8750],\n",
       "         [ -5.6250,  -4.2812,  -7.5000,  ...,   7.2500,   3.6719,   4.9062],\n",
       "         [ -9.1250,  -9.0000,  -9.5000,  ...,   2.0312,   2.7031,   4.3438]]],\n",
       "       device='cuda:0', grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 7, 352])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_action = action_tokenizer.output_logit_to_continous_action(masked_logits,batch['actions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7198, device='cuda:0', grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.l1_loss(pred_action, batch['actions'].to(pred_action.device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_logits[:,1,:50].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = (F.softmax(masked_logits[:, 0,:50], dim = 1)@torch.tensor(action_tokenizer.x_bin_centers,dtype=torch.float32).to('cuda')).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = (F.softmax(masked_logits[:, 1, 50:150], dim = 1)@torch.tensor(action_tokenizer.y_bin_centers,dtype=torch.float32).to('cuda')).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = (F.softmax(masked_logits[:,2,150:250], dim = 1)@torch.tensor(action_tokenizer.z_bin_centers,dtype=torch.float32).to('cuda')).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "rot = F.softmax(masked_logits[:,3:-1,250:350], dim = 2)@torch.tensor(action_tokenizer.rot_bin_centers,dtype=torch.float32).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = (F.softmax(masked_logits[:,-1,350:], dim = 1) @ torch.tensor([0,1],dtype=torch.float32).to('cuda')).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1625, -0.0087,  1.3602, -0.5318,  0.0773, -0.3480,  0.9033],\n",
       "        [ 0.1935,  0.3486,  0.6879, -1.8607,  1.6878, -1.1623,  0.8819]],\n",
       "       device='cuda:0', grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([x,y,z,rot,g],dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.3545e-01,  1.4843e-01,  9.3349e-01, -3.1400e+00,  1.8622e-03,\n",
       "          7.7924e-01,  1.0000e+00],\n",
       "        [ 2.3545e-01,  1.4843e-01,  9.3349e-01, -3.1400e+00,  1.8622e-03,\n",
       "          7.7924e-01,  1.0000e+00]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['actions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 28, 32064])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31647"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_tokenizer.action_token_begin_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31647"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_tokenizer.action_token_begin_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "action_logits\n",
    "action1 = F.softmax(action_logits[:,:,-256:], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "         31691, 31729, 31854, 31898, 31947, 31961, 31998,     2],\n",
       "        [ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100, 31671,\n",
       "         31712, 31835, 31898, 31948, 31992, 31999,     2,  -100]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[7.8702e-11, 6.2277e-09, 1.8299e-10,  ..., 5.8241e-13,\n",
       "          1.0021e-14, 1.8721e-14],\n",
       "         [4.2377e-06, 8.5785e-05, 6.4618e-06,  ..., 4.0644e-11,\n",
       "          4.1049e-10, 5.9137e-11],\n",
       "         [7.2699e-04, 2.7969e-05, 1.5238e-04,  ..., 2.3909e-09,\n",
       "          2.0654e-08, 4.1961e-09],\n",
       "         ...,\n",
       "         [1.7208e-08, 1.7372e-10, 1.1367e-12,  ..., 1.0394e-10,\n",
       "          1.5449e-11, 7.8904e-12],\n",
       "         [5.3094e-16, 9.9611e-14, 3.9790e-12,  ..., 2.4206e-18,\n",
       "          5.8647e-17, 4.2907e-17],\n",
       "         [1.9505e-06, 9.4124e-08, 1.5159e-07,  ..., 2.0914e-10,\n",
       "          1.6805e-10, 4.8146e-11]],\n",
       "\n",
       "        [[3.0287e-07, 3.1740e-07, 1.7730e-09,  ..., 3.4568e-11,\n",
       "          1.1946e-11, 5.6431e-12],\n",
       "         [3.2915e-07, 2.0081e-07, 1.5703e-06,  ..., 1.7571e-12,\n",
       "          3.9596e-12, 7.8746e-12],\n",
       "         [1.2685e-02, 6.3154e-04, 1.3927e-04,  ..., 1.1700e-07,\n",
       "          2.1858e-07, 4.1717e-08],\n",
       "         ...,\n",
       "         [7.4718e-17, 7.0191e-17, 6.4411e-17,  ..., 7.2376e-21,\n",
       "          8.4616e-21, 3.0471e-20],\n",
       "         [1.3171e-08, 1.5103e-08, 3.6742e-09,  ..., 9.2507e-12,\n",
       "          1.1513e-11, 6.7680e-12],\n",
       "         [4.1840e-06, 4.2168e-06, 6.0403e-06,  ..., 2.7055e-09,\n",
       "          3.0657e-09, 1.7468e-09]]], device='cuda:0',\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
