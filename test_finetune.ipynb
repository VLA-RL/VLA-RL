{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-04 01:59:56.011861: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-08-04 01:59:56.011962: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-08-04 01:59:56.071447: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-08-04 01:59:56.188394: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-04 01:59:57.526231: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "sys.path.append('.')\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "import draccus\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import tqdm\n",
    "import wandb\n",
    "from accelerate import PartialState\n",
    "from peft import LoraConfig, PeftModel, get_peft_model, prepare_model_for_kbit_training\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModelForVision2Seq, AutoProcessor, BitsAndBytesConfig\n",
    "from transformers.modeling_outputs import CausalLMOutputWithPast\n",
    "\n",
    "from vla.base_prompter import PurePromptBuilder\n",
    "from vla.utils import PaddedCollatorForPosePrediction, runningLoss, SamplerForPosePrediction\n",
    "from vla.action_tokenizer import RLbenchPoseTokenizer\n",
    "from vla.dataset import RLbenchCotDataset\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class FinetuneConfig:\n",
    "    # fmt: off\n",
    "    vla_path: str = \"/media/lawrence/Work/checkpoints/ecot-openvla-7b-bridge\"   # Path to OpenVLA model \n",
    "    vla_path_q: str = \"/media/lawrence/Work/checkpoints/ecot-openvla-7b-bridge-4b\"   # Path to OpenVLA model\n",
    "\n",
    "    experiment_name: str = \"weighted_loss_cot_\"\n",
    "    dataset_name: str = \"pick_described_object\"                                # Name of fine-tuning dataset (e.g., `droid_wipe`)\n",
    "    # data_path: Path = Path(f\"./datasets/{dataset_name}/data.pt\")\n",
    "    train_data_path: Path = Path(f\"./datasets/{dataset_name}/train_data.pt\")\n",
    "    test_data_path: Path = Path(f\"./datasets/{dataset_name}/test_data.pt\")\n",
    "    item_num = 5\n",
    "    stage_num = 2\n",
    "    run_root_dir: Path = Path(\"./runs\")                               # Path to directory to store logs & checkpoints\n",
    "    adapter_dir: Path = Path(\"./adapter-tmp\")                     # Temporary directory for LoRA weights before fusing\n",
    "\n",
    "    # Fine-tuning Parameters\n",
    "    seed: int = 42                                                  # Random seed\n",
    "    episode: int = 2\n",
    "    batch_size: int = 2#16                                            # Fine-tuning batch size\n",
    "    test_limit_length: int = 30\n",
    "    save_steps: int = 20#5000                                          # Interval for checkpoint saving\n",
    "    learning_rate: float = 5e-5                                     # Fine-tuning learning rate\n",
    "    weight_decay: float = 0.01                                      # Fine-tuning weight decay\n",
    "    grad_accumulation_steps: int = 10                                # Gradient accumulation steps\n",
    "\n",
    "    # LoRA Arguments\n",
    "    use_lora: bool = True                                           # Whether to use LoRA fine-tuning\n",
    "    lora_rank: int = 16#32                                             # Rank of LoRA weight matrix\n",
    "    lora_dropout: float = 0.0                                       # Dropout applied to LoRA weights\n",
    "    use_quantization: bool = True                                  # Whether to 4-bit quantize VLA for LoRA fine-tuning\n",
    "                                                                    #   => CAUTION: Reduces memory but hurts performance\n",
    "    dataset_statistics: tuple = (np.array([-0.2, -0.35,  0.75199986, -np.pi/2, -np.pi/2, -np.pi/2,  0. ]), np.array([0.5, 0.35, 1.3, np.pi/2, 0, np.pi/2, 1.])) # Min-Max normalization statistics\n",
    "\n",
    "    # Tracking Parameters\n",
    "    wandb_project: str = \"vla-rl\"                                  # Name of W&B project to log to (use default!)\n",
    "    wandb_entity: str = \"lawrence-rs-lin-university-of-toronto\"                           # Name of entity to log under\n",
    "\n",
    "    # fmt: on\n",
    "cfg = FinetuneConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning OpenVLA Model `/media/lawrence/Work/checkpoints/ecot-openvla-7b-bridge` on `pick_described_object`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "647bfce1478a429da22b08a09922e7a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(f\"Fine-tuning OpenVLA Model `{cfg.vla_path}` on `{cfg.dataset_name}`\")\n",
    "\n",
    "# [Validate] Ensure GPU Available & Set Device / Distributed Context\n",
    "assert torch.cuda.is_available(), \"Fine-tuning assumes at least one GPU is available!\"\n",
    "distributed_state = PartialState()\n",
    "torch.cuda.set_device(device_id := distributed_state.local_process_index)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Configure Unique Experiment ID & Log Directory\n",
    "exp_id = (\n",
    "    f\"{cfg.vla_path.split('/')[-1]}+{cfg.dataset_name}\"\n",
    "    f\"+b{cfg.batch_size * cfg.grad_accumulation_steps}\"\n",
    "    f\"+lr-{cfg.learning_rate}\"\n",
    ")\n",
    "if cfg.use_lora:\n",
    "    exp_id += f\"+lora-r{cfg.lora_rank}+dropout-{cfg.lora_dropout}\"\n",
    "if cfg.use_quantization:\n",
    "    exp_id += \"+q-4bit\"\n",
    "\n",
    "# Start =>> Build Directories\n",
    "run_dir, adapter_dir = cfg.run_root_dir / exp_id, cfg.adapter_dir / exp_id\n",
    "os.makedirs(run_dir, exist_ok=True)\n",
    "\n",
    "# Quantization Config =>> only if LoRA fine-tuning\n",
    "quantization_config = None\n",
    "if cfg.use_quantization:\n",
    "    assert cfg.use_lora, \"Quantized training only supported for LoRA fine-tuning!\"\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True, bnb_4bit_compute_dtype=torch.bfloat16, bnb_4bit_quant_type=\"nf4\", #llm_int8_skip_modules = ['projector'],\n",
    "    )\n",
    "\n",
    "# Load OpenVLA Processor and Model using HF AutoClasses\n",
    "processor = AutoProcessor.from_pretrained(cfg.vla_path, trust_remote_code=True)\n",
    "add_tokens = ['<g>', '</g>'] + [f'<item_{i}>' for i in np.arange(cfg.item_num)] + ['<o>', '</o>', '<t>', '</t>'] + [f'<stage_{i}>' for i in np.arange(cfg.stage_num)] + ['<a>', '</a>', '<q>', '<cot>']\n",
    "processor.tokenizer.add_tokens(add_tokens)\n",
    "vla = AutoModelForVision2Seq.from_pretrained(\n",
    "    cfg.vla_path,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    attn_implementation=\"sdpa\",\n",
    "    quantization_config=quantization_config,\n",
    "    low_cpu_mem_usage=True,\n",
    "    trust_remote_code=True,\n",
    "    device_map = \"cuda\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 48,801,984 || all params: 7,236,926,592 || trainable%: 0.6743\n"
     ]
    }
   ],
   "source": [
    "# Device Placement =>> note that BitsAndBytes automatically handles for quantized training\n",
    "if cfg.use_quantization:\n",
    "    vla = prepare_model_for_kbit_training(vla)\n",
    "else:\n",
    "    vla = vla.to(device_id)\n",
    "\n",
    "# [LoRA] Wrap Model w/ PEFT `LoraConfig` =>> by default we set `target_modules=all-linear`\n",
    "if cfg.use_lora:\n",
    "    lora_config = LoraConfig(\n",
    "        r=cfg.lora_rank,\n",
    "        lora_alpha=min(cfg.lora_rank, 16),\n",
    "        lora_dropout=cfg.lora_dropout,\n",
    "        target_modules=\"all-linear\",\n",
    "        init_lora_weights=\"gaussian\",\n",
    "    )\n",
    "    vla = get_peft_model(vla, lora_config)\n",
    "    vla.print_trainable_parameters()\n",
    "\n",
    "# # Wrap VLA in PyTorch DDP Wrapper for Multi-GPU Training\n",
    "# vla = DDP(vla, device_ids=[device_id], find_unused_parameters=True, gradient_as_bucket_view=True)\n",
    "\n",
    "# Create Optimizer =>> note that we default to a simple constant learning rate!\n",
    "trainable_params = [param for param in vla.parameters() if param.requires_grad]\n",
    "optimizer = AdamW(trainable_params, lr=cfg.learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Action Tokenizer\n",
    "action_tokenizer = RLbenchPoseTokenizer(processor.tokenizer, cfg.dataset_statistics)\n",
    "\n",
    "trainset = RLbenchCotDataset(\n",
    "    cfg.train_data_path,\n",
    "    action_tokenizer,\n",
    "    processor.tokenizer,\n",
    "    image_transform=processor.image_processor.apply_transform,\n",
    ")\n",
    "\n",
    "testset = RLbenchCotDataset(\n",
    "    cfg.test_data_path,\n",
    "    action_tokenizer,\n",
    "    processor.tokenizer,\n",
    "    image_transform=processor.image_processor.apply_transform,\n",
    ")\n",
    "\n",
    "# Create Collator and DataLoader\n",
    "collator = PaddedCollatorForPosePrediction(\n",
    "    processor.tokenizer.model_max_length, processor.tokenizer.pad_token_id, padding_side=\"right\"\n",
    ")\n",
    "\n",
    "train_sampler = SamplerForPosePrediction(np.array(trainset.data['stages']),group1_ratio=0.3)\n",
    "test_sampler = SamplerForPosePrediction(np.array(testset.data['stages']),group1_ratio=0.3)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    trainset,\n",
    "    batch_size=cfg.batch_size,\n",
    "    shuffle=False,\n",
    "    sampler=train_sampler,\n",
    "    collate_fn=collator,\n",
    "    num_workers=1,  # Important =>> Set to 0 if using RLDS; TFDS rolls its own parallelism!\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    testset,\n",
    "    batch_size=cfg.batch_size,\n",
    "    shuffle=False,\n",
    "    sampler=test_sampler,\n",
    "    collate_fn=collator,\n",
    "    num_workers=1,  # Important =>> Set to 0 if using RLDS; TFDS rolls its own parallelism!\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_running_loss = runningLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vla.train()\n",
    "vla.gradient_checkpointing_enable()\n",
    "optimizer.zero_grad()\n",
    "\n",
    "step_idx, batch = next(enumerate(train_dataloader))\n",
    "\n",
    "with torch.autocast(\"cuda\", dtype=torch.bfloat16):\n",
    "    vla.train()\n",
    "    output: CausalLMOutputWithPast = vla(\n",
    "        input_ids=batch[\"input_ids\"].to(device_id),\n",
    "        attention_mask=batch[\"attention_mask\"].to(device_id),\n",
    "        pixel_values=batch[\"pixel_values\"].to(torch.bfloat16).to(device_id),\n",
    "        labels=batch[\"labels\"],\n",
    "        use_cache=False\n",
    "    )\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    1,   512, 29901,  1724,   881,   367,   278,  2446,  1820, 18593,\n",
       "           310,   278,   330,   374,  2496,   304,  1925,   278,   805,   314,\n",
       "           508,   297,   278, 25972, 29973,   450,  1857,   330,   374,  2496,\n",
       "         18593,   338, 32001, 31449, 31583, 31655, 31734, 31897, 31990, 31998,\n",
       "         32002, 29889,    13,  4451, 29901, 32017, 32005, 29892, 32008, 31450,\n",
       "         31578, 31652, 32009, 29892, 32010, 31398, 31501, 31643, 32011, 29892,\n",
       "         32013, 29892, 32014, 31398, 31501, 31679, 31747, 31897, 31997, 31999,\n",
       "         32015, 29892, 32016, 32000,     2, 32000, 32000],\n",
       "        [    1,   512, 29901,  1724,   881,   367,   278,  2446,  1820, 18593,\n",
       "           310,   278,   330,   374,  2496,   304,  1925,   278,   521,   542,\n",
       "         23167,  9127, 21203,   297,   278, 25972, 29973,   450,  1857,   330,\n",
       "           374,  2496, 18593,   338, 32001, 31423, 31582, 31696, 31752, 31837,\n",
       "         31912, 31999, 32002, 29889,    13,  4451, 29901, 32017, 32003, 29892,\n",
       "         32008, 31466, 31541, 31606, 32009, 29892, 32010, 31398, 31501, 31643,\n",
       "         32011, 29892, 32012, 29892, 32014, 31463, 31543, 31611, 31747, 31897,\n",
       "         31997, 31998, 32015, 29892, 32016, 32000,     2]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "batch['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s> In: What should be the next key pose of the gripper to put the spam can in the basket? The current gripper pose is<g>港－局안ḳὀ弘</g>.\\n Out:<cot><item_2>,<o>任세漢</o>,<t>交ペམ</t>,<stage_1>,<a>交ペ电면ḳ收给</a>,<q><PAD></s><PAD><PAD>',\n",
       " '<s> In: What should be the next key pose of the gripper to put the chocolate gelatin in the basket? The current gripper pose is<g>没민奈ṯ∇昭给</g>.\\n Out:<cot><item_0>,<o>计支関</o>,<t>交ペམ</t>,<stage_0>,<a>序拉共면ḳ收弘</a>,<q><PAD></s>']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.tokenizer.batch_decode(batch['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (0) to match target batch_size (2).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m output_start_idx \u001b[38;5;241m=\u001b[39m vla\u001b[38;5;241m.\u001b[39mvision_backbone\u001b[38;5;241m.\u001b[39mfeaturizer\u001b[38;5;241m.\u001b[39mpatch_embed\u001b[38;5;241m.\u001b[39mnum_patches\n\u001b[0;32m----> 2\u001b[0m loss_dict \u001b[38;5;241m=\u001b[39m action_tokenizer\u001b[38;5;241m.\u001b[39mget_loss(output, batch, output_start_idx)\n\u001b[1;32m      3\u001b[0m normalized_loss_dict \u001b[38;5;241m=\u001b[39m train_running_loss\u001b[38;5;241m.\u001b[39mupdate(loss_dict \u001b[38;5;241m=\u001b[39m loss_dict)\n",
      "File \u001b[0;32m~/VLA-RL/VLA-RL/vla/action_tokenizer.py:222\u001b[0m, in \u001b[0;36mRLbenchPoseTokenizer.get_loss\u001b[0;34m(self, output, batch, output_start_idx)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;66;03m#Gripper Loss\u001b[39;00m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;66;03m# pred_gripper = self.decode(gripper_logits, soft = True)\u001b[39;00m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;66;03m# assert pred_gripper.shape == gt_gripper.shape, f\"Gripper shape {pred_gripper.shape} != {gt_gripper.shape}\"\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    219\u001b[0m \n\u001b[1;32m    220\u001b[0m \u001b[38;5;66;03m#Item Loss\u001b[39;00m\n\u001b[1;32m    221\u001b[0m gt_item \u001b[38;5;241m=\u001b[39m gt_item\u001b[38;5;241m.\u001b[39mclone()\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[0;32m--> 222\u001b[0m item_loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(item_logits, gt_item)\n\u001b[1;32m    224\u001b[0m \u001b[38;5;66;03m#Object Loss\u001b[39;00m\n\u001b[1;32m    225\u001b[0m pred_object \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecode(object_logits, soft \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/functional.py:3086\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3084\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3085\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3086\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mcross_entropy_loss(\u001b[38;5;28minput\u001b[39m, target, weight, _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001b[0;31mValueError\u001b[0m: Expected input batch_size (0) to match target batch_size (2)."
     ]
    }
   ],
   "source": [
    "output_start_idx = vla.vision_backbone.featurizer.patch_embed.num_patches\n",
    "loss_dict = action_tokenizer.get_loss(output, batch, output_start_idx)\n",
    "normalized_loss_dict = train_running_loss.update(loss_dict = loss_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'nll_loss': tensor(19.6134, device='cuda:0', grad_fn=<NllLossBackward0>),\n",
       " 'item_loss': tensor(2.9242, device='cuda:0', grad_fn=<NllLossBackward0>),\n",
       " 'object_position_loss': tensor(0.0366, device='cuda:0', grad_fn=<MseLossBackward0>),\n",
       " 'target_position_loss': tensor(0.0808, device='cuda:0', grad_fn=<MseLossBackward0>),\n",
       " 'stage_loss': tensor(2.2828, device='cuda:0', grad_fn=<NllLossBackward0>),\n",
       " 'action_position_loss': tensor(0.0714, device='cuda:0', grad_fn=<MseLossBackward0>),\n",
       " 'action_orientation_loss': tensor(0.3213, device='cuda:0', grad_fn=<DivBackward0>),\n",
       " 'action_open_loss': tensor(3.8399, device='cuda:0', grad_fn=<NllLossBackward0>)}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "nll_loss = output.loss\n",
    "output_logits = output.logits[:, 256:-1]\n",
    "device_id = output_logits.device\n",
    "batch_size = output_logits.size(0)\n",
    "output_gt = batch[\"labels\"][:, 1:].to(device_id)\n",
    "gripper_mask, item_mask, object_mask, target_mask, stage_mask, action_mask = action_tokenizer.get_mask(output_gt)\n",
    "\n",
    "gripper_logits = output_logits[gripper_mask][:,action_tokenizer.action_token_begin_idx:action_tokenizer.tokenizer.vocab_size].view(batch_size,-1,action_tokenizer.n_bins)\n",
    "item_logits = output_logits[item_mask][:,32003:32008]\n",
    "object_logits = output_logits[object_mask][:,action_tokenizer.action_token_begin_idx:action_tokenizer.tokenizer.vocab_size].view(batch_size,-1,action_tokenizer.n_bins)\n",
    "target_logits = output_logits[target_mask][:,action_tokenizer.action_token_begin_idx:action_tokenizer.tokenizer.vocab_size].view(batch_size,-1,action_tokenizer.n_bins)\n",
    "stage_logits = output_logits[stage_mask][:,32012:32014]\n",
    "action_logits = output_logits[action_mask][:,action_tokenizer.action_token_begin_idx:action_tokenizer.tokenizer.vocab_size].view(batch_size,-1,action_tokenizer.n_bins)\n",
    "\n",
    "gt_gripper = batch['grippers'].to(device_id)\n",
    "gt_item = batch['items'].to(device_id)\n",
    "gt_object = batch['objects'].to(device_id)\n",
    "gt_target = batch['targets'].to(device_id)\n",
    "gt_stage = batch['stages'].to(device_id)\n",
    "gt_action = batch['actions'].to(device_id)\n",
    "\n",
    "#Gripper Loss\n",
    "pred_gripper = action_tokenizer.decode(gripper_logits, soft = True)\n",
    "assert pred_gripper.shape == gt_gripper.shape, f\"Gripper shape {pred_gripper.shape} != {gt_gripper.shape}\"\n",
    "gripper_position_loss = F.mse_loss(pred_gripper[:,:3], gt_gripper[:,:3].to(torch.float32))\n",
    "gripper_orientation_loss = action_tokenizer.angle_loss(pred_gripper[:,3:6], gt_gripper[:,3:6].to(torch.float32))\n",
    "gripper_open_gt = gt_gripper[:,6].to(torch.int64)\n",
    "gripper_open_loss = F.cross_entropy(gripper_logits[:,6,-2:], gripper_open_gt)\n",
    "\n",
    "pred_object = action_tokenizer.decode(object_logits, soft = True)\n",
    "assert pred_object.shape == gt_object.shape, f\"Object shape {pred_object.shape} != {gt_object.shape}\"\n",
    "object_position_loss = F.mse_loss(pred_object[:,:3], gt_object[:,:3].to(torch.float32))\n",
    "\n",
    "#Target Loss\n",
    "pred_target = action_tokenizer.decode(target_logits, soft = True)\n",
    "assert pred_target.shape == gt_target.shape, f\"Target shape {pred_target.shape} != {gt_target.shape}\"\n",
    "target_position_loss = F.mse_loss(pred_target[:,:3], gt_target[:,:3].to(torch.float32))\n",
    "\n",
    "#Stage Loss\n",
    "gt_stage = gt_stage.clone().detach()\n",
    "stage_loss = F.cross_entropy(stage_logits, gt_stage)\n",
    "\n",
    "#Action Loss\n",
    "pred_action = action_tokenizer.decode(action_logits, soft = True)\n",
    "assert pred_action.shape == gt_action.shape, f\"Action shape {pred_action.shape} != {gt_action.shape}\"\n",
    "action_position_loss = F.mse_loss(pred_action[:,:3], gt_action[:,:3].to(torch.float32))\n",
    "action_orientation_loss = action_tokenizer.angle_loss(pred_action[:,3:6], gt_action[:,3:6].to(torch.float32))\n",
    "action_open_gt = gt_action[:,6].to(torch.int64)\n",
    "action_open_loss = F.cross_entropy(action_logits[:,6,-2:], action_open_gt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "/home/lawrence/anaconda3/lib/python3.11/site-packages/torch/utils/checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "step_idx, batch = next(enumerate(test_dataloader))\n",
    "\n",
    "test_loss_dict = {\n",
    "                \"nll_loss\": [],\n",
    "                \"gripper_position_loss\": [],\n",
    "                \"gripper_orientation_loss\": [],\n",
    "                \"gripper_open_loss\": [],\n",
    "                \"item_loss\": [],\n",
    "                \"object_position_loss\": [],\n",
    "                \"target_position_loss\": [],\n",
    "                \"stage_loss\": [],\n",
    "                \"action_position_loss\": [],\n",
    "                \"action_orientation_loss\": [],\n",
    "                \"action_open_loss\": [],\n",
    "            }\n",
    "with torch.no_grad(), torch.autocast(\"cuda\", dtype=torch.bfloat16):\n",
    "    output: CausalLMOutputWithPast = vla(\n",
    "        input_ids=batch[\"input_ids\"].to(device_id),\n",
    "        attention_mask=batch[\"attention_mask\"].to(device_id),\n",
    "        pixel_values=batch[\"pixel_values\"].to(torch.bfloat16).to(device_id),\n",
    "        labels=batch[\"labels\"],\n",
    "    )\n",
    "    output_start_idx = vla.vision_backbone.featurizer.patch_embed.num_patches\n",
    "    loss_dict = action_tokenizer.get_loss(output, batch, output_start_idx)\n",
    "    for k, v in loss_dict.items():\n",
    "        test_loss_dict[k].append(v)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
