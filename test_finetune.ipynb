{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-07 18:51:21.497933: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-08-07 18:51:21.498024: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-08-07 18:51:21.553090: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-08-07 18:51:21.666418: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-07 18:51:22.988330: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "sys.path.append('.')\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "import draccus\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import tqdm\n",
    "import wandb\n",
    "from accelerate import PartialState\n",
    "from peft import LoraConfig, PeftModel, get_peft_model, prepare_model_for_kbit_training\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModelForVision2Seq, AutoProcessor, BitsAndBytesConfig\n",
    "from transformers.modeling_outputs import CausalLMOutputWithPast\n",
    "\n",
    "from vla.base_prompter import PurePromptBuilder\n",
    "from vla.utils import PaddedCollatorForPosePrediction, runningLoss, SamplerForPosePrediction\n",
    "from vla.action_tokenizer import RLbenchPoseTokenizer\n",
    "from vla.dataset import RLbenchCotDataset\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class FinetuneConfig:\n",
    "    # fmt: off\n",
    "    vla_path: str = \"/media/lawrence/Work/checkpoints/ecot-openvla-7b-bridge\"   # Path to OpenVLA model \n",
    "    vla_path_q: str = \"/media/lawrence/Work/checkpoints/ecot-openvla-7b-bridge-4b\"   # Path to OpenVLA model\n",
    "\n",
    "    experiment_name: str = \"weighted_loss_cot_\"\n",
    "    dataset_name: str = \"pick_described_object_replay\"                                # Name of fine-tuning dataset (e.g., `droid_wipe`)\n",
    "    # data_path: Path = Path(f\"./datasets/{dataset_name}/data.pt\")\n",
    "    train_data_path: Path = Path(f\"./datasets/{dataset_name}/data.pt\")\n",
    "    # test_data_path: Path = Path(f\"./datasets/{dataset_name}/test_data.pt\")\n",
    "    item_num = 5\n",
    "    stage_num = 2\n",
    "    run_root_dir: Path = Path(\"./runs\")                               # Path to directory to store logs & checkpoints\n",
    "    adapter_dir: Path = Path(\"./adapter-tmp\")                     # Temporary directory for LoRA weights before fusing\n",
    "\n",
    "    # Fine-tuning Parameters\n",
    "    seed: int = 42                                                  # Random seed\n",
    "    episode: int = 2\n",
    "    batch_size: int = 2#16                                            # Fine-tuning batch size\n",
    "    test_limit_length: int = 30\n",
    "    save_steps: int = 20#5000                                          # Interval for checkpoint saving\n",
    "    learning_rate: float = 5e-5                                     # Fine-tuning learning rate\n",
    "    weight_decay: float = 0.01                                      # Fine-tuning weight decay\n",
    "    grad_accumulation_steps: int = 10                                # Gradient accumulation steps\n",
    "\n",
    "    # LoRA Arguments\n",
    "    use_lora: bool = True                                           # Whether to use LoRA fine-tuning\n",
    "    lora_rank: int = 16#32                                             # Rank of LoRA weight matrix\n",
    "    lora_dropout: float = 0.0                                       # Dropout applied to LoRA weights\n",
    "    use_quantization: bool = True                                  # Whether to 4-bit quantize VLA for LoRA fine-tuning\n",
    "                                                                    #   => CAUTION: Reduces memory but hurts performance\n",
    "    dataset_statistics: tuple = (np.array([-0.2, -0.35,  0.75199986, -np.pi/2, -np.pi/2, -np.pi/2,  0. ]), np.array([0.5, 0.35, 1.3, np.pi/2, 0, np.pi/2, 1.])) # Min-Max normalization statistics\n",
    "\n",
    "    # Tracking Parameters\n",
    "    wandb_project: str = \"vla-rl\"                                  # Name of W&B project to log to (use default!)\n",
    "    wandb_entity: str = \"lawrence-rs-lin-university-of-toronto\"                           # Name of entity to log under\n",
    "\n",
    "    # fmt: on\n",
    "cfg = FinetuneConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning OpenVLA Model `/media/lawrence/Work/checkpoints/ecot-openvla-7b-bridge` on `pick_described_object_replay`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dae2b767d474e7083272c7aa9a5a17e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(f\"Fine-tuning OpenVLA Model `{cfg.vla_path}` on `{cfg.dataset_name}`\")\n",
    "\n",
    "# [Validate] Ensure GPU Available & Set Device / Distributed Context\n",
    "assert torch.cuda.is_available(), \"Fine-tuning assumes at least one GPU is available!\"\n",
    "distributed_state = PartialState()\n",
    "torch.cuda.set_device(device_id := distributed_state.local_process_index)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Configure Unique Experiment ID & Log Directory\n",
    "exp_id = (\n",
    "    f\"{cfg.vla_path.split('/')[-1]}+{cfg.dataset_name}\"\n",
    "    f\"+b{cfg.batch_size * cfg.grad_accumulation_steps}\"\n",
    "    f\"+lr-{cfg.learning_rate}\"\n",
    ")\n",
    "if cfg.use_lora:\n",
    "    exp_id += f\"+lora-r{cfg.lora_rank}+dropout-{cfg.lora_dropout}\"\n",
    "if cfg.use_quantization:\n",
    "    exp_id += \"+q-4bit\"\n",
    "\n",
    "# Start =>> Build Directories\n",
    "run_dir, adapter_dir = cfg.run_root_dir / exp_id, cfg.adapter_dir / exp_id\n",
    "os.makedirs(run_dir, exist_ok=True)\n",
    "\n",
    "# Quantization Config =>> only if LoRA fine-tuning\n",
    "quantization_config = None\n",
    "if cfg.use_quantization:\n",
    "    assert cfg.use_lora, \"Quantized training only supported for LoRA fine-tuning!\"\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True, bnb_4bit_compute_dtype=torch.bfloat16, bnb_4bit_quant_type=\"nf4\", #llm_int8_skip_modules = ['projector'],\n",
    "    )\n",
    "\n",
    "# Load OpenVLA Processor and Model using HF AutoClasses\n",
    "processor = AutoProcessor.from_pretrained(cfg.vla_path, trust_remote_code=True)\n",
    "add_tokens = ['<g>', '</g>'] + [f'<item_{i}>' for i in np.arange(cfg.item_num)] + ['<o>', '</o>', '<t>', '</t>'] + [f'<stage_{i}>' for i in np.arange(cfg.stage_num)] + ['<a>', '</a>', '<q>', '<cot>']\n",
    "processor.tokenizer.add_tokens(add_tokens)\n",
    "vla = AutoModelForVision2Seq.from_pretrained(\n",
    "    cfg.vla_path,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    attn_implementation=\"sdpa\",\n",
    "    quantization_config=quantization_config,\n",
    "    low_cpu_mem_usage=True,\n",
    "    trust_remote_code=True,\n",
    "    device_map = \"cuda\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 48,801,984 || all params: 7,236,926,592 || trainable%: 0.6743\n"
     ]
    }
   ],
   "source": [
    "# Device Placement =>> note that BitsAndBytes automatically handles for quantized training\n",
    "if cfg.use_quantization:\n",
    "    vla = prepare_model_for_kbit_training(vla)\n",
    "else:\n",
    "    vla = vla.to(device_id)\n",
    "\n",
    "# [LoRA] Wrap Model w/ PEFT `LoraConfig` =>> by default we set `target_modules=all-linear`\n",
    "if cfg.use_lora:\n",
    "    lora_config = LoraConfig(\n",
    "        r=cfg.lora_rank,\n",
    "        lora_alpha=min(cfg.lora_rank, 16),\n",
    "        lora_dropout=cfg.lora_dropout,\n",
    "        target_modules=\"all-linear\",\n",
    "        init_lora_weights=\"gaussian\",\n",
    "    )\n",
    "    vla = get_peft_model(vla, lora_config)\n",
    "    vla.print_trainable_parameters()\n",
    "\n",
    "# # Wrap VLA in PyTorch DDP Wrapper for Multi-GPU Training\n",
    "# vla = DDP(vla, device_ids=[device_id], find_unused_parameters=True, gradient_as_bucket_view=True)\n",
    "\n",
    "# Create Optimizer =>> note that we default to a simple constant learning rate!\n",
    "trainable_params = [param for param in vla.parameters() if param.requires_grad]\n",
    "optimizer = AdamW(trainable_params, lr=cfg.learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Action Tokenizer\n",
    "action_tokenizer = RLbenchPoseTokenizer(processor.tokenizer, cfg.dataset_statistics)\n",
    "\n",
    "trainset = RLbenchCotDataset(\n",
    "    cfg.train_data_path,\n",
    "    action_tokenizer,\n",
    "    processor.tokenizer,\n",
    "    image_transform=processor.image_processor.apply_transform,\n",
    ")\n",
    "\n",
    "train_size = int(0.8 * len(trainset))\n",
    "val_size = len(trainset) - train_size\n",
    "train_dataset, val_dataset = random_split(trainset, [train_size, val_size])\n",
    "\n",
    "# Create Collator and DataLoader\n",
    "collator = PaddedCollatorForPosePrediction(\n",
    "    processor.tokenizer.model_max_length, processor.tokenizer.pad_token_id, padding_side=\"right\"\n",
    ")\n",
    "\n",
    "# train_sampler = SamplerForPosePrediction(np.array(trainset.data['stages']),group1_ratio=0.3)\n",
    "# test_sampler = SamplerForPosePrediction(np.array(testset.data['stages']),group1_ratio=0.3)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=cfg.batch_size,\n",
    "    shuffle=True,\n",
    "    # sampler=train_sampler,\n",
    "    collate_fn=collator,\n",
    "    num_workers=1,  # Important =>> Set to 0 if using RLDS; TFDS rolls its own parallelism!\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=cfg.batch_size,\n",
    "    shuffle=False,\n",
    "    sampler=None,\n",
    "    collate_fn=collator,\n",
    "    num_workers=1,  # Important =>> Set to 0 if using RLDS; TFDS rolls its own parallelism!\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pixel_values': tensor([[[[-0.9608, -0.9765, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
       "           [-0.9608, -0.9843, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
       "           [-0.9765, -0.9843, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
       "           ...,\n",
       "           [ 0.5765,  0.5451,  0.5529,  ...,  0.4980,  0.5686,  0.6235],\n",
       "           [ 0.5765,  0.5686,  0.5922,  ...,  0.5529,  0.5373,  0.5843],\n",
       "           [ 0.6157,  0.6157,  0.6392,  ...,  0.6157,  0.5451,  0.5843]],\n",
       " \n",
       "          [[-0.5608, -0.5765, -0.6000,  ..., -0.5843, -0.5686, -0.5686],\n",
       "           [-0.5608, -0.5843, -0.6000,  ..., -0.5843, -0.5686, -0.5686],\n",
       "           [-0.5765, -0.5843, -0.6000,  ..., -0.5843, -0.5686, -0.5686],\n",
       "           ...,\n",
       "           [ 0.2392,  0.2078,  0.2157,  ...,  0.2157,  0.2863,  0.3412],\n",
       "           [ 0.2392,  0.2314,  0.2549,  ...,  0.2706,  0.2549,  0.3020],\n",
       "           [ 0.2784,  0.2784,  0.3020,  ...,  0.3333,  0.2627,  0.3020]],\n",
       " \n",
       "          [[-0.5294, -0.5451, -0.5686,  ..., -0.5529, -0.5451, -0.5451],\n",
       "           [-0.5294, -0.5529, -0.5686,  ..., -0.5529, -0.5451, -0.5451],\n",
       "           [-0.5451, -0.5529, -0.5686,  ..., -0.5529, -0.5451, -0.5451],\n",
       "           ...,\n",
       "           [-0.3804, -0.4118, -0.4039,  ..., -0.3804, -0.3098, -0.2549],\n",
       "           [-0.3804, -0.3882, -0.3647,  ..., -0.3255, -0.3412, -0.2941],\n",
       "           [-0.3412, -0.3412, -0.3176,  ..., -0.2627, -0.3333, -0.2941]]],\n",
       " \n",
       " \n",
       "         [[[-0.9608, -0.9765, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
       "           [-0.9608, -0.9843, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
       "           [-0.9765, -0.9843, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
       "           ...,\n",
       "           [ 0.5765,  0.5451,  0.5529,  ...,  0.4980,  0.5686,  0.6235],\n",
       "           [ 0.5765,  0.5686,  0.5922,  ...,  0.5529,  0.5373,  0.5843],\n",
       "           [ 0.6157,  0.6157,  0.6392,  ...,  0.6157,  0.5451,  0.5843]],\n",
       " \n",
       "          [[-0.5608, -0.5765, -0.6000,  ..., -0.5843, -0.5686, -0.5686],\n",
       "           [-0.5608, -0.5843, -0.6000,  ..., -0.5843, -0.5686, -0.5686],\n",
       "           [-0.5765, -0.5843, -0.6000,  ..., -0.5843, -0.5686, -0.5686],\n",
       "           ...,\n",
       "           [ 0.2392,  0.2078,  0.2157,  ...,  0.2157,  0.2863,  0.3412],\n",
       "           [ 0.2392,  0.2314,  0.2549,  ...,  0.2706,  0.2549,  0.3020],\n",
       "           [ 0.2784,  0.2784,  0.3020,  ...,  0.3333,  0.2627,  0.3020]],\n",
       " \n",
       "          [[-0.5294, -0.5451, -0.5686,  ..., -0.5529, -0.5451, -0.5451],\n",
       "           [-0.5294, -0.5529, -0.5686,  ..., -0.5529, -0.5451, -0.5451],\n",
       "           [-0.5451, -0.5529, -0.5686,  ..., -0.5529, -0.5451, -0.5451],\n",
       "           ...,\n",
       "           [-0.3804, -0.4118, -0.4039,  ..., -0.3804, -0.3098, -0.2549],\n",
       "           [-0.3804, -0.3882, -0.3647,  ..., -0.3255, -0.3412, -0.2941],\n",
       "           [-0.3412, -0.3412, -0.3176,  ..., -0.2627, -0.3333, -0.2941]]]]),\n",
       " 'input_ids': tensor([[    1,   512, 29901,  1724,   881,   367,   278,  2446,  1820, 18593,\n",
       "            310,   278,   330,   374,  2496,   304,  4337,   278, 26438,  5639,\n",
       "            304,   278, 25972, 29973,   450,  1857,   330,   374,  2496, 18593,\n",
       "            338, 32001, 31491, 31529, 31627, 31751, 31897, 31898, 31999, 32002,\n",
       "          29889,    13,  4451, 29901, 32017, 32007, 29892, 32008, 31488, 31529,\n",
       "          31613, 32009, 29892, 32010, 31398, 31501, 31643, 32011, 29892, 32013,\n",
       "          29892, 32014, 31398, 31501, 31679, 31747, 31897, 31997, 31999, 32015,\n",
       "          29892, 32016, 32000,     2],\n",
       "         [    1,   512, 29901,  1724,   881,   367,   278,  2446,  1820, 18593,\n",
       "            310,   278,   330,   374,  2496,   304,  2058,   278, 26438,  3800,\n",
       "            297,   278, 25972, 29973,   450,  1857,   330,   374,  2496, 18593,\n",
       "            338, 32001, 31480, 31573, 31623, 31747, 31897, 31954, 31998, 32002,\n",
       "          29889,    13,  4451, 29901, 32017, 32007, 29892, 32008, 31480, 31573,\n",
       "          31613, 32009, 29892, 32010, 31398, 31501, 31643, 32011, 29892, 32013,\n",
       "          29892, 32014, 31398, 31501, 31679, 31747, 31897, 31997, 31999, 32015,\n",
       "          29892, 32016, 32000,     2]]),\n",
       " 'attention_mask': tensor([[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True, False,  True],\n",
       "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "           True,  True, False,  True]]),\n",
       " 'labels': tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100, 32007, 29892, 32008, 31488, 31529,\n",
       "          31613, 32009, 29892, 32010, 31398, 31501, 31643, 32011, 29892, 32013,\n",
       "          29892, 32014, 31398, 31501, 31679, 31747, 31897, 31997, 31999, 32015,\n",
       "          29892, 32016,  -100,     2],\n",
       "         [ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100, 32007, 29892, 32008, 31480, 31573,\n",
       "          31613, 32009, 29892, 32010, 31398, 31501, 31643, 32011, 29892, 32013,\n",
       "          29892, 32014, 31398, 31501, 31679, 31747, 31897, 31997, 31999, 32015,\n",
       "          29892, 32016,  -100,     2]]),\n",
       " 'grippers': tensor([[ 4.5516e-01, -1.2644e-01,  9.1376e-01, -3.0164e+00,  1.0932e-01,\n",
       "          -2.9244e+00,  1.0000e+00],\n",
       "         [ 3.7405e-01,  1.7576e-01,  8.9267e-01,  3.1415e+00,  1.1139e-03,\n",
       "           2.1122e-01,  0.0000e+00]], dtype=torch.float64),\n",
       " 'items': tensor([4, 4]),\n",
       " 'objects': tensor([[ 0.4338, -0.1315,  0.8394],\n",
       "         [ 0.3747,  0.1756,  0.8376]], dtype=torch.float64),\n",
       " 'targets': tensor([[-0.2000, -0.3250,  1.0000],\n",
       "         [-0.2000, -0.3250,  1.0000]], dtype=torch.float64),\n",
       " 'stages': tensor([1, 1]),\n",
       " 'actions': tensor([[-2.0000e-01, -3.2500e-01,  1.2000e+00,  3.1416e+00, -1.3101e-14,\n",
       "           3.1416e+00,  1.0000e+00],\n",
       "         [-2.0000e-01, -3.2500e-01,  1.2000e+00,  3.1416e+00, -1.3101e-14,\n",
       "           3.1416e+00,  1.0000e+00]], dtype=torch.float64)}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_running_loss = runningLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vla.train()\n",
    "vla.gradient_checkpointing_enable()\n",
    "optimizer.zero_grad()\n",
    "\n",
    "step_idx, batch = next(enumerate(train_dataloader))\n",
    "\n",
    "with torch.autocast(\"cuda\", dtype=torch.bfloat16):\n",
    "    vla.train()\n",
    "    output: CausalLMOutputWithPast = vla(\n",
    "        input_ids=batch[\"input_ids\"].to(device_id),\n",
    "        attention_mask=batch[\"attention_mask\"].to(device_id),\n",
    "        pixel_values=batch[\"pixel_values\"].to(torch.bfloat16).to(device_id),\n",
    "        labels=batch[\"labels\"],\n",
    "        use_cache=False\n",
    "    )\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100, 32007, 29892, 32008, 31434, 31597,\n",
       "         31613, 32009, 29892, 32010, 31398, 31501, 31643, 32011, 29892, 32013,\n",
       "         29892, 32014, 31398, 31501, 31679, 31747, 31897, 31997, 31999, 32015,\n",
       "         29892, 32016,  -100,     2],\n",
       "        [ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100, 32004, 29892, 32008, 31440, 31508,\n",
       "         31606, 32009, 29892, 32010, 31398, 31501, 31643, 32011, 29892, 32012,\n",
       "         29892, 32014, 31440, 31508, 31611, 31747, 31897, 31997, 31998, 32015,\n",
       "         29892, 32016,  -100,     2]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s> In: What should be the next key pose of the gripper to store the sugar box in the basket? The current gripper pose is<g>բˆ್면ḳ彦弘</g>.\\n Out:<cot><item_4>,<o>բˆ态</o>,<t>交ペམ</t>,<stage_1>,<a>交ペ电면ḳ收给</a>,<q><PAD></s>',\n",
       " '<s> In: What should be the next key pose of the gripper to put the soup tin in the basket? The current gripper pose is<g>ォ천ợ该测彦给</g>.\\n Out:<cot><item_1>,<o>\\x91Ṭ関</o>,<t>交ペམ</t>,<stage_0>,<a>\\x91Ṭ共면ḳ收弘</a>,<q><PAD></s>']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.tokenizer.batch_decode(batch['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "tuple indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m output_start_idx \u001b[38;5;241m=\u001b[39m vla\u001b[38;5;241m.\u001b[39mvision_backbone\u001b[38;5;241m.\u001b[39mfeaturizer\u001b[38;5;241m.\u001b[39mpatch_embed\u001b[38;5;241m.\u001b[39mnum_patches\n\u001b[1;32m      2\u001b[0m loss_dict \u001b[38;5;241m=\u001b[39m action_tokenizer\u001b[38;5;241m.\u001b[39mget_loss(output, batch, output_start_idx)\n\u001b[0;32m----> 3\u001b[0m normalized_loss_dict \u001b[38;5;241m=\u001b[39m train_running_loss\u001b[38;5;241m.\u001b[39mupdate(loss_dict \u001b[38;5;241m=\u001b[39m loss_dict)\n",
      "File \u001b[0;32m~/VLA-RL/VLA-RL/vla/utils.py:153\u001b[0m, in \u001b[0;36mrunningLoss.update\u001b[0;34m(self, loss_dict)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate\u001b[39m(\u001b[38;5;28mself\u001b[39m, loss_dict):\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnll_loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 153\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnll_loss \u001b[38;5;241m=\u001b[39m loss_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnll_loss\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    154\u001b[0m         \u001b[38;5;66;03m# self.gripper_position_loss = loss_dict['gripper_position_loss'].item()\u001b[39;00m\n\u001b[1;32m    155\u001b[0m         \u001b[38;5;66;03m# self.gripper_orientation_loss = loss_dict['gripper_orientation_loss'].item()\u001b[39;00m\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;66;03m# self.gripper_open_loss = loss_dict['gripper_open_loss'].item()\u001b[39;00m\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitem_loss \u001b[38;5;241m=\u001b[39m loss_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mitem_loss\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mitem()\n",
      "\u001b[0;31mTypeError\u001b[0m: tuple indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "output_start_idx = vla.vision_backbone.featurizer.patch_embed.num_patches\n",
    "loss_dict = action_tokenizer.get_loss(output, batch, output_start_idx)\n",
    "normalized_loss_dict = train_running_loss.update(loss_dict = loss_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'nll_loss': tensor(19.6134, device='cuda:0', grad_fn=<NllLossBackward0>),\n",
       " 'item_loss': tensor(2.9242, device='cuda:0', grad_fn=<NllLossBackward0>),\n",
       " 'object_position_loss': tensor(0.0366, device='cuda:0', grad_fn=<MseLossBackward0>),\n",
       " 'target_position_loss': tensor(0.0808, device='cuda:0', grad_fn=<MseLossBackward0>),\n",
       " 'stage_loss': tensor(2.2828, device='cuda:0', grad_fn=<NllLossBackward0>),\n",
       " 'action_position_loss': tensor(0.0714, device='cuda:0', grad_fn=<MseLossBackward0>),\n",
       " 'action_orientation_loss': tensor(0.3213, device='cuda:0', grad_fn=<DivBackward0>),\n",
       " 'action_open_loss': tensor(3.8399, device='cuda:0', grad_fn=<NllLossBackward0>)}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "nll_loss = output.loss\n",
    "output_logits = output.logits[:, 256:-1]\n",
    "device_id = output_logits.device\n",
    "batch_size = output_logits.size(0)\n",
    "output_gt = batch[\"labels\"][:, 1:].to(device_id)\n",
    "gripper_mask, item_mask, object_mask, target_mask, stage_mask, action_mask = action_tokenizer.get_mask(output_gt)\n",
    "\n",
    "gripper_logits = output_logits[gripper_mask][:,action_tokenizer.action_token_begin_idx:action_tokenizer.tokenizer.vocab_size].view(batch_size,-1,action_tokenizer.n_bins)\n",
    "item_logits = output_logits[item_mask][:,32003:32008]\n",
    "object_logits = output_logits[object_mask][:,action_tokenizer.action_token_begin_idx:action_tokenizer.tokenizer.vocab_size].view(batch_size,-1,action_tokenizer.n_bins)\n",
    "target_logits = output_logits[target_mask][:,action_tokenizer.action_token_begin_idx:action_tokenizer.tokenizer.vocab_size].view(batch_size,-1,action_tokenizer.n_bins)\n",
    "stage_logits = output_logits[stage_mask][:,32012:32014]\n",
    "action_logits = output_logits[action_mask][:,action_tokenizer.action_token_begin_idx:action_tokenizer.tokenizer.vocab_size].view(batch_size,-1,action_tokenizer.n_bins)\n",
    "\n",
    "gt_gripper = batch['grippers'].to(device_id)\n",
    "gt_item = batch['items'].to(device_id)\n",
    "gt_object = batch['objects'].to(device_id)\n",
    "gt_target = batch['targets'].to(device_id)\n",
    "gt_stage = batch['stages'].to(device_id)\n",
    "gt_action = batch['actions'].to(device_id)\n",
    "\n",
    "#Gripper Loss\n",
    "pred_gripper = action_tokenizer.decode(gripper_logits, soft = True)\n",
    "assert pred_gripper.shape == gt_gripper.shape, f\"Gripper shape {pred_gripper.shape} != {gt_gripper.shape}\"\n",
    "gripper_position_loss = F.mse_loss(pred_gripper[:,:3], gt_gripper[:,:3].to(torch.float32))\n",
    "gripper_orientation_loss = action_tokenizer.angle_loss(pred_gripper[:,3:6], gt_gripper[:,3:6].to(torch.float32))\n",
    "gripper_open_gt = gt_gripper[:,6].to(torch.int64)\n",
    "gripper_open_loss = F.cross_entropy(gripper_logits[:,6,-2:], gripper_open_gt)\n",
    "\n",
    "pred_object = action_tokenizer.decode(object_logits, soft = True)\n",
    "assert pred_object.shape == gt_object.shape, f\"Object shape {pred_object.shape} != {gt_object.shape}\"\n",
    "object_position_loss = F.mse_loss(pred_object[:,:3], gt_object[:,:3].to(torch.float32))\n",
    "\n",
    "#Target Loss\n",
    "pred_target = action_tokenizer.decode(target_logits, soft = True)\n",
    "assert pred_target.shape == gt_target.shape, f\"Target shape {pred_target.shape} != {gt_target.shape}\"\n",
    "target_position_loss = F.mse_loss(pred_target[:,:3], gt_target[:,:3].to(torch.float32))\n",
    "\n",
    "#Stage Loss\n",
    "gt_stage = gt_stage.clone().detach()\n",
    "stage_loss = F.cross_entropy(stage_logits, gt_stage)\n",
    "\n",
    "#Action Loss\n",
    "pred_action = action_tokenizer.decode(action_logits, soft = True)\n",
    "assert pred_action.shape == gt_action.shape, f\"Action shape {pred_action.shape} != {gt_action.shape}\"\n",
    "action_position_loss = F.mse_loss(pred_action[:,:3], gt_action[:,:3].to(torch.float32))\n",
    "action_orientation_loss = action_tokenizer.angle_loss(pred_action[:,3:6], gt_action[:,3:6].to(torch.float32))\n",
    "action_open_gt = gt_action[:,6].to(torch.int64)\n",
    "action_open_loss = F.cross_entropy(action_logits[:,6,-2:], action_open_gt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "/home/lawrence/anaconda3/lib/python3.11/site-packages/torch/utils/checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "step_idx, batch = next(enumerate(test_dataloader))\n",
    "\n",
    "test_loss_dict = {\n",
    "                \"nll_loss\": [],\n",
    "                \"gripper_position_loss\": [],\n",
    "                \"gripper_orientation_loss\": [],\n",
    "                \"gripper_open_loss\": [],\n",
    "                \"item_loss\": [],\n",
    "                \"object_position_loss\": [],\n",
    "                \"target_position_loss\": [],\n",
    "                \"stage_loss\": [],\n",
    "                \"action_position_loss\": [],\n",
    "                \"action_orientation_loss\": [],\n",
    "                \"action_open_loss\": [],\n",
    "            }\n",
    "with torch.no_grad(), torch.autocast(\"cuda\", dtype=torch.bfloat16):\n",
    "    output: CausalLMOutputWithPast = vla(\n",
    "        input_ids=batch[\"input_ids\"].to(device_id),\n",
    "        attention_mask=batch[\"attention_mask\"].to(device_id),\n",
    "        pixel_values=batch[\"pixel_values\"].to(torch.bfloat16).to(device_id),\n",
    "        labels=batch[\"labels\"],\n",
    "    )\n",
    "    output_start_idx = vla.vision_backbone.featurizer.patch_embed.num_patches\n",
    "    loss_dict = action_tokenizer.get_loss(output, batch, output_start_idx)\n",
    "    for k, v in loss_dict.items():\n",
    "        test_loss_dict[k].append(v)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
