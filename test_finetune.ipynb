{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 23:05:24.186790: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-08-01 23:05:24.186904: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-08-01 23:05:24.229604: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-08-01 23:05:24.335046: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-01 23:05:25.608610: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "sys.path.append('.')\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "import draccus\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import tqdm\n",
    "import wandb\n",
    "from accelerate import PartialState\n",
    "from peft import LoraConfig, PeftModel, get_peft_model, prepare_model_for_kbit_training\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModelForVision2Seq, AutoProcessor, BitsAndBytesConfig\n",
    "from transformers.modeling_outputs import CausalLMOutputWithPast\n",
    "\n",
    "from vla.base_prompter import PurePromptBuilder\n",
    "from vla.utils import PaddedCollatorForPosePrediction, runningLoss, SamplerForPosePrediction\n",
    "from vla.action_tokenizer import RLbenchPoseTokenizer\n",
    "from vla.dataset import RLbenchCotDataset\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class FinetuneConfig:\n",
    "    # fmt: off\n",
    "    vla_path: str = \"/media/lawrence/Work/checkpoints/ecot-openvla-7b-bridge\"   # Path to OpenVLA model \n",
    "    vla_path_q: str = \"/media/lawrence/Work/checkpoints/ecot-openvla-7b-bridge-4b\"   # Path to OpenVLA model\n",
    "\n",
    "    experiment_name: str = \"weighted_loss_cot_\"\n",
    "    dataset_name: str = \"pick_described_object1\"                                # Name of fine-tuning dataset (e.g., `droid_wipe`)\n",
    "    # data_path: Path = Path(f\"./datasets/{dataset_name}/data.pt\")\n",
    "    train_data_path: Path = Path(f\"./datasets/{dataset_name}/train_data.pt\")\n",
    "    test_data_path: Path = Path(f\"./datasets/{dataset_name}/test_data.pt\")\n",
    "    item_num = 5\n",
    "    stage_num = 2\n",
    "    run_root_dir: Path = Path(\"./runs\")                               # Path to directory to store logs & checkpoints\n",
    "    adapter_dir: Path = Path(\"./adapter-tmp\")                     # Temporary directory for LoRA weights before fusing\n",
    "\n",
    "    # Fine-tuning Parameters\n",
    "    seed: int = 42                                                  # Random seed\n",
    "    episode: int = 2\n",
    "    batch_size: int = 2#16                                            # Fine-tuning batch size\n",
    "    test_limit_length: int = 30\n",
    "    save_steps: int = 20#5000                                          # Interval for checkpoint saving\n",
    "    learning_rate: float = 5e-5                                     # Fine-tuning learning rate\n",
    "    weight_decay: float = 0.01                                      # Fine-tuning weight decay\n",
    "    grad_accumulation_steps: int = 10                                # Gradient accumulation steps\n",
    "\n",
    "    # LoRA Arguments\n",
    "    use_lora: bool = True                                           # Whether to use LoRA fine-tuning\n",
    "    lora_rank: int = 16#32                                             # Rank of LoRA weight matrix\n",
    "    lora_dropout: float = 0.0                                       # Dropout applied to LoRA weights\n",
    "    use_quantization: bool = True                                  # Whether to 4-bit quantize VLA for LoRA fine-tuning\n",
    "                                                                    #   => CAUTION: Reduces memory but hurts performance\n",
    "    dataset_statistics: tuple = (np.array([-0.2, -0.35,  0.75199986, -np.pi/2, -np.pi/2, -np.pi/2,  0. ]), np.array([0.5, 0.35, 1.3, np.pi/2, 0, np.pi/2, 1.])) # Min-Max normalization statistics\n",
    "\n",
    "    # Tracking Parameters\n",
    "    wandb_project: str = \"vla-rl\"                                  # Name of W&B project to log to (use default!)\n",
    "    wandb_entity: str = \"lawrence-rs-lin-university-of-toronto\"                           # Name of entity to log under\n",
    "\n",
    "    # fmt: on\n",
    "cfg = FinetuneConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning OpenVLA Model `/media/lawrence/Work/checkpoints/ecot-openvla-7b-bridge` on `pick_described_object1`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "/home/lawrence/anaconda3/lib/python3.11/site-packages/transformers/quantizers/auto.py:159: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n",
      "  warnings.warn(warning_msg)\n",
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Fine-tuning OpenVLA Model `{cfg.vla_path}` on `{cfg.dataset_name}`\")\n",
    "\n",
    "# [Validate] Ensure GPU Available & Set Device / Distributed Context\n",
    "assert torch.cuda.is_available(), \"Fine-tuning assumes at least one GPU is available!\"\n",
    "distributed_state = PartialState()\n",
    "torch.cuda.set_device(device_id := distributed_state.local_process_index)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Configure Unique Experiment ID & Log Directory\n",
    "exp_id = (\n",
    "    f\"{cfg.vla_path.split('/')[-1]}+{cfg.dataset_name}\"\n",
    "    f\"+b{cfg.batch_size * cfg.grad_accumulation_steps}\"\n",
    "    f\"+lr-{cfg.learning_rate}\"\n",
    ")\n",
    "if cfg.use_lora:\n",
    "    exp_id += f\"+lora-r{cfg.lora_rank}+dropout-{cfg.lora_dropout}\"\n",
    "if cfg.use_quantization:\n",
    "    exp_id += \"+q-4bit\"\n",
    "\n",
    "# Start =>> Build Directories\n",
    "run_dir, adapter_dir = cfg.run_root_dir / exp_id, cfg.adapter_dir / exp_id\n",
    "os.makedirs(run_dir, exist_ok=True)\n",
    "\n",
    "# Quantization Config =>> only if LoRA fine-tuning\n",
    "quantization_config = None\n",
    "if cfg.use_quantization:\n",
    "    assert cfg.use_lora, \"Quantized training only supported for LoRA fine-tuning!\"\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True, bnb_4bit_compute_dtype=torch.bfloat16, bnb_4bit_quant_type=\"nf4\", #llm_int8_skip_modules = ['projector'],\n",
    "    )\n",
    "\n",
    "# Load OpenVLA Processor and Model using HF AutoClasses\n",
    "processor = AutoProcessor.from_pretrained(cfg.vla_path, trust_remote_code=True)\n",
    "add_tokens = ['<g>', '</g>'] + [f'<item_{i}>' for i in np.arange(cfg.item_num)] + ['<o>', '</o>', '<t>', '</t>'] + [f'<stage_{i}>' for i in np.arange(cfg.stage_num)] + ['<a>', '</a>']\n",
    "processor.tokenizer.add_tokens(add_tokens)\n",
    "vla = AutoModelForVision2Seq.from_pretrained(\n",
    "    cfg.vla_path_q,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    # attn_implementation=\"sdpa\",\n",
    "    quantization_config=quantization_config,\n",
    "    low_cpu_mem_usage=True,\n",
    "    trust_remote_code=True,\n",
    "    device_map = \"auto\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 48,801,984 || all params: 7,236,926,592 || trainable%: 0.6743\n"
     ]
    }
   ],
   "source": [
    "# Device Placement =>> note that BitsAndBytes automatically handles for quantized training\n",
    "if cfg.use_quantization:\n",
    "    vla = prepare_model_for_kbit_training(vla)\n",
    "else:\n",
    "    vla = vla.to(device_id)\n",
    "\n",
    "# [LoRA] Wrap Model w/ PEFT `LoraConfig` =>> by default we set `target_modules=all-linear`\n",
    "if cfg.use_lora:\n",
    "    lora_config = LoraConfig(\n",
    "        r=cfg.lora_rank,\n",
    "        lora_alpha=min(cfg.lora_rank, 16),\n",
    "        lora_dropout=cfg.lora_dropout,\n",
    "        target_modules=\"all-linear\",\n",
    "        init_lora_weights=\"gaussian\",\n",
    "    )\n",
    "    vla = get_peft_model(vla, lora_config)\n",
    "    vla.print_trainable_parameters()\n",
    "\n",
    "# # Wrap VLA in PyTorch DDP Wrapper for Multi-GPU Training\n",
    "# vla = DDP(vla, device_ids=[device_id], find_unused_parameters=True, gradient_as_bucket_view=True)\n",
    "\n",
    "# Create Optimizer =>> note that we default to a simple constant learning rate!\n",
    "trainable_params = [param for param in vla.parameters() if param.requires_grad]\n",
    "optimizer = AdamW(trainable_params, lr=cfg.learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Action Tokenizer\n",
    "action_tokenizer = RLbenchPoseTokenizer(processor.tokenizer, cfg.dataset_statistics)\n",
    "\n",
    "trainset = RLbenchCotDataset(\n",
    "    cfg.train_data_path,\n",
    "    action_tokenizer,\n",
    "    processor.tokenizer,\n",
    "    image_transform=processor.image_processor.apply_transform,\n",
    ")\n",
    "\n",
    "testset = RLbenchCotDataset(\n",
    "    cfg.test_data_path,\n",
    "    action_tokenizer,\n",
    "    processor.tokenizer,\n",
    "    image_transform=processor.image_processor.apply_transform,\n",
    ")\n",
    "\n",
    "# Create Collator and DataLoader\n",
    "collator = PaddedCollatorForPosePrediction(\n",
    "    processor.tokenizer.model_max_length, processor.tokenizer.pad_token_id, padding_side=\"right\"\n",
    ")\n",
    "\n",
    "train_sampler = SamplerForPosePrediction(np.array(trainset.data['stages']),group1_ratio=0.3)\n",
    "test_sampler = SamplerForPosePrediction(np.array(testset.data['stages']),group1_ratio=0.3)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    trainset,\n",
    "    batch_size=cfg.batch_size,\n",
    "    shuffle=False,\n",
    "    sampler=train_sampler,\n",
    "    collate_fn=collator,\n",
    "    num_workers=1,  # Important =>> Set to 0 if using RLDS; TFDS rolls its own parallelism!\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    testset,\n",
    "    batch_size=cfg.batch_size,\n",
    "    shuffle=False,\n",
    "    sampler=test_sampler,\n",
    "    collate_fn=collator,\n",
    "    num_workers=1,  # Important =>> Set to 0 if using RLDS; TFDS rolls its own parallelism!\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_running_loss = runningLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vla.train()\n",
    "vla.gradient_checkpointing_enable()\n",
    "optimizer.zero_grad()\n",
    "\n",
    "step_idx, batch = next(enumerate(train_dataloader))\n",
    "\n",
    "with torch.autocast(\"cuda\", dtype=torch.bfloat16):\n",
    "    vla.train()\n",
    "    output: CausalLMOutputWithPast = vla(\n",
    "        input_ids=batch[\"input_ids\"].to(device_id),\n",
    "        attention_mask=batch[\"attention_mask\"].to(device_id),\n",
    "        pixel_values=batch[\"pixel_values\"].to(torch.bfloat16).to(device_id),\n",
    "        labels=batch[\"labels\"],\n",
    "        use_cache=False\n",
    "    )\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lawrence/VLA-RL/VLA-RL/vla/action_tokenizer.py:221: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  gt_item = torch.tensor(gt_item, dtype=torch.int64).to(device_id)\n",
      "/home/lawrence/VLA-RL/VLA-RL/vla/action_tokenizer.py:235: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  gt_stage = torch.tensor(gt_stage, dtype=torch.int64).clone().detach()\n"
     ]
    }
   ],
   "source": [
    "output_start_idx = vla.vision_backbone.featurizer.patch_embed.num_patches\n",
    "loss_dict = action_tokenizer.get_loss(output, batch, output_start_idx)\n",
    "normalized_loss_dict = train_running_loss.update(loss_dict = loss_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'nll_loss': tensor(19.1355, device='cuda:0', grad_fn=<NllLossBackward0>),\n",
       " 'gripper_position_loss': tensor(0.0203, device='cuda:0', grad_fn=<MseLossBackward0>),\n",
       " 'gripper_orientation_loss': tensor(0.3016, device='cuda:0', grad_fn=<DivBackward0>),\n",
       " 'gripper_open_loss': tensor(3.8078, device='cuda:0', grad_fn=<NllLossBackward0>),\n",
       " 'item_loss': tensor(1.8793, device='cuda:0', grad_fn=<NllLossBackward0>),\n",
       " 'object_position_loss': tensor(0.0262, device='cuda:0', grad_fn=<MseLossBackward0>),\n",
       " 'target_position_loss': tensor(0.0751, device='cuda:0', grad_fn=<MseLossBackward0>),\n",
       " 'stage_loss': tensor(0.0725, device='cuda:0', grad_fn=<NllLossBackward0>),\n",
       " 'action_position_loss': tensor(0.0089, device='cuda:0', grad_fn=<MseLossBackward0>),\n",
       " 'action_orientation_loss': tensor(0.7915, device='cuda:0', grad_fn=<DivBackward0>),\n",
       " 'action_open_loss': tensor(1.2769, device='cuda:0', grad_fn=<NllLossBackward0>)}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "nll_loss = output.loss\n",
    "output_logits = output.logits[:, 256:-1]\n",
    "device_id = output_logits.device\n",
    "batch_size = output_logits.size(0)\n",
    "output_gt = batch[\"labels\"][:, 1:].to(device_id)\n",
    "gripper_mask, item_mask, object_mask, target_mask, stage_mask, action_mask = action_tokenizer.get_mask(output_gt)\n",
    "\n",
    "gripper_logits = output_logits[gripper_mask][:,action_tokenizer.action_token_begin_idx:action_tokenizer.tokenizer.vocab_size].view(batch_size,-1,action_tokenizer.n_bins)\n",
    "item_logits = output_logits[item_mask][:,32003:32008]\n",
    "object_logits = output_logits[object_mask][:,action_tokenizer.action_token_begin_idx:action_tokenizer.tokenizer.vocab_size].view(batch_size,-1,action_tokenizer.n_bins)\n",
    "target_logits = output_logits[target_mask][:,action_tokenizer.action_token_begin_idx:action_tokenizer.tokenizer.vocab_size].view(batch_size,-1,action_tokenizer.n_bins)\n",
    "stage_logits = output_logits[stage_mask][:,32012:32014]\n",
    "action_logits = output_logits[action_mask][:,action_tokenizer.action_token_begin_idx:action_tokenizer.tokenizer.vocab_size].view(batch_size,-1,action_tokenizer.n_bins)\n",
    "\n",
    "gt_gripper = batch['grippers'].to(device_id)\n",
    "gt_item = batch['items'].to(device_id)\n",
    "gt_object = batch['objects'].to(device_id)\n",
    "gt_target = batch['targets'].to(device_id)\n",
    "gt_stage = batch['stages'].to(device_id)\n",
    "gt_action = batch['actions'].to(device_id)\n",
    "\n",
    "#Gripper Loss\n",
    "pred_gripper = action_tokenizer.decode(gripper_logits, soft = True)\n",
    "assert pred_gripper.shape == gt_gripper.shape, f\"Gripper shape {pred_gripper.shape} != {gt_gripper.shape}\"\n",
    "gripper_position_loss = F.mse_loss(pred_gripper[:,:3], gt_gripper[:,:3].to(torch.float32))\n",
    "gripper_orientation_loss = action_tokenizer.angle_loss(pred_gripper[:,3:6], gt_gripper[:,3:6].to(torch.float32))\n",
    "gripper_open_gt = gt_gripper[:,6].to(torch.int64)\n",
    "gripper_open_loss = F.cross_entropy(gripper_logits[:,6,-2:], gripper_open_gt)\n",
    "\n",
    "pred_object = action_tokenizer.decode(object_logits, soft = True)\n",
    "assert pred_object.shape == gt_object.shape, f\"Object shape {pred_object.shape} != {gt_object.shape}\"\n",
    "object_position_loss = F.mse_loss(pred_object[:,:3], gt_object[:,:3].to(torch.float32))\n",
    "\n",
    "#Target Loss\n",
    "pred_target = action_tokenizer.decode(target_logits, soft = True)\n",
    "assert pred_target.shape == gt_target.shape, f\"Target shape {pred_target.shape} != {gt_target.shape}\"\n",
    "target_position_loss = F.mse_loss(pred_target[:,:3], gt_target[:,:3].to(torch.float32))\n",
    "\n",
    "#Stage Loss\n",
    "gt_stage = gt_stage.clone().detach()\n",
    "stage_loss = F.cross_entropy(stage_logits, gt_stage)\n",
    "\n",
    "#Action Loss\n",
    "pred_action = action_tokenizer.decode(action_logits, soft = True)\n",
    "assert pred_action.shape == gt_action.shape, f\"Action shape {pred_action.shape} != {gt_action.shape}\"\n",
    "action_position_loss = F.mse_loss(pred_action[:,:3], gt_action[:,:3].to(torch.float32))\n",
    "action_orientation_loss = action_tokenizer.angle_loss(pred_action[:,3:6], gt_action[:,3:6].to(torch.float32))\n",
    "action_open_gt = gt_action[:,6].to(torch.int64)\n",
    "action_open_loss = F.cross_entropy(action_logits[:,6,-2:], action_open_gt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "/home/lawrence/anaconda3/lib/python3.11/site-packages/torch/utils/checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "step_idx, batch = next(enumerate(test_dataloader))\n",
    "\n",
    "test_loss_dict = {\n",
    "                \"nll_loss\": [],\n",
    "                \"gripper_position_loss\": [],\n",
    "                \"gripper_orientation_loss\": [],\n",
    "                \"gripper_open_loss\": [],\n",
    "                \"item_loss\": [],\n",
    "                \"object_position_loss\": [],\n",
    "                \"target_position_loss\": [],\n",
    "                \"stage_loss\": [],\n",
    "                \"action_position_loss\": [],\n",
    "                \"action_orientation_loss\": [],\n",
    "                \"action_open_loss\": [],\n",
    "            }\n",
    "with torch.no_grad(), torch.autocast(\"cuda\", dtype=torch.bfloat16):\n",
    "    output: CausalLMOutputWithPast = vla(\n",
    "        input_ids=batch[\"input_ids\"].to(device_id),\n",
    "        attention_mask=batch[\"attention_mask\"].to(device_id),\n",
    "        pixel_values=batch[\"pixel_values\"].to(torch.bfloat16).to(device_id),\n",
    "        labels=batch[\"labels\"],\n",
    "    )\n",
    "    output_start_idx = vla.vision_backbone.featurizer.patch_embed.num_patches\n",
    "    loss_dict = action_tokenizer.get_loss(output, batch, output_start_idx)\n",
    "    for k, v in loss_dict.items():\n",
    "        test_loss_dict[k].append(v)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
