{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lawrence/anaconda3/envs/VLA-RL/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "sys.path.append('.')\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "import draccus\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import tqdm\n",
    "import wandb\n",
    "from accelerate import PartialState\n",
    "from peft import LoraConfig, PeftModel, get_peft_model, prepare_model_for_kbit_training\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModelForVision2Seq, AutoProcessor, BitsAndBytesConfig\n",
    "from transformers.modeling_outputs import CausalLMOutputWithPast\n",
    "\n",
    "from finetune.finetune_config import FinetuneConfig\n",
    "from vla.base_prompter import PurePromptBuilder\n",
    "from vla.utils import PaddedCollatorForPosePrediction, runningLoss\n",
    "from vla.action_tokenizer import RLbenchPoseTokenizer\n",
    "from vla.dataset import save_dataset_statistics, RLbenchCotDataset\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class FinetuneConfig:\n",
    "    # fmt: off\n",
    "    vla_path: str = \"/media/lawrence/Work/checkpoints/ecot-openvla-7b-bridge\"   # Path to OpenVLA model \n",
    "    vla_path_q: str = \"/media/lawrence/Work/checkpoints/openvla-cot-4b\"   # Path to OpenVLA model\n",
    "\n",
    "    experiment_name: str = \"Weighted_loss_2\"\n",
    "    dataset_name: str = \"pick_described_object\"                                # Name of fine-tuning dataset (e.g., `droid_wipe`)\n",
    "    # data_path: Path = Path(f\"./datasets/{dataset_name}/data.pt\")\n",
    "    train_data_path: Path = Path(f\"./datasets/{dataset_name}/train_data.pt\")\n",
    "    test_data_path: Path = Path(f\"./datasets/{dataset_name}/test_data.pt\")\n",
    "    run_root_dir: Path = Path(\"./runs\")                               # Path to directory to store logs & checkpoints\n",
    "    adapter_dir: Path = Path(\"./adapter-tmp\")                     # Temporary directory for LoRA weights before fusing\n",
    "\n",
    "    # Fine-tuning Parameters\n",
    "    seed: int = 42                                                  # Random seed\n",
    "    episode: int = 1\n",
    "    batch_size: int = 2#16                                            # Fine-tuning batch size\n",
    "    test_batch_size: int = 2\n",
    "    test_limit_length: int = 30\n",
    "    save_steps: int = 20#5000                                          # Interval for checkpoint saving\n",
    "    learning_rate: float = 5e-5                                     # Fine-tuning learning rate\n",
    "    weight_decay: float = 0.01                                       # Fine-tuning weight decay\n",
    "    grad_accumulation_steps: int = 4                                # Gradient accumulation steps\n",
    "\n",
    "    # LoRA Arguments\n",
    "    use_lora: bool = True                                           # Whether to use LoRA fine-tuning\n",
    "    lora_rank: int = 8#32                                             # Rank of LoRA weight matrix\n",
    "    lora_dropout: float = 0.0                                       # Dropout applied to LoRA weights\n",
    "    use_quantization: bool = True                                  # Whether to 4-bit quantize VLA for LoRA fine-tuning\n",
    "                                                                    #   => CAUTION: Reduces memory but hurts performance\n",
    "    dataset_statistics: tuple = (np.array([-0.20173775, -0.36754665,  0.81396234, -3.14153998, -0.38798628, -3.14158631,  0. ]), np.array([0.41802976, 0.45118147, 1.47966564, 3.14159215, 0.30391057, 3.14157801, 1.])) # Min-Max normalization statistics\n",
    "\n",
    "    # Tracking Parameters\n",
    "    wandb_project: str = \"vla-rl\"                                  # Name of W&B project to log to (use default!)\n",
    "    wandb_entity: str = \"lawrence-rs-lin-university-of-toronto\"                           # Name of entity to log under\n",
    "\n",
    "    # fmt: on\n",
    "cfg = FinetuneConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning OpenVLA Model `/media/lawrence/Work/checkpoints/ecot-openvla-7b-bridge` on `pick_described_object`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "/home/lawrence/anaconda3/envs/VLA-RL/lib/python3.11/site-packages/transformers/quantizers/auto.py:159: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n",
      "  warnings.warn(warning_msg)\n",
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Fine-tuning OpenVLA Model `{cfg.vla_path}` on `{cfg.dataset_name}`\")\n",
    "\n",
    "# [Validate] Ensure GPU Available & Set Device / Distributed Context\n",
    "assert torch.cuda.is_available(), \"Fine-tuning assumes at least one GPU is available!\"\n",
    "distributed_state = PartialState()\n",
    "torch.cuda.set_device(device_id := distributed_state.local_process_index)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Configure Unique Experiment ID & Log Directory\n",
    "exp_id = (\n",
    "    f\"{cfg.vla_path.split('/')[-1]}+{cfg.dataset_name}\"\n",
    "    f\"+b{cfg.batch_size * cfg.grad_accumulation_steps}\"\n",
    "    f\"+lr-{cfg.learning_rate}\"\n",
    ")\n",
    "if cfg.use_lora:\n",
    "    exp_id += f\"+lora-r{cfg.lora_rank}+dropout-{cfg.lora_dropout}\"\n",
    "if cfg.use_quantization:\n",
    "    exp_id += \"+q-4bit\"\n",
    "\n",
    "# Start =>> Build Directories\n",
    "run_dir, adapter_dir = cfg.run_root_dir / exp_id, cfg.adapter_dir / exp_id\n",
    "os.makedirs(run_dir, exist_ok=True)\n",
    "\n",
    "# Quantization Config =>> only if LoRA fine-tuning\n",
    "quantization_config = None\n",
    "if cfg.use_quantization:\n",
    "    assert cfg.use_lora, \"Quantized training only supported for LoRA fine-tuning!\"\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True, bnb_4bit_compute_dtype=torch.bfloat16, bnb_4bit_quant_type=\"nf4\", #llm_int8_skip_modules = ['projector'],\n",
    "    )\n",
    "\n",
    "# Load OpenVLA Processor and Model using HF AutoClasses\n",
    "processor = AutoProcessor.from_pretrained(cfg.vla_path_q, trust_remote_code=True)\n",
    "vla = AutoModelForVision2Seq.from_pretrained(\n",
    "    cfg.vla_path_q,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    # attn_implementation=\"sdpa\",\n",
    "    quantization_config=quantization_config,\n",
    "    low_cpu_mem_usage=True,\n",
    "    trust_remote_code=True,\n",
    "    device_map = \"auto\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 24,400,992 || all params: 7,212,525,600 || trainable%: 0.3383\n"
     ]
    }
   ],
   "source": [
    "# Device Placement =>> note that BitsAndBytes automatically handles for quantized training\n",
    "if cfg.use_quantization:\n",
    "    vla = prepare_model_for_kbit_training(vla)\n",
    "else:\n",
    "    vla = vla.to(device_id)\n",
    "\n",
    "# [LoRA] Wrap Model w/ PEFT `LoraConfig` =>> by default we set `target_modules=all-linear`\n",
    "if cfg.use_lora:\n",
    "    lora_config = LoraConfig(\n",
    "        r=cfg.lora_rank,\n",
    "        lora_alpha=min(cfg.lora_rank, 16),\n",
    "        lora_dropout=cfg.lora_dropout,\n",
    "        target_modules=\"all-linear\",\n",
    "        init_lora_weights=\"gaussian\",\n",
    "    )\n",
    "    vla = get_peft_model(vla, lora_config)\n",
    "    vla.print_trainable_parameters()\n",
    "\n",
    "# # Wrap VLA in PyTorch DDP Wrapper for Multi-GPU Training\n",
    "# vla = DDP(vla, device_ids=[device_id], find_unused_parameters=True, gradient_as_bucket_view=True)\n",
    "\n",
    "# Create Optimizer =>> note that we default to a simple constant learning rate!\n",
    "trainable_params = [param for param in vla.parameters() if param.requires_grad]\n",
    "optimizer = AdamW(trainable_params, lr=cfg.learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Action Tokenizer\n",
    "action_tokenizer = RLbenchPoseTokenizer(processor.tokenizer, cfg.dataset_statistics)\n",
    "\n",
    "trainset = RLbenchCotDataset(\n",
    "    cfg.train_data_path,\n",
    "    action_tokenizer,\n",
    "    processor.tokenizer,\n",
    "    image_transform=processor.image_processor.apply_transform,\n",
    "    prompt_builder_fn=PurePromptBuilder,\n",
    ")\n",
    "\n",
    "testset = RLbenchCotDataset(\n",
    "    cfg.test_data_path,\n",
    "    action_tokenizer,\n",
    "    processor.tokenizer,\n",
    "    image_transform=processor.image_processor.apply_transform,\n",
    "    prompt_builder_fn=PurePromptBuilder,\n",
    ")\n",
    "\n",
    "# Create Collator and DataLoader\n",
    "collator = PaddedCollatorForPosePrediction(\n",
    "    processor.tokenizer.model_max_length, processor.tokenizer.pad_token_id, padding_side=\"right\"\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    trainset,\n",
    "    batch_size=cfg.batch_size,\n",
    "    sampler=None,\n",
    "    collate_fn=collator,\n",
    "    num_workers=1,  # Important =>> Set to 0 if using RLDS; TFDS rolls its own parallelism!\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    testset,\n",
    "    batch_size=4,\n",
    "    sampler=None,\n",
    "    collate_fn=collator,\n",
    "    num_workers=1,  # Important =>> Set to 0 if using RLDS; TFDS rolls its own parallelism!\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    }
   ],
   "source": [
    "vla.train()\n",
    "vla.gradient_checkpointing_enable()\n",
    "optimizer.zero_grad()\n",
    "\n",
    "step_idx, batch = next(enumerate(train_dataloader))\n",
    "\n",
    "with torch.autocast(\"cuda\", dtype=torch.bfloat16):\n",
    "    vla.train()\n",
    "    output: CausalLMOutputWithPast = vla(\n",
    "        input_ids=batch[\"input_ids\"].to(device_id),\n",
    "        attention_mask=batch[\"attention_mask\"].to(device_id),\n",
    "        pixel_values=batch[\"pixel_values\"].to(torch.bfloat16).to(device_id),\n",
    "        labels=batch[\"labels\"],\n",
    "    )\n",
    "    train_nll_loss = output.loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<s> \\n        In: What action should the robot take to pick up the chocolate jello and place in the basket?\\n        Out: \\nchocolate jello POSE:<object>密死ἡ</object>\\nBASKET POSE:<target>交宇ൾ</target>\\nGRIPPER POSE:<gripper>∣Մ；親터助给</gripper>\\nREASONING: Gripper haven't grasped the chocolate jello\\nSUBTASKS: 1.Move to the chocolate jello and pick it up. 2.Move over the basket. 3.Place the chocolate jello in the basket.\\nCURRENT STEP: Move to the chocolate jello and pick it up.\\nACTION:<action>♀세展ữ菜ន弘</action>\\n</s>\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.tokenizer.decode(batch[\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Accuracy and L1 Loss for Logging\n",
    "output_logits = output.logits[:, vla.vision_backbone.featurizer.patch_embed.num_patches:-1]\n",
    "output_gt = batch[\"labels\"][:, 1:].to(device_id)\n",
    "action_mask, gripper_mask, object_mask, target_mask = action_tokenizer.get_mask(output_gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_logits = output_logits[action_mask][:,action_tokenizer.action_token_begin_idx:processor.tokenizer.vocab_size].view(cfg.batch_size,-1,action_tokenizer.n_bins)\n",
    "gripper_logits = output_logits[gripper_mask][:,action_tokenizer.action_token_begin_idx:processor.tokenizer.vocab_size].view(cfg.batch_size,-1,action_tokenizer.n_bins)\n",
    "object_logits = output_logits[object_mask][:,action_tokenizer.action_token_begin_idx:processor.tokenizer.vocab_size].view(cfg.batch_size,-1,action_tokenizer.n_bins)\n",
    "target_logits = output_logits[target_mask][:,action_tokenizer.action_token_begin_idx:processor.tokenizer.vocab_size].view(cfg.batch_size,-1,action_tokenizer.n_bins)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_running_loss = runningLoss()\n",
    "scaler = torch.cuda.amp.GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [1, 29871, 31430, 31542, 31598, 31747, 31854, 31948, 31999], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_action = torch.tensor([0, 0, 0.5, 0, 0, 0, 1])\n",
    "processor.tokenizer(action_tokenizer(test_action))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = torch.tensor([31448, 31548, 31598, 31748, 31848, 31948, 31998]) - action_tokenizer.action_token_begin_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = torch.zeros(1,7,action_tokenizer.n_bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1112,  0.0459,  0.8173,  0.0314, -0.0386,  0.0314,  0.0000]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_tokenizer.decode(logits.scatter_(2, idx.unsqueeze(0).unsqueeze(-1), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "runningLoss.update() takes 10 positional arguments but 11 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 32\u001b[0m\n\u001b[1;32m     29\u001b[0m action_open_gt \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(action_logits[:,\u001b[38;5;241m6\u001b[39m,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m:])\u001b[38;5;241m.\u001b[39mscatter_(\u001b[38;5;241m1\u001b[39m, gt_action[:,\u001b[38;5;241m6\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mint64), \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     30\u001b[0m action_open_loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mbinary_cross_entropy_with_logits(action_logits[:,\u001b[38;5;241m6\u001b[39m,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m:], action_open_gt\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32))\n\u001b[0;32m---> 32\u001b[0m normalized_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_running_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_nll_loss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobject_position_loss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobject_orientation_loss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_position_loss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgripper_position_loss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgripper_orientation_loss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgripper_open_loss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43maction_position_loss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43maction_orientation_loss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43maction_open_loss\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: runningLoss.update() takes 10 positional arguments but 11 were given"
     ]
    }
   ],
   "source": [
    "# object\n",
    "pred_object = action_tokenizer.decode(object_logits, soft = True)\n",
    "gt_object = batch['target_item_poses'].to(device_id)\n",
    "assert pred_object.shape == gt_object.shape, f\"Object shape {pred_object.shape} != {gt_object.shape}\"\n",
    "object_position_loss = F.mse_loss(pred_object[:,:3], gt_object[:,:3].to(torch.float32))\n",
    "object_orientation_loss = F.mse_loss(pred_object[:,3:], gt_object[:,3:].to(torch.float32))\n",
    "\n",
    "# target\n",
    "pred_target = action_tokenizer.decode(target_logits, soft = True)\n",
    "gt_target = batch['basket_positions'].to(device_id)\n",
    "assert pred_target.shape == gt_target.shape, f\"Target shape {pred_target.shape} != {gt_target.shape}\"\n",
    "target_position_loss = F.mse_loss(pred_target[:,:3], gt_target[:,:3].to(torch.float32))\n",
    "\n",
    "# gripper\n",
    "pred_gripper = action_tokenizer.decode(gripper_logits, soft = True)\n",
    "gt_gripper = batch['gripper_poses'].to(device_id)\n",
    "assert pred_gripper.shape == gt_gripper.shape, f\"Gripper shape {pred_gripper.shape} != {gt_gripper.shape}\"\n",
    "gripper_position_loss = F.mse_loss(pred_gripper[:,:3], gt_gripper[:,:3].to(torch.float32))\n",
    "gripper_orientation_loss = F.mse_loss(pred_gripper[:,3:6], gt_gripper[:,3:6].to(torch.float32))\n",
    "gripper_open_gt = torch.zeros_like(gripper_logits[:,6,-2:]).scatter_(1, gt_gripper[:,6].unsqueeze(1).to(torch.int64), 1)\n",
    "gripper_open_loss = F.binary_cross_entropy_with_logits(gripper_logits[:,6,-2:], gripper_open_gt.to(torch.float32))\n",
    "\n",
    "#action\n",
    "pred_action = action_tokenizer.decode(action_logits, soft = True)\n",
    "gt_action = batch['actions'].to(device_id)\n",
    "assert pred_action.shape == gt_action.shape, f\"Action shape {pred_action.shape} != {gt_action.shape}\"\n",
    "action_position_loss = F.mse_loss(pred_action[:,:3], gt_action[:,:3].to(torch.float32))\n",
    "action_orientation_loss = F.mse_loss(pred_action[:,3:6], gt_action[:,3:6].to(torch.float32))\n",
    "action_open_gt = torch.zeros_like(action_logits[:,6,-2:]).scatter_(1, gt_action[:,6].unsqueeze(1).to(torch.int64), 1)\n",
    "action_open_loss = F.binary_cross_entropy_with_logits(action_logits[:,6,-2:], action_open_gt.to(torch.float32))\n",
    "\n",
    "normalized_loss = train_running_loss.update(\n",
    "    train_nll_loss,\n",
    "    object_position_loss,\n",
    "    object_orientation_loss,\n",
    "    target_position_loss,\n",
    "    gripper_position_loss,\n",
    "    gripper_orientation_loss,\n",
    "    gripper_open_loss,\n",
    "    action_position_loss,\n",
    "    action_orientation_loss,\n",
    "    action_open_loss\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.77499008178711"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gripper_orientation_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1., device='cuda:0', grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_loss['action_open_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_loss['total_loss'].backward(retain_graph=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vla.eval()\n",
    "# best_test_loss = float(\"inf\")\n",
    "# test_loss = 0\n",
    "# test_nll_loss = []\n",
    "# test_object_position_loss = []\n",
    "# test_object_orientation_loss = []\n",
    "# test_target_position_loss = []\n",
    "# test_gripper_position_loss = []\n",
    "# test_gripper_orientation_loss = []\n",
    "# test_gripper_open_loss = []\n",
    "# test_action_position_loss = []\n",
    "# test_action_orientation_loss = []\n",
    "# test_action_open_loss = []\n",
    "# for batch in test_dataloader:\n",
    "#     with torch.no_grad(), torch.autocast(\"cuda\", dtype=torch.bfloat16):\n",
    "#         output: CausalLMOutputWithPast = vla(\n",
    "#             input_ids=batch[\"input_ids\"].to(device_id),\n",
    "#             attention_mask=batch[\"attention_mask\"].to(device_id),\n",
    "#             pixel_values=batch[\"pixel_values\"].to(torch.bfloat16).to(device_id),\n",
    "#             labels=batch[\"labels\"],\n",
    "#         )\n",
    "#         test_nll_loss_ = output.loss\n",
    "#         test_nll_loss.append(test_nll_loss_)\n",
    "\n",
    "#         output_logits = output.logits[:, vla.vision_backbone.featurizer.patch_embed.num_patches:-1]\n",
    "#         output_gt = batch[\"labels\"][:, 1:].to(device_id)\n",
    "#         action_mask, gripper_mask, object_mask, target_mask = action_tokenizer.get_mask(output_gt)\n",
    "\n",
    "#         action_logits = output_logits[action_mask][:,action_tokenizer.action_token_begin_idx:processor.tokenizer.vocab_size].view(4,-1,action_tokenizer.n_bins)\n",
    "#         gripper_logits = output_logits[gripper_mask][:,action_tokenizer.action_token_begin_idx:processor.tokenizer.vocab_size].view(4,-1,action_tokenizer.n_bins)\n",
    "#         object_logits = output_logits[object_mask][:,action_tokenizer.action_token_begin_idx:processor.tokenizer.vocab_size].view(4,-1,action_tokenizer.n_bins)\n",
    "#         target_logits = output_logits[target_mask][:,action_tokenizer.action_token_begin_idx:processor.tokenizer.vocab_size].view(4,-1,action_tokenizer.n_bins)\n",
    "\n",
    "#         # object\n",
    "#         pred_object = action_tokenizer.decode(object_logits, soft = True)\n",
    "#         gt_object = batch['target_item_poses'].to(device_id)\n",
    "#         assert pred_object.shape == gt_object.shape, f\"Object shape {pred_object.shape} != {gt_object.shape}\"\n",
    "#         object_position_loss = F.mse_loss(pred_object[:,:3], gt_object[:,:3])\n",
    "#         object_orientation_loss = F.mse_loss(pred_object[:,3:], gt_object[:,3:])\n",
    "#         test_object_position_loss.append(object_position_loss)\n",
    "#         test_object_orientation_loss.append(object_orientation_loss)\n",
    "\n",
    "#         # target\n",
    "#         pred_target = action_tokenizer.decode(target_logits, soft = True)\n",
    "#         gt_target = batch['basket_positions'].to(device_id)\n",
    "#         assert pred_target.shape == gt_target.shape, f\"Target shape {pred_target.shape} != {gt_target.shape}\"\n",
    "#         target_position_loss = F.mse_loss(pred_target[:,:3], gt_target[:,:3])\n",
    "#         test_target_position_loss.append(target_position_loss)\n",
    "\n",
    "#         # gripper\n",
    "#         pred_gripper = action_tokenizer.decode(gripper_logits, soft = True)\n",
    "#         gt_gripper = batch['gripper_poses'].to(device_id)\n",
    "#         assert pred_gripper.shape == gt_gripper.shape, f\"Gripper shape {pred_gripper.shape} != {gt_gripper.shape}\"\n",
    "#         gripper_position_loss = F.mse_loss(pred_gripper[:,:3], gt_gripper[:,:3])\n",
    "#         gripper_orientation_loss = F.mse_loss(pred_gripper[:,3:6], gt_gripper[:,3:6])\n",
    "#         gripper_open_loss = F.mse_loss(pred_gripper[:,6], gt_gripper[:,6])\n",
    "#         test_gripper_position_loss.append(gripper_position_loss)\n",
    "#         test_gripper_orientation_loss.append(gripper_orientation_loss)\n",
    "#         test_gripper_open_loss.append(gripper_open_loss)\n",
    "\n",
    "#         #action\n",
    "#         pred_action = action_tokenizer.decode(action_logits, soft = True)\n",
    "#         gt_action = batch['actions'].to(device_id)\n",
    "#         assert pred_action.shape == gt_action.shape, f\"Action shape {pred_action.shape} != {gt_action.shape}\"\n",
    "#         action_position_loss = F.mse_loss(pred_action[:,:3], gt_action[:,:3])\n",
    "#         action_orientation_loss = F.mse_loss(pred_action[:,3:6], gt_action[:,3:6])\n",
    "#         action_open_loss = F.mse_loss(pred_action[:,6], gt_action[:,6])\n",
    "#         test_action_position_loss.append(action_position_loss)\n",
    "#         test_action_orientation_loss.append(action_orientation_loss)\n",
    "#         test_action_open_loss.append(action_open_loss)\n",
    "\n",
    "#         test_nll_loss = torch.stack(test_nll_loss).mean()\n",
    "#         test_object_position_loss = torch.stack(test_object_position_loss).mean()\n",
    "#         test_object_orientation_loss = torch.stack(test_object_orientation_loss).mean()\n",
    "#         test_target_position_loss = torch.stack(test_target_position_loss).mean()\n",
    "#         test_gripper_position_loss = torch.stack(test_gripper_position_loss).mean()\n",
    "#         test_gripper_orientation_loss = torch.stack(test_gripper_orientation_loss).mean()\n",
    "#         test_gripper_open_loss = torch.stack(test_gripper_open_loss).mean()\n",
    "#         test_action_position_loss = torch.stack(test_action_position_loss).mean()\n",
    "#         test_action_orientation_loss = torch.stack(test_action_orientation_loss).mean()\n",
    "#         test_action_open_loss = torch.stack(test_action_open_loss).mean()\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
