{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import tqdm\n",
    "import wandb\n",
    "from accelerate import PartialState\n",
    "from peft import LoraConfig, PeftModel, get_peft_model, prepare_model_for_kbit_training, PeftConfig\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModelForVision2Seq, AutoProcessor, BitsAndBytesConfig\n",
    "from transformers.modeling_outputs import CausalLMOutputWithPast\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "from vla.base_prompter import PurePromptBuilder\n",
    "from vla.utils import PaddedCollatorForPosePrediction, runningLoss\n",
    "from vla.action_tokenizer import RLbenchPoseTokenizer\n",
    "from vla.dataset import RLbenchCotDataset\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_path = \"/media/lawrence/Work/checkpoints/ecot-openvla-7b-bridge\"\n",
    "adapter_path = \"adapter-tmp/1_sample+nll+pick_described_object+e1+b8+lr-0.0001+lora-r16+dropout-0.0+q-4bit\"\n",
    "adapter_path1 = \"adapter-tmp/weighted_loss_cot_1+nll+pick_described_object2+e1+b8+lr-0.0001+lora-r16+dropout-0.0+q-4bit\"\n",
    "data_path = \"datasets/pick_described_object/train_data.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:37<00:00, 12.41s/it]\n"
     ]
    }
   ],
   "source": [
    "quantization_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True, bnb_4bit_compute_dtype=torch.bfloat16, bnb_4bit_quant_type=\"nf4\", #llm_int8_skip_modules = ['projector'],\n",
    "        )\n",
    "base_model = AutoModelForVision2Seq.from_pretrained(\n",
    "        base_model_path,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        attn_implementation=\"sdpa\",\n",
    "        quantization_config=quantization_config,\n",
    "        low_cpu_mem_usage=True,\n",
    "        trust_remote_code=True,\n",
    "        device_map = \"cuda\"\n",
    "    )\n",
    "base_model = prepare_model_for_kbit_training(base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_num = 5\n",
    "stage_num = 2 \n",
    "add_tokens = ['<g>', '</g>'] + [f'<item_{i}>' for i in np.arange(item_num)] + ['<o>', '</o>', '<t>', '</t>'] + [f'<stage_{i}>' for i in np.arange(stage_num)] + ['<a>', '</a>']\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(base_model_path, trust_remote_code=True)\n",
    "processor.tokenizer.add_tokens(add_tokens)\n",
    "dataset_statistics: tuple = (np.array([-0.2, -0.35,  0.75199986, -np.pi/2, -np.pi/2, -np.pi/2,  0. ]), np.array([0.5, 0.35, 1.3, np.pi/2, 0, np.pi/2, 1.])) # Min-Max normalization statistics\n",
    "\n",
    "action_tokenizer = RLbenchPoseTokenizer(processor.tokenizer, dataset_statistics)\n",
    "trainset = RLbenchCotDataset(\n",
    "    data_path,\n",
    "    action_tokenizer,\n",
    "    processor.tokenizer,\n",
    "    image_transform=processor.image_processor.apply_transform,\n",
    ")\n",
    "\n",
    "collator = PaddedCollatorForPosePrediction(\n",
    "    processor.tokenizer.model_max_length, processor.tokenizer.pad_token_id, padding_side=\"right\"\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    trainset,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    sampler=None,\n",
    "    collate_fn=collator,\n",
    "    num_workers=1,  # Important =>> Set to 0 if using RLDS; TFDS rolls its own parallelism!\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "adapter_config = LoraConfig.from_pretrained(adapter_path)\n",
    "adapter_config.inference_mode = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vla = get_peft_model(base_model, adapter_config, adapter_name = \"actor_critic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<q><PAD></s>'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"<q><PAD></s>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.tokenizer.add_tokens([\"<q>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [1, 1919, 32016, 32000, 2], 'attention_mask': [1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.tokenizer(\", <q><PAD></s>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 48,801,984 || all params: 7,236,926,592 || trainable%: 0.6743\n"
     ]
    }
   ],
   "source": [
    "vla.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_instruct_prompt(action_tokenizer, gripper, instruction: str):\n",
    "    # gripper = action_tokenizer(gripper)\n",
    "    prompt = (\n",
    "        \"In: What the next key pose of gripper should the robot take to {instruction}? Current pose is <g>{gripper} </g>, let's think step by step.\\n \"\n",
    "        \"Out: \"\n",
    "    )\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['pixel_values', 'input_ids', 'attention_mask', 'labels', 'grippers', 'items', 'objects', 'targets', 'stages', 'actions'])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image.fromarray(batch['pixel_values'].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    1,   512, 29901,  1724,   278,  2446,  1820, 18593,   310,   330,\n",
       "          374,  2496,   881,   278, 19964,  2125,   304,   426,  2611,  4080,\n",
       "        29913, 29973,  9626, 18593,   338, 32001, 29912, 29887,   374,  2496,\n",
       "        29913, 32002, 29892,  1235, 29915, 29879,  1348,  4331,   491,  4331,\n",
       "        29889,    13,  4451, 29901, 29871])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(processor.tokenizer(get_instruct_prompt(None,1,1)).input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    1,   512, 29901,  1724,   881,   367,   278,  2446,  1820, 18593,\n",
       "           310,   278,   330,   374,  2496,   304,  4337,   278, 26438,  3800,\n",
       "           304,   278, 25972, 29973,   450,  1857,   330,   374,  2496, 18593,\n",
       "           338, 32001, 31417, 31501, 31668, 31747, 31888, 31995, 31998,   829,\n",
       "         29887, 15513,  2803, 29915, 29879,  1348,  4331,   491,  4331, 29889,\n",
       "            13,  4451, 29901, 32007, 29892, 32008, 31437, 31588, 31613, 32009,\n",
       "         29892, 32010, 31398, 31501, 31643, 32011, 29892, 32012, 29892, 32014,\n",
       "         31417, 31501, 31668, 31747, 31888, 31995, 31999, 32015,     2]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['input_ids'][:,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = get_instruct_prompt(gripper,instr)\n",
    "image = Image.fromarray(obs.front_rgb)\n",
    "inputs = processor(prompt, image).to(vla.device, dtype=torch.bfloat16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "vla.set_adapter('actor_critic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.load_adapter(adapter_path1, adapter_name= \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test']"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model.active_adapters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad(), torch.autocast(\"cuda\", dtype=torch.bfloat16):\n",
    "    output = base_model.generate(\n",
    "        input_ids = batch['input_ids'][:,:-26].to(vla.device),\n",
    "        pixel_values=batch[\"pixel_values\"].to(torch.bfloat16).to(vla.device),\n",
    "        max_new_tokens = 40\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    1,   512, 29901,  1724,   881,   367,   278,  2446,  1820,\n",
       "        18593,   310,   278,   330,   374,  2496,   304,  4337,   278,\n",
       "        26438,  3800,   304,   278, 25972, 29973,   450,  1857,   330,\n",
       "          374,  2496, 18593,   338, 32001, 31417, 31501, 31668, 31747,\n",
       "        31888, 31995, 31998,   829, 29887, 15513,  2803, 29915, 29879,\n",
       "         1348,  4331,   491,  4331, 29889,    13,  4451, 29901, 32001,\n",
       "        31530, 31501, 31742, 31772, 31876, 31958, 31999, 32002, 29892,\n",
       "        32007, 29892, 32008, 31474, 31613, 32009, 29892, 32010, 31398,\n",
       "        31501, 31643, 32011, 29892, 32012, 29892, 32014, 31474, 31613,\n",
       "        31625, 31748, 31897, 31997, 31998, 32015,     2]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<s> In: What should be the next key pose of the gripper to move the sugar box to the basket? The current gripper pose is<g>마ペή면项黃弘</g>. Let's think step by step.\\n Out:<g>马ペĖₗ頭ヨ给</g>,<item_4>,<o>意态</o>,<t>交ペམ</t>,<stage_0>,<a>意态ൾķḳ收弘</a></s>\""
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.tokenizer.decode(output[0].cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vla = PeftModel.from_pretrained(base_model, adapter_path, is_trainable=True, config=adapter_config, device_map=\"cuda\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VLA-RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
