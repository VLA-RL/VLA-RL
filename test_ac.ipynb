{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-04 15:29:54.592518: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-08-04 15:29:54.592580: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-08-04 15:29:54.594420: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-08-04 15:29:54.602413: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-04 15:29:55.734254: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import tqdm\n",
    "import wandb\n",
    "from accelerate import PartialState\n",
    "from peft import LoraConfig, PeftModel, get_peft_model, prepare_model_for_kbit_training, PeftConfig\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModelForVision2Seq, AutoProcessor, BitsAndBytesConfig\n",
    "from transformers.modeling_outputs import CausalLMOutputWithPast\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "from vla.base_prompter import PurePromptBuilder\n",
    "from vla.utils import PaddedCollatorForPosePrediction, runningLoss\n",
    "from vla.action_tokenizer import RLbenchPoseTokenizer\n",
    "from vla.dataset import RLbenchCotDataset\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "from rlbench.action_modes.action_mode import MoveArmThenGripper\n",
    "from rlbench.action_modes.arm_action_modes import ArmActionMode, JointVelocity, JointPosition, EndEffectorPoseViaPlanning, EndEffectorPoseViaIK\n",
    "\n",
    "\n",
    "from rlbench.action_modes.gripper_action_modes import Discrete\n",
    "from rlbench.environment import Environment\n",
    "from rlbench.observation_config import ObservationConfig, CameraConfig\n",
    "# from rlbench.tasks.pick_described_object import PickDescribedObject\n",
    "from rlbench.tasks import PutGroceriesInCupboard, PickAndLift, StackBlocks, PlaceHangerOnRack, PickDescribedObject, TakeLidOffSaucepan, SetTheTable, PutGroceriesInCupboard\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "from pyrep.const import RenderMode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_path = \"/media/lawrence/Work/checkpoints/ecot-openvla-7b-bridge\"\n",
    "adapter_path = \"adapter-tmp/2_sample_data_q+nll+pick_described_object+e1+b8+lr-0.0001+lora-r16+dropout-0.0+q-4bit\"\n",
    "adapter_path1 = \"adapter-tmp/weighted_loss_cot_1+nll+pick_described_object2+e1+b8+lr-0.0001+lora-r16+dropout-0.0+q-4bit\"\n",
    "adapter_path2 = \"adapter-tmp/weighted_loss+weighted+pick_described_object+e1+b8+lr-0.0005+lora-r16+dropout-0.0+q-4bit\"\n",
    "data_path = \"datasets/pick_described_object/train_data.pt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:31<00:00, 10.66s/it]\n"
     ]
    }
   ],
   "source": [
    "quantization_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True, bnb_4bit_compute_dtype=torch.bfloat16, bnb_4bit_quant_type=\"nf4\", #llm_int8_skip_modules = ['projector'],\n",
    "        )\n",
    "base_model = AutoModelForVision2Seq.from_pretrained(\n",
    "        base_model_path,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        attn_implementation=\"sdpa\",\n",
    "        quantization_config=quantization_config,\n",
    "        low_cpu_mem_usage=True,\n",
    "        trust_remote_code=True,\n",
    "        device_map = \"cuda\"\n",
    "    )\n",
    "base_model = prepare_model_for_kbit_training(base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_num = 5\n",
    "stage_num = 2 \n",
    "add_tokens = ['<g>', '</g>'] + [f'<item_{i}>' for i in np.arange(item_num)] + ['<o>', '</o>', '<t>', '</t>'] + [f'<stage_{i}>' for i in np.arange(stage_num)] + ['<a>', '</a>', '<q>', '<cot>']\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(base_model_path, trust_remote_code=True)\n",
    "processor.tokenizer.add_tokens(add_tokens)\n",
    "dataset_statistics: tuple = (np.array([-0.2, -0.35,  0.75199986, -np.pi/2, -np.pi/2, -np.pi/2,  0. ]), np.array([0.5, 0.35, 1.3, np.pi/2, 0, np.pi/2, 1.])) # Min-Max normalization statistics\n",
    "\n",
    "action_tokenizer = RLbenchPoseTokenizer(processor.tokenizer, dataset_statistics)\n",
    "trainset = RLbenchCotDataset(\n",
    "    data_path,\n",
    "    action_tokenizer,\n",
    "    processor.tokenizer,\n",
    "    image_transform=processor.image_processor.apply_transform,\n",
    ")\n",
    "\n",
    "collator = PaddedCollatorForPosePrediction(\n",
    "    processor.tokenizer.model_max_length, processor.tokenizer.pad_token_id, padding_side=\"right\"\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    trainset,\n",
    "    batch_size=2,\n",
    "    shuffle=False,\n",
    "    sampler=None,\n",
    "    collate_fn=collator,\n",
    "    num_workers=1,  # Important =>> Set to 0 if using RLDS; TFDS rolls its own parallelism!\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=['base_model.model.vision_backbone.featurizer.pos_embed', 'base_model.model.vision_backbone.featurizer.patch_embed.proj.base_layer.weight', 'base_model.model.vision_backbone.featurizer.patch_embed.proj.base_layer.bias', 'base_model.model.vision_backbone.featurizer.patch_embed.proj.lora_A.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.patch_embed.proj.lora_A.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.patch_embed.proj.lora_B.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.patch_embed.proj.lora_B.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.0.norm1.weight', 'base_model.model.vision_backbone.featurizer.blocks.0.norm1.bias', 'base_model.model.vision_backbone.featurizer.blocks.0.attn.qkv.base_layer.weight', 'base_model.model.vision_backbone.featurizer.blocks.0.attn.qkv.base_layer.bias', 'base_model.model.vision_backbone.featurizer.blocks.0.attn.qkv.lora_A.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.0.attn.qkv.lora_A.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.0.attn.qkv.lora_B.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.0.attn.qkv.lora_B.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.0.attn.proj.base_layer.weight', 'base_model.model.vision_backbone.featurizer.blocks.0.attn.proj.base_layer.bias', 'base_model.model.vision_backbone.featurizer.blocks.0.attn.proj.lora_A.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.0.attn.proj.lora_A.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.0.attn.proj.lora_B.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.0.attn.proj.lora_B.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.0.norm2.weight', 'base_model.model.vision_backbone.featurizer.blocks.0.norm2.bias', 'base_model.model.vision_backbone.featurizer.blocks.0.mlp.fc1.base_layer.weight', 'base_model.model.vision_backbone.featurizer.blocks.0.mlp.fc1.base_layer.bias', 'base_model.model.vision_backbone.featurizer.blocks.0.mlp.fc1.lora_A.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.0.mlp.fc1.lora_A.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.0.mlp.fc1.lora_B.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.0.mlp.fc1.lora_B.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.0.mlp.fc2.base_layer.weight', 'base_model.model.vision_backbone.featurizer.blocks.0.mlp.fc2.base_layer.bias', 'base_model.model.vision_backbone.featurizer.blocks.0.mlp.fc2.lora_A.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.0.mlp.fc2.lora_A.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.0.mlp.fc2.lora_B.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.0.mlp.fc2.lora_B.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.1.norm1.weight', 'base_model.model.vision_backbone.featurizer.blocks.1.norm1.bias', 'base_model.model.vision_backbone.featurizer.blocks.1.attn.qkv.base_layer.weight', 'base_model.model.vision_backbone.featurizer.blocks.1.attn.qkv.base_layer.bias', 'base_model.model.vision_backbone.featurizer.blocks.1.attn.qkv.lora_A.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.1.attn.qkv.lora_A.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.1.attn.qkv.lora_B.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.1.attn.qkv.lora_B.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.1.attn.proj.base_layer.weight', 'base_model.model.vision_backbone.featurizer.blocks.1.attn.proj.base_layer.bias', 'base_model.model.vision_backbone.featurizer.blocks.1.attn.proj.lora_A.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.1.attn.proj.lora_A.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.1.attn.proj.lora_B.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.1.attn.proj.lora_B.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.1.norm2.weight', 'base_model.model.vision_backbone.featurizer.blocks.1.norm2.bias', 'base_model.model.vision_backbone.featurizer.blocks.1.mlp.fc1.base_layer.weight', 'base_model.model.vision_backbone.featurizer.blocks.1.mlp.fc1.base_layer.bias', 'base_model.model.vision_backbone.featurizer.blocks.1.mlp.fc1.lora_A.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.1.mlp.fc1.lora_A.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.1.mlp.fc1.lora_B.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.1.mlp.fc1.lora_B.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.1.mlp.fc2.base_layer.weight', 'base_model.model.vision_backbone.featurizer.blocks.1.mlp.fc2.base_layer.bias', 'base_model.model.vision_backbone.featurizer.blocks.1.mlp.fc2.lora_A.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.1.mlp.fc2.lora_A.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.1.mlp.fc2.lora_B.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.1.mlp.fc2.lora_B.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.2.norm1.weight', 'base_model.model.vision_backbone.featurizer.blocks.2.norm1.bias', 'base_model.model.vision_backbone.featurizer.blocks.2.attn.qkv.base_layer.weight', 'base_model.model.vision_backbone.featurizer.blocks.2.attn.qkv.base_layer.bias', 'base_model.model.vision_backbone.featurizer.blocks.2.attn.qkv.lora_A.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.2.attn.qkv.lora_A.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.2.attn.qkv.lora_B.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.2.attn.qkv.lora_B.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.2.attn.proj.base_layer.weight', 'base_model.model.vision_backbone.featurizer.blocks.2.attn.proj.base_layer.bias', 'base_model.model.vision_backbone.featurizer.blocks.2.attn.proj.lora_A.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.2.attn.proj.lora_A.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.2.attn.proj.lora_B.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.2.attn.proj.lora_B.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.2.norm2.weight', 'base_model.model.vision_backbone.featurizer.blocks.2.norm2.bias', 'base_model.model.vision_backbone.featurizer.blocks.2.mlp.fc1.base_layer.weight', 'base_model.model.vision_backbone.featurizer.blocks.2.mlp.fc1.base_layer.bias', 'base_model.model.vision_backbone.featurizer.blocks.2.mlp.fc1.lora_A.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.2.mlp.fc1.lora_A.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.2.mlp.fc1.lora_B.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.2.mlp.fc1.lora_B.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.2.mlp.fc2.base_layer.weight', 'base_model.model.vision_backbone.featurizer.blocks.2.mlp.fc2.base_layer.bias', 'base_model.model.vision_backbone.featurizer.blocks.2.mlp.fc2.lora_A.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.2.mlp.fc2.lora_A.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.2.mlp.fc2.lora_B.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.2.mlp.fc2.lora_B.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.3.norm1.weight', 'base_model.model.vision_backbone.featurizer.blocks.3.norm1.bias', 'base_model.model.vision_backbone.featurizer.blocks.3.attn.qkv.base_layer.weight', 'base_model.model.vision_backbone.featurizer.blocks.3.attn.qkv.base_layer.bias', 'base_model.model.vision_backbone.featurizer.blocks.3.attn.qkv.lora_A.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.3.attn.qkv.lora_A.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.3.attn.qkv.lora_B.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.3.attn.qkv.lora_B.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.3.attn.proj.base_layer.weight', 'base_model.model.vision_backbone.featurizer.blocks.3.attn.proj.base_layer.bias', 'base_model.model.vision_backbone.featurizer.blocks.3.attn.proj.lora_A.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.3.attn.proj.lora_A.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.3.attn.proj.lora_B.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.3.attn.proj.lora_B.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.3.norm2.weight', 'base_model.model.vision_backbone.featurizer.blocks.3.norm2.bias', 'base_model.model.vision_backbone.featurizer.blocks.3.mlp.fc1.base_layer.weight', 'base_model.model.vision_backbone.featurizer.blocks.3.mlp.fc1.base_layer.bias', 'base_model.model.vision_backbone.featurizer.blocks.3.mlp.fc1.lora_A.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.3.mlp.fc1.lora_A.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.3.mlp.fc1.lora_B.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.3.mlp.fc1.lora_B.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.3.mlp.fc2.base_layer.weight', 'base_model.model.vision_backbone.featurizer.blocks.3.mlp.fc2.base_layer.bias', 'base_model.model.vision_backbone.featurizer.blocks.3.mlp.fc2.lora_A.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.3.mlp.fc2.lora_A.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.3.mlp.fc2.lora_B.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.3.mlp.fc2.lora_B.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.4.norm1.weight', 'base_model.model.vision_backbone.featurizer.blocks.4.norm1.bias', 'base_model.model.vision_backbone.featurizer.blocks.4.attn.qkv.base_layer.weight', 'base_model.model.vision_backbone.featurizer.blocks.4.attn.qkv.base_layer.bias', 'base_model.model.vision_backbone.featurizer.blocks.4.attn.qkv.lora_A.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.4.attn.qkv.lora_A.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.4.attn.qkv.lora_B.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.4.attn.qkv.lora_B.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.4.attn.proj.base_layer.weight', 'base_model.model.vision_backbone.featurizer.blocks.4.attn.proj.base_layer.bias', 'base_model.model.vision_backbone.featurizer.blocks.4.attn.proj.lora_A.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.4.attn.proj.lora_A.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.4.attn.proj.lora_B.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.4.attn.proj.lora_B.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.4.norm2.weight', 'base_model.model.vision_backbone.featurizer.blocks.4.norm2.bias', 'base_model.model.vision_backbone.featurizer.blocks.4.mlp.fc1.base_layer.weight', 'base_model.model.vision_backbone.featurizer.blocks.4.mlp.fc1.base_layer.bias', 'base_model.model.vision_backbone.featurizer.blocks.4.mlp.fc1.lora_A.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.4.mlp.fc1.lora_A.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.4.mlp.fc1.lora_B.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.4.mlp.fc1.lora_B.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.4.mlp.fc2.base_layer.weight', 'base_model.model.vision_backbone.featurizer.blocks.4.mlp.fc2.base_layer.bias', 'base_model.model.vision_backbone.featurizer.blocks.4.mlp.fc2.lora_A.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.4.mlp.fc2.lora_A.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.4.mlp.fc2.lora_B.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.4.mlp.fc2.lora_B.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.5.norm1.weight', 'base_model.model.vision_backbone.featurizer.blocks.5.norm1.bias', 'base_model.model.vision_backbone.featurizer.blocks.5.attn.qkv.base_layer.weight', 'base_model.model.vision_backbone.featurizer.blocks.5.attn.qkv.base_layer.bias', 'base_model.model.vision_backbone.featurizer.blocks.5.attn.qkv.lora_A.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.5.attn.qkv.lora_A.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.5.attn.qkv.lora_B.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.5.attn.qkv.lora_B.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.5.attn.proj.base_layer.weight', 'base_model.model.vision_backbone.featurizer.blocks.5.attn.proj.base_layer.bias', 'base_model.model.vision_backbone.featurizer.blocks.5.attn.proj.lora_A.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.5.attn.proj.lora_A.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.5.attn.proj.lora_B.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.5.attn.proj.lora_B.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.5.norm2.weight', 'base_model.model.vision_backbone.featurizer.blocks.5.norm2.bias', 'base_model.model.vision_backbone.featurizer.blocks.5.mlp.fc1.base_layer.weight', 'base_model.model.vision_backbone.featurizer.blocks.5.mlp.fc1.base_layer.bias', 'base_model.model.vision_backbone.featurizer.blocks.5.mlp.fc1.lora_A.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.5.mlp.fc1.lora_A.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.5.mlp.fc1.lora_B.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.5.mlp.fc1.lora_B.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.5.mlp.fc2.base_layer.weight', 'base_model.model.vision_backbone.featurizer.blocks.5.mlp.fc2.base_layer.bias', 'base_model.model.vision_backbone.featurizer.blocks.5.mlp.fc2.lora_A.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.5.mlp.fc2.lora_A.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.5.mlp.fc2.lora_B.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.5.mlp.fc2.lora_B.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.6.norm1.weight', 'base_model.model.vision_backbone.featurizer.blocks.6.norm1.bias', 'base_model.model.vision_backbone.featurizer.blocks.6.attn.qkv.base_layer.weight', 'base_model.model.vision_backbone.featurizer.blocks.6.attn.qkv.base_layer.bias', 'base_model.model.vision_backbone.featurizer.blocks.6.attn.qkv.lora_A.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.6.attn.qkv.lora_A.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.6.attn.qkv.lora_B.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.6.attn.qkv.lora_B.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.6.attn.proj.base_layer.weight', 'base_model.model.vision_backbone.featurizer.blocks.6.attn.proj.base_layer.bias', 'base_model.model.vision_backbone.featurizer.blocks.6.attn.proj.lora_A.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.6.attn.proj.lora_A.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.6.attn.proj.lora_B.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.6.attn.proj.lora_B.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.6.norm2.weight', 'base_model.model.vision_backbone.featurizer.blocks.6.norm2.bias', 'base_model.model.vision_backbone.featurizer.blocks.6.mlp.fc1.base_layer.weight', 'base_model.model.vision_backbone.featurizer.blocks.6.mlp.fc1.base_layer.bias', 'base_model.model.vision_backbone.featurizer.blocks.6.mlp.fc1.lora_A.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.6.mlp.fc1.lora_A.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.6.mlp.fc1.lora_B.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.6.mlp.fc1.lora_B.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.6.mlp.fc2.base_layer.weight', 'base_model.model.vision_backbone.featurizer.blocks.6.mlp.fc2.base_layer.bias', 'base_model.model.vision_backbone.featurizer.blocks.6.mlp.fc2.lora_A.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.6.mlp.fc2.lora_A.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.6.mlp.fc2.lora_B.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.6.mlp.fc2.lora_B.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.7.norm1.weight', 'base_model.model.vision_backbone.featurizer.blocks.7.norm1.bias', 'base_model.model.vision_backbone.featurizer.blocks.7.attn.qkv.base_layer.weight', 'base_model.model.vision_backbone.featurizer.blocks.7.attn.qkv.base_layer.bias', 'base_model.model.vision_backbone.featurizer.blocks.7.attn.qkv.lora_A.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.7.attn.qkv.lora_A.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.7.attn.qkv.lora_B.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.7.attn.qkv.lora_B.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.7.attn.proj.base_layer.weight', 'base_model.model.vision_backbone.featurizer.blocks.7.attn.proj.base_layer.bias', 'base_model.model.vision_backbone.featurizer.blocks.7.attn.proj.lora_A.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.7.attn.proj.lora_A.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.7.attn.proj.lora_B.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.7.attn.proj.lora_B.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.7.norm2.weight', 'base_model.model.vision_backbone.featurizer.blocks.7.norm2.bias', 'base_model.model.vision_backbone.featurizer.blocks.7.mlp.fc1.base_layer.weight', 'base_model.model.vision_backbone.featurizer.blocks.7.mlp.fc1.base_layer.bias', 'base_model.model.vision_backbone.featurizer.blocks.7.mlp.fc1.lora_A.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.7.mlp.fc1.lora_A.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.7.mlp.fc1.lora_B.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.7.mlp.fc1.lora_B.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.7.mlp.fc2.base_layer.weight', 'base_model.model.vision_backbone.featurizer.blocks.7.mlp.fc2.base_layer.bias', 'base_model.model.vision_backbone.featurizer.blocks.7.mlp.fc2.lora_A.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.7.mlp.fc2.lora_A.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.7.mlp.fc2.lora_B.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.7.mlp.fc2.lora_B.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.8.norm1.weight', 'base_model.model.vision_backbone.featurizer.blocks.8.norm1.bias', 'base_model.model.vision_backbone.featurizer.blocks.8.attn.qkv.base_layer.weight', 'base_model.model.vision_backbone.featurizer.blocks.8.attn.qkv.base_layer.bias', 'base_model.model.vision_backbone.featurizer.blocks.8.attn.qkv.lora_A.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.8.attn.qkv.lora_A.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.8.attn.qkv.lora_B.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.8.attn.qkv.lora_B.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.8.attn.proj.base_layer.weight', 'base_model.model.vision_backbone.featurizer.blocks.8.attn.proj.base_layer.bias', 'base_model.model.vision_backbone.featurizer.blocks.8.attn.proj.lora_A.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.8.attn.proj.lora_A.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.8.attn.proj.lora_B.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.8.attn.proj.lora_B.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.8.norm2.weight', 'base_model.model.vision_backbone.featurizer.blocks.8.norm2.bias', 'base_model.model.vision_backbone.featurizer.blocks.8.mlp.fc1.base_layer.weight', 'base_model.model.vision_backbone.featurizer.blocks.8.mlp.fc1.base_layer.bias', 'base_model.model.vision_backbone.featurizer.blocks.8.mlp.fc1.lora_A.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.8.mlp.fc1.lora_A.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.8.mlp.fc1.lora_B.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.8.mlp.fc1.lora_B.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.8.mlp.fc2.base_layer.weight', 'base_model.model.vision_backbone.featurizer.blocks.8.mlp.fc2.base_layer.bias', 'base_model.model.vision_backbone.featurizer.blocks.8.mlp.fc2.lora_A.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.8.mlp.fc2.lora_A.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.8.mlp.fc2.lora_B.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.8.mlp.fc2.lora_B.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.9.norm1.weight', 'base_model.model.vision_backbone.featurizer.blocks.9.norm1.bias', 'base_model.model.vision_backbone.featurizer.blocks.9.attn.qkv.base_layer.weight', 'base_model.model.vision_backbone.featurizer.blocks.9.attn.qkv.base_layer.bias', 'base_model.model.vision_backbone.featurizer.blocks.9.attn.qkv.lora_A.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.9.attn.qkv.lora_A.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.9.attn.qkv.lora_B.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.9.attn.qkv.lora_B.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.9.attn.proj.base_layer.weight', 'base_model.model.vision_backbone.featurizer.blocks.9.attn.proj.base_layer.bias', 'base_model.model.vision_backbone.featurizer.blocks.9.attn.proj.lora_A.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.9.attn.proj.lora_A.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.9.attn.proj.lora_B.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.9.attn.proj.lora_B.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.9.norm2.weight', 'base_model.model.vision_backbone.featurizer.blocks.9.norm2.bias', 'base_model.model.vision_backbone.featurizer.blocks.9.mlp.fc1.base_layer.weight', 'base_model.model.vision_backbone.featurizer.blocks.9.mlp.fc1.base_layer.bias', 'base_model.model.vision_backbone.featurizer.blocks.9.mlp.fc1.lora_A.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.9.mlp.fc1.lora_A.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.9.mlp.fc1.lora_B.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.9.mlp.fc1.lora_B.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.9.mlp.fc2.base_layer.weight', 'base_model.model.vision_backbone.featurizer.blocks.9.mlp.fc2.base_layer.bias', 'base_model.model.vision_backbone.featurizer.blocks.9.mlp.fc2.lora_A.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.9.mlp.fc2.lora_A.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.9.mlp.fc2.lora_B.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.9.mlp.fc2.lora_B.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.10.norm1.weight', 'base_model.model.vision_backbone.featurizer.blocks.10.norm1.bias', 'base_model.model.vision_backbone.featurizer.blocks.10.attn.qkv.base_layer.weight', 'base_model.model.vision_backbone.featurizer.blocks.10.attn.qkv.base_layer.bias', 'base_model.model.vision_backbone.featurizer.blocks.10.attn.qkv.lora_A.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.10.attn.qkv.lora_A.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.10.attn.qkv.lora_B.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.10.attn.qkv.lora_B.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.10.attn.proj.base_layer.weight', 'base_model.model.vision_backbone.featurizer.blocks.10.attn.proj.base_layer.bias', 'base_model.model.vision_backbone.featurizer.blocks.10.attn.proj.lora_A.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.10.attn.proj.lora_A.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.10.attn.proj.lora_B.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.10.attn.proj.lora_B.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.10.norm2.weight', 'base_model.model.vision_backbone.featurizer.blocks.10.norm2.bias', 'base_model.model.vision_backbone.featurizer.blocks.10.mlp.fc1.base_layer.weight', 'base_model.model.vision_backbone.featurizer.blocks.10.mlp.fc1.base_layer.bias', 'base_model.model.vision_backbone.featurizer.blocks.10.mlp.fc1.lora_A.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.10.mlp.fc1.lora_A.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.10.mlp.fc1.lora_B.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.10.mlp.fc1.lora_B.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.10.mlp.fc2.base_layer.weight', 'base_model.model.vision_backbone.featurizer.blocks.10.mlp.fc2.base_layer.bias', 'base_model.model.vision_backbone.featurizer.blocks.10.mlp.fc2.lora_A.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.10.mlp.fc2.lora_A.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.10.mlp.fc2.lora_B.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.10.mlp.fc2.lora_B.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.11.norm1.weight', 'base_model.model.vision_backbone.featurizer.blocks.11.norm1.bias', 'base_model.model.vision_backbone.featurizer.blocks.11.attn.qkv.base_layer.weight', 'base_model.model.vision_backbone.featurizer.blocks.11.attn.qkv.base_layer.bias', 'base_model.model.vision_backbone.featurizer.blocks.11.attn.qkv.lora_A.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.11.attn.qkv.lora_A.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.11.attn.qkv.lora_B.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.11.attn.qkv.lora_B.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.11.attn.proj.base_layer.weight', 'base_model.model.vision_backbone.featurizer.blocks.11.attn.proj.base_layer.bias', 'base_model.model.vision_backbone.featurizer.blocks.11.attn.proj.lora_A.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.11.attn.proj.lora_A.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.11.attn.proj.lora_B.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.11.attn.proj.lora_B.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.11.norm2.weight', 'base_model.model.vision_backbone.featurizer.blocks.11.norm2.bias', 'base_model.model.vision_backbone.featurizer.blocks.11.mlp.fc1.base_layer.weight', 'base_model.model.vision_backbone.featurizer.blocks.11.mlp.fc1.base_layer.bias', 'base_model.model.vision_backbone.featurizer.blocks.11.mlp.fc1.lora_A.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.11.mlp.fc1.lora_A.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.11.mlp.fc1.lora_B.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.11.mlp.fc1.lora_B.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.11.mlp.fc2.base_layer.weight', 'base_model.model.vision_backbone.featurizer.blocks.11.mlp.fc2.base_layer.bias', 'base_model.model.vision_backbone.featurizer.blocks.11.mlp.fc2.lora_A.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.11.mlp.fc2.lora_A.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.11.mlp.fc2.lora_B.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.11.mlp.fc2.lora_B.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.12.norm1.weight', 'base_model.model.vision_backbone.featurizer.blocks.12.norm1.bias', 'base_model.model.vision_backbone.featurizer.blocks.12.attn.qkv.base_layer.weight', 'base_model.model.vision_backbone.featurizer.blocks.12.attn.qkv.base_layer.bias', 'base_model.model.vision_backbone.featurizer.blocks.12.attn.qkv.lora_A.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.12.attn.qkv.lora_A.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.12.attn.qkv.lora_B.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.12.attn.qkv.lora_B.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.12.attn.proj.base_layer.weight', 'base_model.model.vision_backbone.featurizer.blocks.12.attn.proj.base_layer.bias', 'base_model.model.vision_backbone.featurizer.blocks.12.attn.proj.lora_A.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.12.attn.proj.lora_A.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.12.attn.proj.lora_B.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.12.attn.proj.lora_B.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.12.norm2.weight', 'base_model.model.vision_backbone.featurizer.blocks.12.norm2.bias', 'base_model.model.vision_backbone.featurizer.blocks.12.mlp.fc1.base_layer.weight', 'base_model.model.vision_backbone.featurizer.blocks.12.mlp.fc1.base_layer.bias', 'base_model.model.vision_backbone.featurizer.blocks.12.mlp.fc1.lora_A.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.12.mlp.fc1.lora_A.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.12.mlp.fc1.lora_B.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.12.mlp.fc1.lora_B.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.12.mlp.fc2.base_layer.weight', 'base_model.model.vision_backbone.featurizer.blocks.12.mlp.fc2.base_layer.bias', 'base_model.model.vision_backbone.featurizer.blocks.12.mlp.fc2.lora_A.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.12.mlp.fc2.lora_A.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.12.mlp.fc2.lora_B.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.12.mlp.fc2.lora_B.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.13.norm1.weight', 'base_model.model.vision_backbone.featurizer.blocks.13.norm1.bias', 'base_model.model.vision_backbone.featurizer.blocks.13.attn.qkv.base_layer.weight', 'base_model.model.vision_backbone.featurizer.blocks.13.attn.qkv.base_layer.bias', 'base_model.model.vision_backbone.featurizer.blocks.13.attn.qkv.lora_A.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.13.attn.qkv.lora_A.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.13.attn.qkv.lora_B.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.13.attn.qkv.lora_B.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.13.attn.proj.base_layer.weight', 'base_model.model.vision_backbone.featurizer.blocks.13.attn.proj.base_layer.bias', 'base_model.model.vision_backbone.featurizer.blocks.13.attn.proj.lora_A.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.13.attn.proj.lora_A.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.13.attn.proj.lora_B.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.13.attn.proj.lora_B.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.13.norm2.weight', 'base_model.model.vision_backbone.featurizer.blocks.13.norm2.bias', 'base_model.model.vision_backbone.featurizer.blocks.13.mlp.fc1.base_layer.weight', 'base_model.model.vision_backbone.featurizer.blocks.13.mlp.fc1.base_layer.bias', 'base_model.model.vision_backbone.featurizer.blocks.13.mlp.fc1.lora_A.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.13.mlp.fc1.lora_A.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.13.mlp.fc1.lora_B.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.13.mlp.fc1.lora_B.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.13.mlp.fc2.base_layer.weight', 'base_model.model.vision_backbone.featurizer.blocks.13.mlp.fc2.base_layer.bias', 'base_model.model.vision_backbone.featurizer.blocks.13.mlp.fc2.lora_A.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.13.mlp.fc2.lora_A.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.13.mlp.fc2.lora_B.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.13.mlp.fc2.lora_B.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.14.norm1.weight', 'base_model.model.vision_backbone.featurizer.blocks.14.norm1.bias', 'base_model.model.vision_backbone.featurizer.blocks.14.attn.qkv.base_layer.weight', 'base_model.model.vision_backbone.featurizer.blocks.14.attn.qkv.base_layer.bias', 'base_model.model.vision_backbone.featurizer.blocks.14.attn.qkv.lora_A.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.14.attn.qkv.lora_A.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.14.attn.qkv.lora_B.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.14.attn.qkv.lora_B.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.14.attn.proj.base_layer.weight', 'base_model.model.vision_backbone.featurizer.blocks.14.attn.proj.base_layer.bias', 'base_model.model.vision_backbone.featurizer.blocks.14.attn.proj.lora_A.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.14.attn.proj.lora_A.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.14.attn.proj.lora_B.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.14.attn.proj.lora_B.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.14.norm2.weight', 'base_model.model.vision_backbone.featurizer.blocks.14.norm2.bias', 'base_model.model.vision_backbone.featurizer.blocks.14.mlp.fc1.base_layer.weight', 'base_model.model.vision_backbone.featurizer.blocks.14.mlp.fc1.base_layer.bias', 'base_model.model.vision_backbone.featurizer.blocks.14.mlp.fc1.lora_A.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.14.mlp.fc1.lora_A.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.14.mlp.fc1.lora_B.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.14.mlp.fc1.lora_B.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.14.mlp.fc2.base_layer.weight', 'base_model.model.vision_backbone.featurizer.blocks.14.mlp.fc2.base_layer.bias', 'base_model.model.vision_backbone.featurizer.blocks.14.mlp.fc2.lora_A.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.14.mlp.fc2.lora_A.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.14.mlp.fc2.lora_B.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.14.mlp.fc2.lora_B.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.15.norm1.weight', 'base_model.model.vision_backbone.featurizer.blocks.15.norm1.bias', 'base_model.model.vision_backbone.featurizer.blocks.15.attn.qkv.base_layer.weight', 'base_model.model.vision_backbone.featurizer.blocks.15.attn.qkv.base_layer.bias', 'base_model.model.vision_backbone.featurizer.blocks.15.attn.qkv.lora_A.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.15.attn.qkv.lora_A.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.15.attn.qkv.lora_B.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.15.attn.qkv.lora_B.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.15.attn.proj.base_layer.weight', 'base_model.model.vision_backbone.featurizer.blocks.15.attn.proj.base_layer.bias', 'base_model.model.vision_backbone.featurizer.blocks.15.attn.proj.lora_A.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.15.attn.proj.lora_A.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.15.attn.proj.lora_B.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.15.attn.proj.lora_B.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.15.norm2.weight', 'base_model.model.vision_backbone.featurizer.blocks.15.norm2.bias', 'base_model.model.vision_backbone.featurizer.blocks.15.mlp.fc1.base_layer.weight', 'base_model.model.vision_backbone.featurizer.blocks.15.mlp.fc1.base_layer.bias', 'base_model.model.vision_backbone.featurizer.blocks.15.mlp.fc1.lora_A.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.15.mlp.fc1.lora_A.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.15.mlp.fc1.lora_B.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.15.mlp.fc1.lora_B.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.15.mlp.fc2.base_layer.weight', 'base_model.model.vision_backbone.featurizer.blocks.15.mlp.fc2.base_layer.bias', 'base_model.model.vision_backbone.featurizer.blocks.15.mlp.fc2.lora_A.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.15.mlp.fc2.lora_A.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.15.mlp.fc2.lora_B.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.15.mlp.fc2.lora_B.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.16.norm1.weight', 'base_model.model.vision_backbone.featurizer.blocks.16.norm1.bias', 'base_model.model.vision_backbone.featurizer.blocks.16.attn.qkv.base_layer.weight', 'base_model.model.vision_backbone.featurizer.blocks.16.attn.qkv.base_layer.bias', 'base_model.model.vision_backbone.featurizer.blocks.16.attn.qkv.lora_A.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.16.attn.qkv.lora_A.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.16.attn.qkv.lora_B.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.16.attn.qkv.lora_B.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.16.attn.proj.base_layer.weight', 'base_model.model.vision_backbone.featurizer.blocks.16.attn.proj.base_layer.bias', 'base_model.model.vision_backbone.featurizer.blocks.16.attn.proj.lora_A.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.16.attn.proj.lora_A.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.16.attn.proj.lora_B.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.16.attn.proj.lora_B.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.16.norm2.weight', 'base_model.model.vision_backbone.featurizer.blocks.16.norm2.bias', 'base_model.model.vision_backbone.featurizer.blocks.16.mlp.fc1.base_layer.weight', 'base_model.model.vision_backbone.featurizer.blocks.16.mlp.fc1.base_layer.bias', 'base_model.model.vision_backbone.featurizer.blocks.16.mlp.fc1.lora_A.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.16.mlp.fc1.lora_A.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.16.mlp.fc1.lora_B.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.16.mlp.fc1.lora_B.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.16.mlp.fc2.base_layer.weight', 'base_model.model.vision_backbone.featurizer.blocks.16.mlp.fc2.base_layer.bias', 'base_model.model.vision_backbone.featurizer.blocks.16.mlp.fc2.lora_A.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.16.mlp.fc2.lora_A.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.16.mlp.fc2.lora_B.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.16.mlp.fc2.lora_B.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.17.norm1.weight', 'base_model.model.vision_backbone.featurizer.blocks.17.norm1.bias', 'base_model.model.vision_backbone.featurizer.blocks.17.attn.qkv.base_layer.weight', 'base_model.model.vision_backbone.featurizer.blocks.17.attn.qkv.base_layer.bias', 'base_model.model.vision_backbone.featurizer.blocks.17.attn.qkv.lora_A.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.17.attn.qkv.lora_A.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.17.attn.qkv.lora_B.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.17.attn.qkv.lora_B.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.17.attn.proj.base_layer.weight', 'base_model.model.vision_backbone.featurizer.blocks.17.attn.proj.base_layer.bias', 'base_model.model.vision_backbone.featurizer.blocks.17.attn.proj.lora_A.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.17.attn.proj.lora_A.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.17.attn.proj.lora_B.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.17.attn.proj.lora_B.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.17.norm2.weight', 'base_model.model.vision_backbone.featurizer.blocks.17.norm2.bias', 'base_model.model.vision_backbone.featurizer.blocks.17.mlp.fc1.base_layer.weight', 'base_model.model.vision_backbone.featurizer.blocks.17.mlp.fc1.base_layer.bias', 'base_model.model.vision_backbone.featurizer.blocks.17.mlp.fc1.lora_A.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.17.mlp.fc1.lora_A.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.17.mlp.fc1.lora_B.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.17.mlp.fc1.lora_B.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.17.mlp.fc2.base_layer.weight', 'base_model.model.vision_backbone.featurizer.blocks.17.mlp.fc2.base_layer.bias', 'base_model.model.vision_backbone.featurizer.blocks.17.mlp.fc2.lora_A.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.17.mlp.fc2.lora_A.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.17.mlp.fc2.lora_B.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.17.mlp.fc2.lora_B.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.18.norm1.weight', 'base_model.model.vision_backbone.featurizer.blocks.18.norm1.bias', 'base_model.model.vision_backbone.featurizer.blocks.18.attn.qkv.base_layer.weight', 'base_model.model.vision_backbone.featurizer.blocks.18.attn.qkv.base_layer.bias', 'base_model.model.vision_backbone.featurizer.blocks.18.attn.qkv.lora_A.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.18.attn.qkv.lora_A.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.18.attn.qkv.lora_B.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.18.attn.qkv.lora_B.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.18.attn.proj.base_layer.weight', 'base_model.model.vision_backbone.featurizer.blocks.18.attn.proj.base_layer.bias', 'base_model.model.vision_backbone.featurizer.blocks.18.attn.proj.lora_A.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.18.attn.proj.lora_A.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.18.attn.proj.lora_B.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.18.attn.proj.lora_B.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.18.norm2.weight', 'base_model.model.vision_backbone.featurizer.blocks.18.norm2.bias', 'base_model.model.vision_backbone.featurizer.blocks.18.mlp.fc1.base_layer.weight', 'base_model.model.vision_backbone.featurizer.blocks.18.mlp.fc1.base_layer.bias', 'base_model.model.vision_backbone.featurizer.blocks.18.mlp.fc1.lora_A.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.18.mlp.fc1.lora_A.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.18.mlp.fc1.lora_B.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.18.mlp.fc1.lora_B.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.18.mlp.fc2.base_layer.weight', 'base_model.model.vision_backbone.featurizer.blocks.18.mlp.fc2.base_layer.bias', 'base_model.model.vision_backbone.featurizer.blocks.18.mlp.fc2.lora_A.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.18.mlp.fc2.lora_A.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.18.mlp.fc2.lora_B.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.18.mlp.fc2.lora_B.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.19.norm1.weight', 'base_model.model.vision_backbone.featurizer.blocks.19.norm1.bias', 'base_model.model.vision_backbone.featurizer.blocks.19.attn.qkv.base_layer.weight', 'base_model.model.vision_backbone.featurizer.blocks.19.attn.qkv.base_layer.bias', 'base_model.model.vision_backbone.featurizer.blocks.19.attn.qkv.lora_A.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.19.attn.qkv.lora_A.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.19.attn.qkv.lora_B.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.19.attn.qkv.lora_B.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.19.attn.proj.base_layer.weight', 'base_model.model.vision_backbone.featurizer.blocks.19.attn.proj.base_layer.bias', 'base_model.model.vision_backbone.featurizer.blocks.19.attn.proj.lora_A.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.19.attn.proj.lora_A.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.19.attn.proj.lora_B.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.19.attn.proj.lora_B.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.19.norm2.weight', 'base_model.model.vision_backbone.featurizer.blocks.19.norm2.bias', 'base_model.model.vision_backbone.featurizer.blocks.19.mlp.fc1.base_layer.weight', 'base_model.model.vision_backbone.featurizer.blocks.19.mlp.fc1.base_layer.bias', 'base_model.model.vision_backbone.featurizer.blocks.19.mlp.fc1.lora_A.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.19.mlp.fc1.lora_A.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.19.mlp.fc1.lora_B.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.19.mlp.fc1.lora_B.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.19.mlp.fc2.base_layer.weight', 'base_model.model.vision_backbone.featurizer.blocks.19.mlp.fc2.base_layer.bias', 'base_model.model.vision_backbone.featurizer.blocks.19.mlp.fc2.lora_A.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.19.mlp.fc2.lora_A.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.19.mlp.fc2.lora_B.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.19.mlp.fc2.lora_B.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.20.norm1.weight', 'base_model.model.vision_backbone.featurizer.blocks.20.norm1.bias', 'base_model.model.vision_backbone.featurizer.blocks.20.attn.qkv.base_layer.weight', 'base_model.model.vision_backbone.featurizer.blocks.20.attn.qkv.base_layer.bias', 'base_model.model.vision_backbone.featurizer.blocks.20.attn.qkv.lora_A.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.20.attn.qkv.lora_A.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.20.attn.qkv.lora_B.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.20.attn.qkv.lora_B.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.20.attn.proj.base_layer.weight', 'base_model.model.vision_backbone.featurizer.blocks.20.attn.proj.base_layer.bias', 'base_model.model.vision_backbone.featurizer.blocks.20.attn.proj.lora_A.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.20.attn.proj.lora_A.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.20.attn.proj.lora_B.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.20.attn.proj.lora_B.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.20.norm2.weight', 'base_model.model.vision_backbone.featurizer.blocks.20.norm2.bias', 'base_model.model.vision_backbone.featurizer.blocks.20.mlp.fc1.base_layer.weight', 'base_model.model.vision_backbone.featurizer.blocks.20.mlp.fc1.base_layer.bias', 'base_model.model.vision_backbone.featurizer.blocks.20.mlp.fc1.lora_A.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.20.mlp.fc1.lora_A.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.20.mlp.fc1.lora_B.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.20.mlp.fc1.lora_B.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.20.mlp.fc2.base_layer.weight', 'base_model.model.vision_backbone.featurizer.blocks.20.mlp.fc2.base_layer.bias', 'base_model.model.vision_backbone.featurizer.blocks.20.mlp.fc2.lora_A.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.20.mlp.fc2.lora_A.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.20.mlp.fc2.lora_B.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.20.mlp.fc2.lora_B.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.21.norm1.weight', 'base_model.model.vision_backbone.featurizer.blocks.21.norm1.bias', 'base_model.model.vision_backbone.featurizer.blocks.21.attn.qkv.base_layer.weight', 'base_model.model.vision_backbone.featurizer.blocks.21.attn.qkv.base_layer.bias', 'base_model.model.vision_backbone.featurizer.blocks.21.attn.qkv.lora_A.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.21.attn.qkv.lora_A.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.21.attn.qkv.lora_B.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.21.attn.qkv.lora_B.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.21.attn.proj.base_layer.weight', 'base_model.model.vision_backbone.featurizer.blocks.21.attn.proj.base_layer.bias', 'base_model.model.vision_backbone.featurizer.blocks.21.attn.proj.lora_A.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.21.attn.proj.lora_A.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.21.attn.proj.lora_B.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.21.attn.proj.lora_B.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.21.norm2.weight', 'base_model.model.vision_backbone.featurizer.blocks.21.norm2.bias', 'base_model.model.vision_backbone.featurizer.blocks.21.mlp.fc1.base_layer.weight', 'base_model.model.vision_backbone.featurizer.blocks.21.mlp.fc1.base_layer.bias', 'base_model.model.vision_backbone.featurizer.blocks.21.mlp.fc1.lora_A.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.21.mlp.fc1.lora_A.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.21.mlp.fc1.lora_B.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.21.mlp.fc1.lora_B.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.21.mlp.fc2.base_layer.weight', 'base_model.model.vision_backbone.featurizer.blocks.21.mlp.fc2.base_layer.bias', 'base_model.model.vision_backbone.featurizer.blocks.21.mlp.fc2.lora_A.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.21.mlp.fc2.lora_A.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.21.mlp.fc2.lora_B.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.21.mlp.fc2.lora_B.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.22.norm1.weight', 'base_model.model.vision_backbone.featurizer.blocks.22.norm1.bias', 'base_model.model.vision_backbone.featurizer.blocks.22.attn.qkv.base_layer.weight', 'base_model.model.vision_backbone.featurizer.blocks.22.attn.qkv.base_layer.bias', 'base_model.model.vision_backbone.featurizer.blocks.22.attn.qkv.lora_A.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.22.attn.qkv.lora_A.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.22.attn.qkv.lora_B.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.22.attn.qkv.lora_B.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.22.attn.proj.base_layer.weight', 'base_model.model.vision_backbone.featurizer.blocks.22.attn.proj.base_layer.bias', 'base_model.model.vision_backbone.featurizer.blocks.22.attn.proj.lora_A.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.22.attn.proj.lora_A.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.22.attn.proj.lora_B.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.22.attn.proj.lora_B.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.22.norm2.weight', 'base_model.model.vision_backbone.featurizer.blocks.22.norm2.bias', 'base_model.model.vision_backbone.featurizer.blocks.22.mlp.fc1.base_layer.weight', 'base_model.model.vision_backbone.featurizer.blocks.22.mlp.fc1.base_layer.bias', 'base_model.model.vision_backbone.featurizer.blocks.22.mlp.fc1.lora_A.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.22.mlp.fc1.lora_A.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.22.mlp.fc1.lora_B.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.22.mlp.fc1.lora_B.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.22.mlp.fc2.base_layer.weight', 'base_model.model.vision_backbone.featurizer.blocks.22.mlp.fc2.base_layer.bias', 'base_model.model.vision_backbone.featurizer.blocks.22.mlp.fc2.lora_A.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.22.mlp.fc2.lora_A.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.22.mlp.fc2.lora_B.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.22.mlp.fc2.lora_B.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.23.norm1.weight', 'base_model.model.vision_backbone.featurizer.blocks.23.norm1.bias', 'base_model.model.vision_backbone.featurizer.blocks.23.attn.qkv.base_layer.weight', 'base_model.model.vision_backbone.featurizer.blocks.23.attn.qkv.base_layer.bias', 'base_model.model.vision_backbone.featurizer.blocks.23.attn.qkv.lora_A.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.23.attn.qkv.lora_A.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.23.attn.qkv.lora_B.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.23.attn.qkv.lora_B.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.23.attn.proj.base_layer.weight', 'base_model.model.vision_backbone.featurizer.blocks.23.attn.proj.base_layer.bias', 'base_model.model.vision_backbone.featurizer.blocks.23.attn.proj.lora_A.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.23.attn.proj.lora_A.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.23.attn.proj.lora_B.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.23.attn.proj.lora_B.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.23.norm2.weight', 'base_model.model.vision_backbone.featurizer.blocks.23.norm2.bias', 'base_model.model.vision_backbone.featurizer.blocks.23.mlp.fc1.base_layer.weight', 'base_model.model.vision_backbone.featurizer.blocks.23.mlp.fc1.base_layer.bias', 'base_model.model.vision_backbone.featurizer.blocks.23.mlp.fc1.lora_A.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.23.mlp.fc1.lora_A.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.23.mlp.fc1.lora_B.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.23.mlp.fc1.lora_B.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.23.mlp.fc2.base_layer.weight', 'base_model.model.vision_backbone.featurizer.blocks.23.mlp.fc2.base_layer.bias', 'base_model.model.vision_backbone.featurizer.blocks.23.mlp.fc2.lora_A.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.23.mlp.fc2.lora_A.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.23.mlp.fc2.lora_B.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.23.mlp.fc2.lora_B.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.24.norm1.weight', 'base_model.model.vision_backbone.featurizer.blocks.24.norm1.bias', 'base_model.model.vision_backbone.featurizer.blocks.24.attn.qkv.base_layer.weight', 'base_model.model.vision_backbone.featurizer.blocks.24.attn.qkv.base_layer.bias', 'base_model.model.vision_backbone.featurizer.blocks.24.attn.qkv.lora_A.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.24.attn.qkv.lora_A.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.24.attn.qkv.lora_B.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.24.attn.qkv.lora_B.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.24.attn.proj.base_layer.weight', 'base_model.model.vision_backbone.featurizer.blocks.24.attn.proj.base_layer.bias', 'base_model.model.vision_backbone.featurizer.blocks.24.attn.proj.lora_A.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.24.attn.proj.lora_A.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.24.attn.proj.lora_B.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.24.attn.proj.lora_B.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.24.norm2.weight', 'base_model.model.vision_backbone.featurizer.blocks.24.norm2.bias', 'base_model.model.vision_backbone.featurizer.blocks.24.mlp.fc1.base_layer.weight', 'base_model.model.vision_backbone.featurizer.blocks.24.mlp.fc1.base_layer.bias', 'base_model.model.vision_backbone.featurizer.blocks.24.mlp.fc1.lora_A.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.24.mlp.fc1.lora_A.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.24.mlp.fc1.lora_B.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.24.mlp.fc1.lora_B.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.24.mlp.fc2.base_layer.weight', 'base_model.model.vision_backbone.featurizer.blocks.24.mlp.fc2.base_layer.bias', 'base_model.model.vision_backbone.featurizer.blocks.24.mlp.fc2.lora_A.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.24.mlp.fc2.lora_A.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.24.mlp.fc2.lora_B.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.24.mlp.fc2.lora_B.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.25.norm1.weight', 'base_model.model.vision_backbone.featurizer.blocks.25.norm1.bias', 'base_model.model.vision_backbone.featurizer.blocks.25.attn.qkv.base_layer.weight', 'base_model.model.vision_backbone.featurizer.blocks.25.attn.qkv.base_layer.bias', 'base_model.model.vision_backbone.featurizer.blocks.25.attn.qkv.lora_A.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.25.attn.qkv.lora_A.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.25.attn.qkv.lora_B.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.25.attn.qkv.lora_B.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.25.attn.proj.base_layer.weight', 'base_model.model.vision_backbone.featurizer.blocks.25.attn.proj.base_layer.bias', 'base_model.model.vision_backbone.featurizer.blocks.25.attn.proj.lora_A.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.25.attn.proj.lora_A.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.25.attn.proj.lora_B.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.25.attn.proj.lora_B.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.25.norm2.weight', 'base_model.model.vision_backbone.featurizer.blocks.25.norm2.bias', 'base_model.model.vision_backbone.featurizer.blocks.25.mlp.fc1.base_layer.weight', 'base_model.model.vision_backbone.featurizer.blocks.25.mlp.fc1.base_layer.bias', 'base_model.model.vision_backbone.featurizer.blocks.25.mlp.fc1.lora_A.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.25.mlp.fc1.lora_A.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.25.mlp.fc1.lora_B.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.25.mlp.fc1.lora_B.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.25.mlp.fc2.base_layer.weight', 'base_model.model.vision_backbone.featurizer.blocks.25.mlp.fc2.base_layer.bias', 'base_model.model.vision_backbone.featurizer.blocks.25.mlp.fc2.lora_A.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.25.mlp.fc2.lora_A.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.25.mlp.fc2.lora_B.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.25.mlp.fc2.lora_B.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.26.norm1.weight', 'base_model.model.vision_backbone.featurizer.blocks.26.norm1.bias', 'base_model.model.vision_backbone.featurizer.blocks.26.attn.qkv.base_layer.weight', 'base_model.model.vision_backbone.featurizer.blocks.26.attn.qkv.base_layer.bias', 'base_model.model.vision_backbone.featurizer.blocks.26.attn.qkv.lora_A.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.26.attn.qkv.lora_A.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.26.attn.qkv.lora_B.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.26.attn.qkv.lora_B.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.26.attn.proj.base_layer.weight', 'base_model.model.vision_backbone.featurizer.blocks.26.attn.proj.base_layer.bias', 'base_model.model.vision_backbone.featurizer.blocks.26.attn.proj.lora_A.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.26.attn.proj.lora_A.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.26.attn.proj.lora_B.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.26.attn.proj.lora_B.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.26.norm2.weight', 'base_model.model.vision_backbone.featurizer.blocks.26.norm2.bias', 'base_model.model.vision_backbone.featurizer.blocks.26.mlp.fc1.base_layer.weight', 'base_model.model.vision_backbone.featurizer.blocks.26.mlp.fc1.base_layer.bias', 'base_model.model.vision_backbone.featurizer.blocks.26.mlp.fc1.lora_A.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.26.mlp.fc1.lora_A.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.26.mlp.fc1.lora_B.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.26.mlp.fc1.lora_B.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.26.mlp.fc2.base_layer.weight', 'base_model.model.vision_backbone.featurizer.blocks.26.mlp.fc2.base_layer.bias', 'base_model.model.vision_backbone.featurizer.blocks.26.mlp.fc2.lora_A.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.26.mlp.fc2.lora_A.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.blocks.26.mlp.fc2.lora_B.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.blocks.26.mlp.fc2.lora_B.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.norm.weight', 'base_model.model.vision_backbone.featurizer.norm.bias', 'base_model.model.vision_backbone.featurizer.attn_pool.latent', 'base_model.model.vision_backbone.featurizer.attn_pool.q.base_layer.weight', 'base_model.model.vision_backbone.featurizer.attn_pool.q.base_layer.bias', 'base_model.model.vision_backbone.featurizer.attn_pool.q.lora_A.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.attn_pool.q.lora_A.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.attn_pool.q.lora_B.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.attn_pool.q.lora_B.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.attn_pool.kv.base_layer.weight', 'base_model.model.vision_backbone.featurizer.attn_pool.kv.base_layer.bias', 'base_model.model.vision_backbone.featurizer.attn_pool.kv.lora_A.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.attn_pool.kv.lora_A.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.attn_pool.kv.lora_B.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.attn_pool.kv.lora_B.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.attn_pool.proj.base_layer.weight', 'base_model.model.vision_backbone.featurizer.attn_pool.proj.base_layer.bias', 'base_model.model.vision_backbone.featurizer.attn_pool.proj.lora_A.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.attn_pool.proj.lora_A.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.attn_pool.proj.lora_B.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.attn_pool.proj.lora_B.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.attn_pool.norm.weight', 'base_model.model.vision_backbone.featurizer.attn_pool.norm.bias', 'base_model.model.vision_backbone.featurizer.attn_pool.mlp.fc1.base_layer.weight', 'base_model.model.vision_backbone.featurizer.attn_pool.mlp.fc1.base_layer.bias', 'base_model.model.vision_backbone.featurizer.attn_pool.mlp.fc1.lora_A.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.attn_pool.mlp.fc1.lora_A.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.attn_pool.mlp.fc1.lora_B.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.attn_pool.mlp.fc1.lora_B.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.attn_pool.mlp.fc2.base_layer.weight', 'base_model.model.vision_backbone.featurizer.attn_pool.mlp.fc2.base_layer.bias', 'base_model.model.vision_backbone.featurizer.attn_pool.mlp.fc2.lora_A.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.attn_pool.mlp.fc2.lora_A.actor_critic1.weight', 'base_model.model.vision_backbone.featurizer.attn_pool.mlp.fc2.lora_B.actor_critic.weight', 'base_model.model.vision_backbone.featurizer.attn_pool.mlp.fc2.lora_B.actor_critic1.weight', 'base_model.model.projector.fc1.base_layer.weight', 'base_model.model.projector.fc1.base_layer.bias', 'base_model.model.projector.fc1.lora_A.actor_critic.weight', 'base_model.model.projector.fc1.lora_A.actor_critic1.weight', 'base_model.model.projector.fc1.lora_B.actor_critic.weight', 'base_model.model.projector.fc1.lora_B.actor_critic1.weight', 'base_model.model.projector.fc2.base_layer.weight', 'base_model.model.projector.fc2.base_layer.bias', 'base_model.model.projector.fc2.lora_A.actor_critic.weight', 'base_model.model.projector.fc2.lora_A.actor_critic1.weight', 'base_model.model.projector.fc2.lora_B.actor_critic.weight', 'base_model.model.projector.fc2.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.0.self_attn.q_proj.base_layer.weight', 'base_model.model.language_model.model.layers.0.self_attn.q_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.0.self_attn.q_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.0.self_attn.q_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.0.self_attn.q_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.0.self_attn.k_proj.base_layer.weight', 'base_model.model.language_model.model.layers.0.self_attn.k_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.0.self_attn.k_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.0.self_attn.k_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.0.self_attn.k_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.0.self_attn.v_proj.base_layer.weight', 'base_model.model.language_model.model.layers.0.self_attn.v_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.0.self_attn.v_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.0.self_attn.v_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.0.self_attn.v_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.0.self_attn.o_proj.base_layer.weight', 'base_model.model.language_model.model.layers.0.self_attn.o_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.0.self_attn.o_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.0.self_attn.o_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.0.self_attn.o_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.0.mlp.gate_proj.base_layer.weight', 'base_model.model.language_model.model.layers.0.mlp.gate_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.0.mlp.gate_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.0.mlp.gate_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.0.mlp.gate_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.0.mlp.up_proj.base_layer.weight', 'base_model.model.language_model.model.layers.0.mlp.up_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.0.mlp.up_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.0.mlp.up_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.0.mlp.up_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.0.mlp.down_proj.base_layer.weight', 'base_model.model.language_model.model.layers.0.mlp.down_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.0.mlp.down_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.0.mlp.down_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.0.mlp.down_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.0.input_layernorm.weight', 'base_model.model.language_model.model.layers.0.post_attention_layernorm.weight', 'base_model.model.language_model.model.layers.1.self_attn.q_proj.base_layer.weight', 'base_model.model.language_model.model.layers.1.self_attn.q_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.1.self_attn.q_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.1.self_attn.q_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.1.self_attn.q_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.1.self_attn.k_proj.base_layer.weight', 'base_model.model.language_model.model.layers.1.self_attn.k_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.1.self_attn.k_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.1.self_attn.k_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.1.self_attn.k_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.1.self_attn.v_proj.base_layer.weight', 'base_model.model.language_model.model.layers.1.self_attn.v_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.1.self_attn.v_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.1.self_attn.v_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.1.self_attn.v_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.1.self_attn.o_proj.base_layer.weight', 'base_model.model.language_model.model.layers.1.self_attn.o_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.1.self_attn.o_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.1.self_attn.o_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.1.self_attn.o_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.1.mlp.gate_proj.base_layer.weight', 'base_model.model.language_model.model.layers.1.mlp.gate_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.1.mlp.gate_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.1.mlp.gate_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.1.mlp.gate_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.1.mlp.up_proj.base_layer.weight', 'base_model.model.language_model.model.layers.1.mlp.up_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.1.mlp.up_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.1.mlp.up_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.1.mlp.up_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.1.mlp.down_proj.base_layer.weight', 'base_model.model.language_model.model.layers.1.mlp.down_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.1.mlp.down_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.1.mlp.down_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.1.mlp.down_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.1.input_layernorm.weight', 'base_model.model.language_model.model.layers.1.post_attention_layernorm.weight', 'base_model.model.language_model.model.layers.2.self_attn.q_proj.base_layer.weight', 'base_model.model.language_model.model.layers.2.self_attn.q_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.2.self_attn.q_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.2.self_attn.q_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.2.self_attn.q_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.2.self_attn.k_proj.base_layer.weight', 'base_model.model.language_model.model.layers.2.self_attn.k_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.2.self_attn.k_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.2.self_attn.k_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.2.self_attn.k_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.2.self_attn.v_proj.base_layer.weight', 'base_model.model.language_model.model.layers.2.self_attn.v_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.2.self_attn.v_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.2.self_attn.v_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.2.self_attn.v_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.2.self_attn.o_proj.base_layer.weight', 'base_model.model.language_model.model.layers.2.self_attn.o_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.2.self_attn.o_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.2.self_attn.o_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.2.self_attn.o_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.2.mlp.gate_proj.base_layer.weight', 'base_model.model.language_model.model.layers.2.mlp.gate_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.2.mlp.gate_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.2.mlp.gate_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.2.mlp.gate_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.2.mlp.up_proj.base_layer.weight', 'base_model.model.language_model.model.layers.2.mlp.up_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.2.mlp.up_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.2.mlp.up_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.2.mlp.up_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.2.mlp.down_proj.base_layer.weight', 'base_model.model.language_model.model.layers.2.mlp.down_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.2.mlp.down_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.2.mlp.down_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.2.mlp.down_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.2.input_layernorm.weight', 'base_model.model.language_model.model.layers.2.post_attention_layernorm.weight', 'base_model.model.language_model.model.layers.3.self_attn.q_proj.base_layer.weight', 'base_model.model.language_model.model.layers.3.self_attn.q_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.3.self_attn.q_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.3.self_attn.q_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.3.self_attn.q_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.3.self_attn.k_proj.base_layer.weight', 'base_model.model.language_model.model.layers.3.self_attn.k_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.3.self_attn.k_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.3.self_attn.k_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.3.self_attn.k_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.3.self_attn.v_proj.base_layer.weight', 'base_model.model.language_model.model.layers.3.self_attn.v_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.3.self_attn.v_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.3.self_attn.v_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.3.self_attn.v_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.3.self_attn.o_proj.base_layer.weight', 'base_model.model.language_model.model.layers.3.self_attn.o_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.3.self_attn.o_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.3.self_attn.o_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.3.self_attn.o_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.3.mlp.gate_proj.base_layer.weight', 'base_model.model.language_model.model.layers.3.mlp.gate_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.3.mlp.gate_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.3.mlp.gate_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.3.mlp.gate_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.3.mlp.up_proj.base_layer.weight', 'base_model.model.language_model.model.layers.3.mlp.up_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.3.mlp.up_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.3.mlp.up_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.3.mlp.up_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.3.mlp.down_proj.base_layer.weight', 'base_model.model.language_model.model.layers.3.mlp.down_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.3.mlp.down_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.3.mlp.down_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.3.mlp.down_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.3.input_layernorm.weight', 'base_model.model.language_model.model.layers.3.post_attention_layernorm.weight', 'base_model.model.language_model.model.layers.4.self_attn.q_proj.base_layer.weight', 'base_model.model.language_model.model.layers.4.self_attn.q_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.4.self_attn.q_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.4.self_attn.q_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.4.self_attn.q_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.4.self_attn.k_proj.base_layer.weight', 'base_model.model.language_model.model.layers.4.self_attn.k_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.4.self_attn.k_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.4.self_attn.k_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.4.self_attn.k_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.4.self_attn.v_proj.base_layer.weight', 'base_model.model.language_model.model.layers.4.self_attn.v_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.4.self_attn.v_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.4.self_attn.v_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.4.self_attn.v_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.4.self_attn.o_proj.base_layer.weight', 'base_model.model.language_model.model.layers.4.self_attn.o_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.4.self_attn.o_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.4.self_attn.o_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.4.self_attn.o_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.4.mlp.gate_proj.base_layer.weight', 'base_model.model.language_model.model.layers.4.mlp.gate_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.4.mlp.gate_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.4.mlp.gate_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.4.mlp.gate_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.4.mlp.up_proj.base_layer.weight', 'base_model.model.language_model.model.layers.4.mlp.up_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.4.mlp.up_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.4.mlp.up_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.4.mlp.up_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.4.mlp.down_proj.base_layer.weight', 'base_model.model.language_model.model.layers.4.mlp.down_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.4.mlp.down_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.4.mlp.down_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.4.mlp.down_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.4.input_layernorm.weight', 'base_model.model.language_model.model.layers.4.post_attention_layernorm.weight', 'base_model.model.language_model.model.layers.5.self_attn.q_proj.base_layer.weight', 'base_model.model.language_model.model.layers.5.self_attn.q_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.5.self_attn.q_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.5.self_attn.q_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.5.self_attn.q_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.5.self_attn.k_proj.base_layer.weight', 'base_model.model.language_model.model.layers.5.self_attn.k_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.5.self_attn.k_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.5.self_attn.k_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.5.self_attn.k_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.5.self_attn.v_proj.base_layer.weight', 'base_model.model.language_model.model.layers.5.self_attn.v_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.5.self_attn.v_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.5.self_attn.v_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.5.self_attn.v_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.5.self_attn.o_proj.base_layer.weight', 'base_model.model.language_model.model.layers.5.self_attn.o_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.5.self_attn.o_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.5.self_attn.o_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.5.self_attn.o_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.5.mlp.gate_proj.base_layer.weight', 'base_model.model.language_model.model.layers.5.mlp.gate_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.5.mlp.gate_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.5.mlp.gate_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.5.mlp.gate_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.5.mlp.up_proj.base_layer.weight', 'base_model.model.language_model.model.layers.5.mlp.up_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.5.mlp.up_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.5.mlp.up_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.5.mlp.up_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.5.mlp.down_proj.base_layer.weight', 'base_model.model.language_model.model.layers.5.mlp.down_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.5.mlp.down_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.5.mlp.down_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.5.mlp.down_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.5.input_layernorm.weight', 'base_model.model.language_model.model.layers.5.post_attention_layernorm.weight', 'base_model.model.language_model.model.layers.6.self_attn.q_proj.base_layer.weight', 'base_model.model.language_model.model.layers.6.self_attn.q_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.6.self_attn.q_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.6.self_attn.q_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.6.self_attn.q_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.6.self_attn.k_proj.base_layer.weight', 'base_model.model.language_model.model.layers.6.self_attn.k_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.6.self_attn.k_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.6.self_attn.k_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.6.self_attn.k_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.6.self_attn.v_proj.base_layer.weight', 'base_model.model.language_model.model.layers.6.self_attn.v_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.6.self_attn.v_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.6.self_attn.v_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.6.self_attn.v_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.6.self_attn.o_proj.base_layer.weight', 'base_model.model.language_model.model.layers.6.self_attn.o_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.6.self_attn.o_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.6.self_attn.o_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.6.self_attn.o_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.6.mlp.gate_proj.base_layer.weight', 'base_model.model.language_model.model.layers.6.mlp.gate_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.6.mlp.gate_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.6.mlp.gate_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.6.mlp.gate_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.6.mlp.up_proj.base_layer.weight', 'base_model.model.language_model.model.layers.6.mlp.up_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.6.mlp.up_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.6.mlp.up_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.6.mlp.up_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.6.mlp.down_proj.base_layer.weight', 'base_model.model.language_model.model.layers.6.mlp.down_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.6.mlp.down_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.6.mlp.down_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.6.mlp.down_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.6.input_layernorm.weight', 'base_model.model.language_model.model.layers.6.post_attention_layernorm.weight', 'base_model.model.language_model.model.layers.7.self_attn.q_proj.base_layer.weight', 'base_model.model.language_model.model.layers.7.self_attn.q_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.7.self_attn.q_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.7.self_attn.q_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.7.self_attn.q_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.7.self_attn.k_proj.base_layer.weight', 'base_model.model.language_model.model.layers.7.self_attn.k_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.7.self_attn.k_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.7.self_attn.k_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.7.self_attn.k_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.7.self_attn.v_proj.base_layer.weight', 'base_model.model.language_model.model.layers.7.self_attn.v_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.7.self_attn.v_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.7.self_attn.v_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.7.self_attn.v_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.7.self_attn.o_proj.base_layer.weight', 'base_model.model.language_model.model.layers.7.self_attn.o_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.7.self_attn.o_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.7.self_attn.o_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.7.self_attn.o_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.7.mlp.gate_proj.base_layer.weight', 'base_model.model.language_model.model.layers.7.mlp.gate_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.7.mlp.gate_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.7.mlp.gate_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.7.mlp.gate_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.7.mlp.up_proj.base_layer.weight', 'base_model.model.language_model.model.layers.7.mlp.up_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.7.mlp.up_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.7.mlp.up_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.7.mlp.up_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.7.mlp.down_proj.base_layer.weight', 'base_model.model.language_model.model.layers.7.mlp.down_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.7.mlp.down_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.7.mlp.down_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.7.mlp.down_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.7.input_layernorm.weight', 'base_model.model.language_model.model.layers.7.post_attention_layernorm.weight', 'base_model.model.language_model.model.layers.8.self_attn.q_proj.base_layer.weight', 'base_model.model.language_model.model.layers.8.self_attn.q_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.8.self_attn.q_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.8.self_attn.q_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.8.self_attn.q_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.8.self_attn.k_proj.base_layer.weight', 'base_model.model.language_model.model.layers.8.self_attn.k_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.8.self_attn.k_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.8.self_attn.k_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.8.self_attn.k_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.8.self_attn.v_proj.base_layer.weight', 'base_model.model.language_model.model.layers.8.self_attn.v_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.8.self_attn.v_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.8.self_attn.v_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.8.self_attn.v_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.8.self_attn.o_proj.base_layer.weight', 'base_model.model.language_model.model.layers.8.self_attn.o_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.8.self_attn.o_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.8.self_attn.o_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.8.self_attn.o_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.8.mlp.gate_proj.base_layer.weight', 'base_model.model.language_model.model.layers.8.mlp.gate_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.8.mlp.gate_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.8.mlp.gate_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.8.mlp.gate_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.8.mlp.up_proj.base_layer.weight', 'base_model.model.language_model.model.layers.8.mlp.up_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.8.mlp.up_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.8.mlp.up_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.8.mlp.up_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.8.mlp.down_proj.base_layer.weight', 'base_model.model.language_model.model.layers.8.mlp.down_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.8.mlp.down_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.8.mlp.down_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.8.mlp.down_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.8.input_layernorm.weight', 'base_model.model.language_model.model.layers.8.post_attention_layernorm.weight', 'base_model.model.language_model.model.layers.9.self_attn.q_proj.base_layer.weight', 'base_model.model.language_model.model.layers.9.self_attn.q_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.9.self_attn.q_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.9.self_attn.q_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.9.self_attn.q_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.9.self_attn.k_proj.base_layer.weight', 'base_model.model.language_model.model.layers.9.self_attn.k_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.9.self_attn.k_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.9.self_attn.k_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.9.self_attn.k_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.9.self_attn.v_proj.base_layer.weight', 'base_model.model.language_model.model.layers.9.self_attn.v_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.9.self_attn.v_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.9.self_attn.v_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.9.self_attn.v_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.9.self_attn.o_proj.base_layer.weight', 'base_model.model.language_model.model.layers.9.self_attn.o_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.9.self_attn.o_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.9.self_attn.o_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.9.self_attn.o_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.9.mlp.gate_proj.base_layer.weight', 'base_model.model.language_model.model.layers.9.mlp.gate_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.9.mlp.gate_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.9.mlp.gate_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.9.mlp.gate_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.9.mlp.up_proj.base_layer.weight', 'base_model.model.language_model.model.layers.9.mlp.up_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.9.mlp.up_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.9.mlp.up_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.9.mlp.up_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.9.mlp.down_proj.base_layer.weight', 'base_model.model.language_model.model.layers.9.mlp.down_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.9.mlp.down_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.9.mlp.down_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.9.mlp.down_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.9.input_layernorm.weight', 'base_model.model.language_model.model.layers.9.post_attention_layernorm.weight', 'base_model.model.language_model.model.layers.10.self_attn.q_proj.base_layer.weight', 'base_model.model.language_model.model.layers.10.self_attn.q_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.10.self_attn.q_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.10.self_attn.q_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.10.self_attn.q_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.10.self_attn.k_proj.base_layer.weight', 'base_model.model.language_model.model.layers.10.self_attn.k_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.10.self_attn.k_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.10.self_attn.k_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.10.self_attn.k_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.10.self_attn.v_proj.base_layer.weight', 'base_model.model.language_model.model.layers.10.self_attn.v_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.10.self_attn.v_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.10.self_attn.v_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.10.self_attn.v_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.10.self_attn.o_proj.base_layer.weight', 'base_model.model.language_model.model.layers.10.self_attn.o_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.10.self_attn.o_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.10.self_attn.o_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.10.self_attn.o_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.10.mlp.gate_proj.base_layer.weight', 'base_model.model.language_model.model.layers.10.mlp.gate_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.10.mlp.gate_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.10.mlp.gate_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.10.mlp.gate_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.10.mlp.up_proj.base_layer.weight', 'base_model.model.language_model.model.layers.10.mlp.up_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.10.mlp.up_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.10.mlp.up_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.10.mlp.up_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.10.mlp.down_proj.base_layer.weight', 'base_model.model.language_model.model.layers.10.mlp.down_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.10.mlp.down_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.10.mlp.down_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.10.mlp.down_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.10.input_layernorm.weight', 'base_model.model.language_model.model.layers.10.post_attention_layernorm.weight', 'base_model.model.language_model.model.layers.11.self_attn.q_proj.base_layer.weight', 'base_model.model.language_model.model.layers.11.self_attn.q_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.11.self_attn.q_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.11.self_attn.q_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.11.self_attn.q_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.11.self_attn.k_proj.base_layer.weight', 'base_model.model.language_model.model.layers.11.self_attn.k_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.11.self_attn.k_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.11.self_attn.k_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.11.self_attn.k_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.11.self_attn.v_proj.base_layer.weight', 'base_model.model.language_model.model.layers.11.self_attn.v_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.11.self_attn.v_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.11.self_attn.v_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.11.self_attn.v_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.11.self_attn.o_proj.base_layer.weight', 'base_model.model.language_model.model.layers.11.self_attn.o_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.11.self_attn.o_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.11.self_attn.o_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.11.self_attn.o_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.11.mlp.gate_proj.base_layer.weight', 'base_model.model.language_model.model.layers.11.mlp.gate_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.11.mlp.gate_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.11.mlp.gate_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.11.mlp.gate_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.11.mlp.up_proj.base_layer.weight', 'base_model.model.language_model.model.layers.11.mlp.up_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.11.mlp.up_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.11.mlp.up_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.11.mlp.up_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.11.mlp.down_proj.base_layer.weight', 'base_model.model.language_model.model.layers.11.mlp.down_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.11.mlp.down_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.11.mlp.down_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.11.mlp.down_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.11.input_layernorm.weight', 'base_model.model.language_model.model.layers.11.post_attention_layernorm.weight', 'base_model.model.language_model.model.layers.12.self_attn.q_proj.base_layer.weight', 'base_model.model.language_model.model.layers.12.self_attn.q_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.12.self_attn.q_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.12.self_attn.q_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.12.self_attn.q_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.12.self_attn.k_proj.base_layer.weight', 'base_model.model.language_model.model.layers.12.self_attn.k_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.12.self_attn.k_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.12.self_attn.k_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.12.self_attn.k_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.12.self_attn.v_proj.base_layer.weight', 'base_model.model.language_model.model.layers.12.self_attn.v_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.12.self_attn.v_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.12.self_attn.v_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.12.self_attn.v_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.12.self_attn.o_proj.base_layer.weight', 'base_model.model.language_model.model.layers.12.self_attn.o_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.12.self_attn.o_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.12.self_attn.o_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.12.self_attn.o_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.12.mlp.gate_proj.base_layer.weight', 'base_model.model.language_model.model.layers.12.mlp.gate_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.12.mlp.gate_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.12.mlp.gate_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.12.mlp.gate_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.12.mlp.up_proj.base_layer.weight', 'base_model.model.language_model.model.layers.12.mlp.up_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.12.mlp.up_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.12.mlp.up_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.12.mlp.up_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.12.mlp.down_proj.base_layer.weight', 'base_model.model.language_model.model.layers.12.mlp.down_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.12.mlp.down_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.12.mlp.down_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.12.mlp.down_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.12.input_layernorm.weight', 'base_model.model.language_model.model.layers.12.post_attention_layernorm.weight', 'base_model.model.language_model.model.layers.13.self_attn.q_proj.base_layer.weight', 'base_model.model.language_model.model.layers.13.self_attn.q_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.13.self_attn.q_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.13.self_attn.q_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.13.self_attn.q_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.13.self_attn.k_proj.base_layer.weight', 'base_model.model.language_model.model.layers.13.self_attn.k_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.13.self_attn.k_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.13.self_attn.k_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.13.self_attn.k_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.13.self_attn.v_proj.base_layer.weight', 'base_model.model.language_model.model.layers.13.self_attn.v_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.13.self_attn.v_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.13.self_attn.v_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.13.self_attn.v_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.13.self_attn.o_proj.base_layer.weight', 'base_model.model.language_model.model.layers.13.self_attn.o_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.13.self_attn.o_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.13.self_attn.o_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.13.self_attn.o_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.13.mlp.gate_proj.base_layer.weight', 'base_model.model.language_model.model.layers.13.mlp.gate_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.13.mlp.gate_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.13.mlp.gate_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.13.mlp.gate_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.13.mlp.up_proj.base_layer.weight', 'base_model.model.language_model.model.layers.13.mlp.up_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.13.mlp.up_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.13.mlp.up_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.13.mlp.up_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.13.mlp.down_proj.base_layer.weight', 'base_model.model.language_model.model.layers.13.mlp.down_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.13.mlp.down_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.13.mlp.down_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.13.mlp.down_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.13.input_layernorm.weight', 'base_model.model.language_model.model.layers.13.post_attention_layernorm.weight', 'base_model.model.language_model.model.layers.14.self_attn.q_proj.base_layer.weight', 'base_model.model.language_model.model.layers.14.self_attn.q_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.14.self_attn.q_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.14.self_attn.q_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.14.self_attn.q_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.14.self_attn.k_proj.base_layer.weight', 'base_model.model.language_model.model.layers.14.self_attn.k_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.14.self_attn.k_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.14.self_attn.k_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.14.self_attn.k_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.14.self_attn.v_proj.base_layer.weight', 'base_model.model.language_model.model.layers.14.self_attn.v_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.14.self_attn.v_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.14.self_attn.v_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.14.self_attn.v_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.14.self_attn.o_proj.base_layer.weight', 'base_model.model.language_model.model.layers.14.self_attn.o_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.14.self_attn.o_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.14.self_attn.o_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.14.self_attn.o_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.14.mlp.gate_proj.base_layer.weight', 'base_model.model.language_model.model.layers.14.mlp.gate_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.14.mlp.gate_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.14.mlp.gate_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.14.mlp.gate_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.14.mlp.up_proj.base_layer.weight', 'base_model.model.language_model.model.layers.14.mlp.up_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.14.mlp.up_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.14.mlp.up_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.14.mlp.up_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.14.mlp.down_proj.base_layer.weight', 'base_model.model.language_model.model.layers.14.mlp.down_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.14.mlp.down_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.14.mlp.down_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.14.mlp.down_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.14.input_layernorm.weight', 'base_model.model.language_model.model.layers.14.post_attention_layernorm.weight', 'base_model.model.language_model.model.layers.15.self_attn.q_proj.base_layer.weight', 'base_model.model.language_model.model.layers.15.self_attn.q_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.15.self_attn.q_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.15.self_attn.q_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.15.self_attn.q_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.15.self_attn.k_proj.base_layer.weight', 'base_model.model.language_model.model.layers.15.self_attn.k_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.15.self_attn.k_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.15.self_attn.k_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.15.self_attn.k_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.15.self_attn.v_proj.base_layer.weight', 'base_model.model.language_model.model.layers.15.self_attn.v_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.15.self_attn.v_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.15.self_attn.v_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.15.self_attn.v_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.15.self_attn.o_proj.base_layer.weight', 'base_model.model.language_model.model.layers.15.self_attn.o_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.15.self_attn.o_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.15.self_attn.o_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.15.self_attn.o_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.15.mlp.gate_proj.base_layer.weight', 'base_model.model.language_model.model.layers.15.mlp.gate_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.15.mlp.gate_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.15.mlp.gate_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.15.mlp.gate_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.15.mlp.up_proj.base_layer.weight', 'base_model.model.language_model.model.layers.15.mlp.up_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.15.mlp.up_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.15.mlp.up_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.15.mlp.up_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.15.mlp.down_proj.base_layer.weight', 'base_model.model.language_model.model.layers.15.mlp.down_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.15.mlp.down_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.15.mlp.down_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.15.mlp.down_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.15.input_layernorm.weight', 'base_model.model.language_model.model.layers.15.post_attention_layernorm.weight', 'base_model.model.language_model.model.layers.16.self_attn.q_proj.base_layer.weight', 'base_model.model.language_model.model.layers.16.self_attn.q_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.16.self_attn.q_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.16.self_attn.q_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.16.self_attn.q_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.16.self_attn.k_proj.base_layer.weight', 'base_model.model.language_model.model.layers.16.self_attn.k_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.16.self_attn.k_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.16.self_attn.k_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.16.self_attn.k_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.16.self_attn.v_proj.base_layer.weight', 'base_model.model.language_model.model.layers.16.self_attn.v_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.16.self_attn.v_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.16.self_attn.v_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.16.self_attn.v_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.16.self_attn.o_proj.base_layer.weight', 'base_model.model.language_model.model.layers.16.self_attn.o_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.16.self_attn.o_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.16.self_attn.o_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.16.self_attn.o_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.16.mlp.gate_proj.base_layer.weight', 'base_model.model.language_model.model.layers.16.mlp.gate_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.16.mlp.gate_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.16.mlp.gate_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.16.mlp.gate_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.16.mlp.up_proj.base_layer.weight', 'base_model.model.language_model.model.layers.16.mlp.up_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.16.mlp.up_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.16.mlp.up_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.16.mlp.up_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.16.mlp.down_proj.base_layer.weight', 'base_model.model.language_model.model.layers.16.mlp.down_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.16.mlp.down_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.16.mlp.down_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.16.mlp.down_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.16.input_layernorm.weight', 'base_model.model.language_model.model.layers.16.post_attention_layernorm.weight', 'base_model.model.language_model.model.layers.17.self_attn.q_proj.base_layer.weight', 'base_model.model.language_model.model.layers.17.self_attn.q_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.17.self_attn.q_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.17.self_attn.q_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.17.self_attn.q_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.17.self_attn.k_proj.base_layer.weight', 'base_model.model.language_model.model.layers.17.self_attn.k_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.17.self_attn.k_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.17.self_attn.k_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.17.self_attn.k_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.17.self_attn.v_proj.base_layer.weight', 'base_model.model.language_model.model.layers.17.self_attn.v_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.17.self_attn.v_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.17.self_attn.v_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.17.self_attn.v_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.17.self_attn.o_proj.base_layer.weight', 'base_model.model.language_model.model.layers.17.self_attn.o_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.17.self_attn.o_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.17.self_attn.o_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.17.self_attn.o_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.17.mlp.gate_proj.base_layer.weight', 'base_model.model.language_model.model.layers.17.mlp.gate_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.17.mlp.gate_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.17.mlp.gate_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.17.mlp.gate_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.17.mlp.up_proj.base_layer.weight', 'base_model.model.language_model.model.layers.17.mlp.up_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.17.mlp.up_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.17.mlp.up_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.17.mlp.up_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.17.mlp.down_proj.base_layer.weight', 'base_model.model.language_model.model.layers.17.mlp.down_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.17.mlp.down_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.17.mlp.down_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.17.mlp.down_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.17.input_layernorm.weight', 'base_model.model.language_model.model.layers.17.post_attention_layernorm.weight', 'base_model.model.language_model.model.layers.18.self_attn.q_proj.base_layer.weight', 'base_model.model.language_model.model.layers.18.self_attn.q_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.18.self_attn.q_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.18.self_attn.q_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.18.self_attn.q_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.18.self_attn.k_proj.base_layer.weight', 'base_model.model.language_model.model.layers.18.self_attn.k_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.18.self_attn.k_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.18.self_attn.k_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.18.self_attn.k_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.18.self_attn.v_proj.base_layer.weight', 'base_model.model.language_model.model.layers.18.self_attn.v_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.18.self_attn.v_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.18.self_attn.v_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.18.self_attn.v_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.18.self_attn.o_proj.base_layer.weight', 'base_model.model.language_model.model.layers.18.self_attn.o_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.18.self_attn.o_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.18.self_attn.o_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.18.self_attn.o_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.18.mlp.gate_proj.base_layer.weight', 'base_model.model.language_model.model.layers.18.mlp.gate_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.18.mlp.gate_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.18.mlp.gate_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.18.mlp.gate_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.18.mlp.up_proj.base_layer.weight', 'base_model.model.language_model.model.layers.18.mlp.up_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.18.mlp.up_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.18.mlp.up_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.18.mlp.up_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.18.mlp.down_proj.base_layer.weight', 'base_model.model.language_model.model.layers.18.mlp.down_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.18.mlp.down_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.18.mlp.down_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.18.mlp.down_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.18.input_layernorm.weight', 'base_model.model.language_model.model.layers.18.post_attention_layernorm.weight', 'base_model.model.language_model.model.layers.19.self_attn.q_proj.base_layer.weight', 'base_model.model.language_model.model.layers.19.self_attn.q_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.19.self_attn.q_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.19.self_attn.q_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.19.self_attn.q_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.19.self_attn.k_proj.base_layer.weight', 'base_model.model.language_model.model.layers.19.self_attn.k_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.19.self_attn.k_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.19.self_attn.k_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.19.self_attn.k_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.19.self_attn.v_proj.base_layer.weight', 'base_model.model.language_model.model.layers.19.self_attn.v_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.19.self_attn.v_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.19.self_attn.v_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.19.self_attn.v_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.19.self_attn.o_proj.base_layer.weight', 'base_model.model.language_model.model.layers.19.self_attn.o_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.19.self_attn.o_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.19.self_attn.o_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.19.self_attn.o_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.19.mlp.gate_proj.base_layer.weight', 'base_model.model.language_model.model.layers.19.mlp.gate_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.19.mlp.gate_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.19.mlp.gate_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.19.mlp.gate_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.19.mlp.up_proj.base_layer.weight', 'base_model.model.language_model.model.layers.19.mlp.up_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.19.mlp.up_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.19.mlp.up_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.19.mlp.up_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.19.mlp.down_proj.base_layer.weight', 'base_model.model.language_model.model.layers.19.mlp.down_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.19.mlp.down_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.19.mlp.down_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.19.mlp.down_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.19.input_layernorm.weight', 'base_model.model.language_model.model.layers.19.post_attention_layernorm.weight', 'base_model.model.language_model.model.layers.20.self_attn.q_proj.base_layer.weight', 'base_model.model.language_model.model.layers.20.self_attn.q_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.20.self_attn.q_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.20.self_attn.q_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.20.self_attn.q_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.20.self_attn.k_proj.base_layer.weight', 'base_model.model.language_model.model.layers.20.self_attn.k_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.20.self_attn.k_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.20.self_attn.k_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.20.self_attn.k_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.20.self_attn.v_proj.base_layer.weight', 'base_model.model.language_model.model.layers.20.self_attn.v_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.20.self_attn.v_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.20.self_attn.v_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.20.self_attn.v_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.20.self_attn.o_proj.base_layer.weight', 'base_model.model.language_model.model.layers.20.self_attn.o_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.20.self_attn.o_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.20.self_attn.o_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.20.self_attn.o_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.20.mlp.gate_proj.base_layer.weight', 'base_model.model.language_model.model.layers.20.mlp.gate_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.20.mlp.gate_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.20.mlp.gate_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.20.mlp.gate_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.20.mlp.up_proj.base_layer.weight', 'base_model.model.language_model.model.layers.20.mlp.up_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.20.mlp.up_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.20.mlp.up_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.20.mlp.up_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.20.mlp.down_proj.base_layer.weight', 'base_model.model.language_model.model.layers.20.mlp.down_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.20.mlp.down_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.20.mlp.down_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.20.mlp.down_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.20.input_layernorm.weight', 'base_model.model.language_model.model.layers.20.post_attention_layernorm.weight', 'base_model.model.language_model.model.layers.21.self_attn.q_proj.base_layer.weight', 'base_model.model.language_model.model.layers.21.self_attn.q_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.21.self_attn.q_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.21.self_attn.q_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.21.self_attn.q_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.21.self_attn.k_proj.base_layer.weight', 'base_model.model.language_model.model.layers.21.self_attn.k_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.21.self_attn.k_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.21.self_attn.k_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.21.self_attn.k_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.21.self_attn.v_proj.base_layer.weight', 'base_model.model.language_model.model.layers.21.self_attn.v_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.21.self_attn.v_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.21.self_attn.v_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.21.self_attn.v_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.21.self_attn.o_proj.base_layer.weight', 'base_model.model.language_model.model.layers.21.self_attn.o_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.21.self_attn.o_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.21.self_attn.o_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.21.self_attn.o_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.21.mlp.gate_proj.base_layer.weight', 'base_model.model.language_model.model.layers.21.mlp.gate_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.21.mlp.gate_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.21.mlp.gate_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.21.mlp.gate_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.21.mlp.up_proj.base_layer.weight', 'base_model.model.language_model.model.layers.21.mlp.up_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.21.mlp.up_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.21.mlp.up_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.21.mlp.up_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.21.mlp.down_proj.base_layer.weight', 'base_model.model.language_model.model.layers.21.mlp.down_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.21.mlp.down_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.21.mlp.down_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.21.mlp.down_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.21.input_layernorm.weight', 'base_model.model.language_model.model.layers.21.post_attention_layernorm.weight', 'base_model.model.language_model.model.layers.22.self_attn.q_proj.base_layer.weight', 'base_model.model.language_model.model.layers.22.self_attn.q_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.22.self_attn.q_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.22.self_attn.q_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.22.self_attn.q_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.22.self_attn.k_proj.base_layer.weight', 'base_model.model.language_model.model.layers.22.self_attn.k_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.22.self_attn.k_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.22.self_attn.k_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.22.self_attn.k_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.22.self_attn.v_proj.base_layer.weight', 'base_model.model.language_model.model.layers.22.self_attn.v_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.22.self_attn.v_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.22.self_attn.v_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.22.self_attn.v_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.22.self_attn.o_proj.base_layer.weight', 'base_model.model.language_model.model.layers.22.self_attn.o_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.22.self_attn.o_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.22.self_attn.o_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.22.self_attn.o_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.22.mlp.gate_proj.base_layer.weight', 'base_model.model.language_model.model.layers.22.mlp.gate_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.22.mlp.gate_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.22.mlp.gate_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.22.mlp.gate_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.22.mlp.up_proj.base_layer.weight', 'base_model.model.language_model.model.layers.22.mlp.up_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.22.mlp.up_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.22.mlp.up_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.22.mlp.up_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.22.mlp.down_proj.base_layer.weight', 'base_model.model.language_model.model.layers.22.mlp.down_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.22.mlp.down_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.22.mlp.down_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.22.mlp.down_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.22.input_layernorm.weight', 'base_model.model.language_model.model.layers.22.post_attention_layernorm.weight', 'base_model.model.language_model.model.layers.23.self_attn.q_proj.base_layer.weight', 'base_model.model.language_model.model.layers.23.self_attn.q_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.23.self_attn.q_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.23.self_attn.q_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.23.self_attn.q_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.23.self_attn.k_proj.base_layer.weight', 'base_model.model.language_model.model.layers.23.self_attn.k_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.23.self_attn.k_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.23.self_attn.k_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.23.self_attn.k_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.23.self_attn.v_proj.base_layer.weight', 'base_model.model.language_model.model.layers.23.self_attn.v_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.23.self_attn.v_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.23.self_attn.v_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.23.self_attn.v_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.23.self_attn.o_proj.base_layer.weight', 'base_model.model.language_model.model.layers.23.self_attn.o_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.23.self_attn.o_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.23.self_attn.o_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.23.self_attn.o_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.23.mlp.gate_proj.base_layer.weight', 'base_model.model.language_model.model.layers.23.mlp.gate_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.23.mlp.gate_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.23.mlp.gate_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.23.mlp.gate_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.23.mlp.up_proj.base_layer.weight', 'base_model.model.language_model.model.layers.23.mlp.up_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.23.mlp.up_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.23.mlp.up_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.23.mlp.up_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.23.mlp.down_proj.base_layer.weight', 'base_model.model.language_model.model.layers.23.mlp.down_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.23.mlp.down_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.23.mlp.down_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.23.mlp.down_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.23.input_layernorm.weight', 'base_model.model.language_model.model.layers.23.post_attention_layernorm.weight', 'base_model.model.language_model.model.layers.24.self_attn.q_proj.base_layer.weight', 'base_model.model.language_model.model.layers.24.self_attn.q_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.24.self_attn.q_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.24.self_attn.q_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.24.self_attn.q_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.24.self_attn.k_proj.base_layer.weight', 'base_model.model.language_model.model.layers.24.self_attn.k_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.24.self_attn.k_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.24.self_attn.k_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.24.self_attn.k_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.24.self_attn.v_proj.base_layer.weight', 'base_model.model.language_model.model.layers.24.self_attn.v_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.24.self_attn.v_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.24.self_attn.v_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.24.self_attn.v_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.24.self_attn.o_proj.base_layer.weight', 'base_model.model.language_model.model.layers.24.self_attn.o_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.24.self_attn.o_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.24.self_attn.o_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.24.self_attn.o_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.24.mlp.gate_proj.base_layer.weight', 'base_model.model.language_model.model.layers.24.mlp.gate_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.24.mlp.gate_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.24.mlp.gate_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.24.mlp.gate_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.24.mlp.up_proj.base_layer.weight', 'base_model.model.language_model.model.layers.24.mlp.up_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.24.mlp.up_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.24.mlp.up_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.24.mlp.up_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.24.mlp.down_proj.base_layer.weight', 'base_model.model.language_model.model.layers.24.mlp.down_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.24.mlp.down_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.24.mlp.down_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.24.mlp.down_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.24.input_layernorm.weight', 'base_model.model.language_model.model.layers.24.post_attention_layernorm.weight', 'base_model.model.language_model.model.layers.25.self_attn.q_proj.base_layer.weight', 'base_model.model.language_model.model.layers.25.self_attn.q_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.25.self_attn.q_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.25.self_attn.q_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.25.self_attn.q_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.25.self_attn.k_proj.base_layer.weight', 'base_model.model.language_model.model.layers.25.self_attn.k_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.25.self_attn.k_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.25.self_attn.k_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.25.self_attn.k_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.25.self_attn.v_proj.base_layer.weight', 'base_model.model.language_model.model.layers.25.self_attn.v_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.25.self_attn.v_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.25.self_attn.v_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.25.self_attn.v_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.25.self_attn.o_proj.base_layer.weight', 'base_model.model.language_model.model.layers.25.self_attn.o_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.25.self_attn.o_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.25.self_attn.o_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.25.self_attn.o_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.25.mlp.gate_proj.base_layer.weight', 'base_model.model.language_model.model.layers.25.mlp.gate_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.25.mlp.gate_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.25.mlp.gate_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.25.mlp.gate_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.25.mlp.up_proj.base_layer.weight', 'base_model.model.language_model.model.layers.25.mlp.up_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.25.mlp.up_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.25.mlp.up_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.25.mlp.up_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.25.mlp.down_proj.base_layer.weight', 'base_model.model.language_model.model.layers.25.mlp.down_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.25.mlp.down_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.25.mlp.down_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.25.mlp.down_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.25.input_layernorm.weight', 'base_model.model.language_model.model.layers.25.post_attention_layernorm.weight', 'base_model.model.language_model.model.layers.26.self_attn.q_proj.base_layer.weight', 'base_model.model.language_model.model.layers.26.self_attn.q_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.26.self_attn.q_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.26.self_attn.q_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.26.self_attn.q_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.26.self_attn.k_proj.base_layer.weight', 'base_model.model.language_model.model.layers.26.self_attn.k_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.26.self_attn.k_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.26.self_attn.k_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.26.self_attn.k_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.26.self_attn.v_proj.base_layer.weight', 'base_model.model.language_model.model.layers.26.self_attn.v_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.26.self_attn.v_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.26.self_attn.v_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.26.self_attn.v_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.26.self_attn.o_proj.base_layer.weight', 'base_model.model.language_model.model.layers.26.self_attn.o_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.26.self_attn.o_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.26.self_attn.o_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.26.self_attn.o_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.26.mlp.gate_proj.base_layer.weight', 'base_model.model.language_model.model.layers.26.mlp.gate_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.26.mlp.gate_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.26.mlp.gate_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.26.mlp.gate_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.26.mlp.up_proj.base_layer.weight', 'base_model.model.language_model.model.layers.26.mlp.up_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.26.mlp.up_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.26.mlp.up_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.26.mlp.up_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.26.mlp.down_proj.base_layer.weight', 'base_model.model.language_model.model.layers.26.mlp.down_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.26.mlp.down_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.26.mlp.down_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.26.mlp.down_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.26.input_layernorm.weight', 'base_model.model.language_model.model.layers.26.post_attention_layernorm.weight', 'base_model.model.language_model.model.layers.27.self_attn.q_proj.base_layer.weight', 'base_model.model.language_model.model.layers.27.self_attn.q_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.27.self_attn.q_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.27.self_attn.q_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.27.self_attn.q_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.27.self_attn.k_proj.base_layer.weight', 'base_model.model.language_model.model.layers.27.self_attn.k_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.27.self_attn.k_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.27.self_attn.k_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.27.self_attn.k_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.27.self_attn.v_proj.base_layer.weight', 'base_model.model.language_model.model.layers.27.self_attn.v_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.27.self_attn.v_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.27.self_attn.v_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.27.self_attn.v_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.27.self_attn.o_proj.base_layer.weight', 'base_model.model.language_model.model.layers.27.self_attn.o_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.27.self_attn.o_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.27.self_attn.o_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.27.self_attn.o_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.27.mlp.gate_proj.base_layer.weight', 'base_model.model.language_model.model.layers.27.mlp.gate_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.27.mlp.gate_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.27.mlp.gate_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.27.mlp.gate_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.27.mlp.up_proj.base_layer.weight', 'base_model.model.language_model.model.layers.27.mlp.up_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.27.mlp.up_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.27.mlp.up_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.27.mlp.up_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.27.mlp.down_proj.base_layer.weight', 'base_model.model.language_model.model.layers.27.mlp.down_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.27.mlp.down_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.27.mlp.down_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.27.mlp.down_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.27.input_layernorm.weight', 'base_model.model.language_model.model.layers.27.post_attention_layernorm.weight', 'base_model.model.language_model.model.layers.28.self_attn.q_proj.base_layer.weight', 'base_model.model.language_model.model.layers.28.self_attn.q_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.28.self_attn.q_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.28.self_attn.q_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.28.self_attn.q_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.28.self_attn.k_proj.base_layer.weight', 'base_model.model.language_model.model.layers.28.self_attn.k_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.28.self_attn.k_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.28.self_attn.k_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.28.self_attn.k_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.28.self_attn.v_proj.base_layer.weight', 'base_model.model.language_model.model.layers.28.self_attn.v_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.28.self_attn.v_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.28.self_attn.v_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.28.self_attn.v_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.28.self_attn.o_proj.base_layer.weight', 'base_model.model.language_model.model.layers.28.self_attn.o_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.28.self_attn.o_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.28.self_attn.o_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.28.self_attn.o_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.28.mlp.gate_proj.base_layer.weight', 'base_model.model.language_model.model.layers.28.mlp.gate_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.28.mlp.gate_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.28.mlp.gate_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.28.mlp.gate_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.28.mlp.up_proj.base_layer.weight', 'base_model.model.language_model.model.layers.28.mlp.up_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.28.mlp.up_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.28.mlp.up_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.28.mlp.up_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.28.mlp.down_proj.base_layer.weight', 'base_model.model.language_model.model.layers.28.mlp.down_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.28.mlp.down_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.28.mlp.down_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.28.mlp.down_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.28.input_layernorm.weight', 'base_model.model.language_model.model.layers.28.post_attention_layernorm.weight', 'base_model.model.language_model.model.layers.29.self_attn.q_proj.base_layer.weight', 'base_model.model.language_model.model.layers.29.self_attn.q_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.29.self_attn.q_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.29.self_attn.q_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.29.self_attn.q_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.29.self_attn.k_proj.base_layer.weight', 'base_model.model.language_model.model.layers.29.self_attn.k_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.29.self_attn.k_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.29.self_attn.k_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.29.self_attn.k_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.29.self_attn.v_proj.base_layer.weight', 'base_model.model.language_model.model.layers.29.self_attn.v_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.29.self_attn.v_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.29.self_attn.v_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.29.self_attn.v_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.29.self_attn.o_proj.base_layer.weight', 'base_model.model.language_model.model.layers.29.self_attn.o_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.29.self_attn.o_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.29.self_attn.o_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.29.self_attn.o_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.29.mlp.gate_proj.base_layer.weight', 'base_model.model.language_model.model.layers.29.mlp.gate_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.29.mlp.gate_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.29.mlp.gate_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.29.mlp.gate_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.29.mlp.up_proj.base_layer.weight', 'base_model.model.language_model.model.layers.29.mlp.up_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.29.mlp.up_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.29.mlp.up_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.29.mlp.up_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.29.mlp.down_proj.base_layer.weight', 'base_model.model.language_model.model.layers.29.mlp.down_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.29.mlp.down_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.29.mlp.down_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.29.mlp.down_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.29.input_layernorm.weight', 'base_model.model.language_model.model.layers.29.post_attention_layernorm.weight', 'base_model.model.language_model.model.layers.30.self_attn.q_proj.base_layer.weight', 'base_model.model.language_model.model.layers.30.self_attn.q_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.30.self_attn.q_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.30.self_attn.q_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.30.self_attn.q_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.30.self_attn.k_proj.base_layer.weight', 'base_model.model.language_model.model.layers.30.self_attn.k_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.30.self_attn.k_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.30.self_attn.k_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.30.self_attn.k_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.30.self_attn.v_proj.base_layer.weight', 'base_model.model.language_model.model.layers.30.self_attn.v_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.30.self_attn.v_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.30.self_attn.v_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.30.self_attn.v_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.30.self_attn.o_proj.base_layer.weight', 'base_model.model.language_model.model.layers.30.self_attn.o_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.30.self_attn.o_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.30.self_attn.o_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.30.self_attn.o_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.30.mlp.gate_proj.base_layer.weight', 'base_model.model.language_model.model.layers.30.mlp.gate_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.30.mlp.gate_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.30.mlp.gate_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.30.mlp.gate_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.30.mlp.up_proj.base_layer.weight', 'base_model.model.language_model.model.layers.30.mlp.up_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.30.mlp.up_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.30.mlp.up_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.30.mlp.up_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.30.mlp.down_proj.base_layer.weight', 'base_model.model.language_model.model.layers.30.mlp.down_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.30.mlp.down_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.30.mlp.down_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.30.mlp.down_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.30.input_layernorm.weight', 'base_model.model.language_model.model.layers.30.post_attention_layernorm.weight', 'base_model.model.language_model.model.layers.31.self_attn.q_proj.base_layer.weight', 'base_model.model.language_model.model.layers.31.self_attn.q_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.31.self_attn.q_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.31.self_attn.q_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.31.self_attn.q_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.31.self_attn.k_proj.base_layer.weight', 'base_model.model.language_model.model.layers.31.self_attn.k_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.31.self_attn.k_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.31.self_attn.k_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.31.self_attn.k_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.31.self_attn.v_proj.base_layer.weight', 'base_model.model.language_model.model.layers.31.self_attn.v_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.31.self_attn.v_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.31.self_attn.v_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.31.self_attn.v_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.31.self_attn.o_proj.base_layer.weight', 'base_model.model.language_model.model.layers.31.self_attn.o_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.31.self_attn.o_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.31.self_attn.o_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.31.self_attn.o_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.31.mlp.gate_proj.base_layer.weight', 'base_model.model.language_model.model.layers.31.mlp.gate_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.31.mlp.gate_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.31.mlp.gate_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.31.mlp.gate_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.31.mlp.up_proj.base_layer.weight', 'base_model.model.language_model.model.layers.31.mlp.up_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.31.mlp.up_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.31.mlp.up_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.31.mlp.up_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.31.mlp.down_proj.base_layer.weight', 'base_model.model.language_model.model.layers.31.mlp.down_proj.lora_A.actor_critic.weight', 'base_model.model.language_model.model.layers.31.mlp.down_proj.lora_A.actor_critic1.weight', 'base_model.model.language_model.model.layers.31.mlp.down_proj.lora_B.actor_critic.weight', 'base_model.model.language_model.model.layers.31.mlp.down_proj.lora_B.actor_critic1.weight', 'base_model.model.language_model.model.layers.31.input_layernorm.weight', 'base_model.model.language_model.model.layers.31.post_attention_layernorm.weight', 'base_model.model.language_model.model.norm.weight', 'base_model.model.language_model.lm_head.lora_A.actor_critic.weight', 'base_model.model.language_model.lm_head.lora_A.actor_critic1.weight', 'base_model.model.language_model.lm_head.lora_B.actor_critic.weight', 'base_model.model.language_model.lm_head.lora_B.actor_critic1.weight'], unexpected_keys=[])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vla = PeftModel.from_pretrained(base_model, adapter_path, adapter_name=\"actor_critic\")\n",
    "vla.load_adapter(adapter_path1, adapter_name=\"actor_critic1\")\n",
    "vla.load_adapter(adapter_path2, adapter_name=\"actor_critic2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_instruct_prompt(action_tokenizer, gripper, instruction: str):\n",
    "    # gripper = action_tokenizer(gripper)\n",
    "    prompt = (\n",
    "        f\"In: What should be the next key pose of the gripper to {instruction}? The current gripper pose is <g>{gripper} </g>.\\n \"\n",
    "        \"Out: <cot> \"\n",
    "    )\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    1,   512, 29901,  1724,   881,   367,   278,  2446,  1820, 18593,\n",
       "          310,   278,   330,   374,  2496,   304, 29871, 29896, 29973,   450,\n",
       "         1857,   330,   374,  2496, 18593,   338, 32001, 29896, 32002, 29889,\n",
       "           13,  4451, 29901, 32017, 29871])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(processor.tokenizer(get_instruct_prompt(None,1,1)).input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    1,   512, 29901,  1724,   881,   367,   278,  2446,  1820, 18593,\n",
       "           310,   278,   330,   374,  2496,   304,  4337,   278, 26438,  3800,\n",
       "           304,   278, 25972, 29973,   450,  1857,   330,   374,  2496, 18593,\n",
       "           338, 32001, 31417, 31501, 31668, 31747, 31888, 31995, 31998, 32002,\n",
       "         29889,    13,  4451, 29901, 32017, 32007, 29892, 32008, 31437, 31588,\n",
       "         31613, 32009, 29892, 32010, 31398, 31501, 31643, 32011, 29892, 32012,\n",
       "         29892, 32014, 31417, 31501, 31668, 31747, 31888, 31995, 31999, 32015,\n",
       "         29892, 32016, 32000,     2],\n",
       "        [    1,   512, 29901,  1724,   881,   367,   278,  2446,  1820, 18593,\n",
       "           310,   278,   330,   374,  2496,   304,  1925,   278, 26438,  3800,\n",
       "           297,   278, 25972, 29973,   450,  1857,   330,   374,  2496, 18593,\n",
       "           338, 32001, 31425, 31579, 31669, 31764, 31856, 31926, 31999, 32002,\n",
       "         29889,    13,  4451, 29901, 32017, 32007, 29892, 32008, 31437, 31588,\n",
       "         31613, 32009, 29892, 32010, 31398, 31501, 31643, 32011, 29892, 32012,\n",
       "         29892, 32014, 31437, 31588, 31623, 31747, 31897, 31974, 31998, 32015,\n",
       "         29892, 32016, 32000,     2]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['input_ids'][:,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = get_instruct_prompt(gripper,instr)\n",
    "# image = Image.fromarray(obs.front_rgb)\n",
    "# inputs = processor(prompt, image).to(vla.device, dtype=torch.bfloat16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    1,   512, 29901,  1724,   881,   367,   278,  2446,  1820, 18593,\n",
       "           310,   278,   330,   374,  2496,   304,  4337,   278, 26438,  3800,\n",
       "           304,   278, 25972, 29973,   450,  1857,   330,   374,  2496, 18593,\n",
       "           338, 32001, 31417, 31501, 31668, 31747, 31888, 31995, 31998, 32002,\n",
       "         29889,    13,  4451, 29901, 32017],\n",
       "        [    1,   512, 29901,  1724,   881,   367,   278,  2446,  1820, 18593,\n",
       "           310,   278,   330,   374,  2496,   304,  1925,   278, 26438,  3800,\n",
       "           297,   278, 25972, 29973,   450,  1857,   330,   374,  2496, 18593,\n",
       "           338, 32001, 31425, 31579, 31669, 31764, 31856, 31926, 31999, 32002,\n",
       "         29889,    13,  4451, 29901, 32017]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['input_ids'][:,:-29]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 224, 224])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"pixel_values\"][1:2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vla.set_adapter(\"actor_critic\")\n",
    "with torch.no_grad(), torch.autocast(\"cuda\", dtype=torch.bfloat16):\n",
    "    output_dict = vla.generate(\n",
    "        input_ids = batch['input_ids'][1:2,:-29].to(vla.device),\n",
    "        pixel_values=batch[\"pixel_values\"][1:2].to(torch.bfloat16).to(vla.device),\n",
    "        max_new_tokens = 28,\n",
    "        do_sample=False,\n",
    "        temperature=1,\n",
    "        return_dict_in_generate=True,\n",
    "        output_scores = True,\n",
    "        \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, _, _,_, action_mask = action_tokenizer.get_mask(output_dict.sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaTokenizerFast(name_or_path='/media/lawrence/Work/checkpoints/ecot-openvla-7b-bridge', vocab_size=32000, model_max_length=2048, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<PAD>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32000: AddedToken(\"<PAD>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32001: AddedToken(\"<g>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t32002: AddedToken(\"</g>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t32003: AddedToken(\"<item_0>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t32004: AddedToken(\"<item_1>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t32005: AddedToken(\"<item_2>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t32006: AddedToken(\"<item_3>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t32007: AddedToken(\"<item_4>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t32008: AddedToken(\"<o>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t32009: AddedToken(\"</o>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t32010: AddedToken(\"<t>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t32011: AddedToken(\"</t>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t32012: AddedToken(\"<stage_0>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t32013: AddedToken(\"<stage_1>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t32014: AddedToken(\"<a>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t32015: AddedToken(\"</a>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t32016: AddedToken(\"<q>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t32017: AddedToken(\"<cot>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> In: What should be the next key pose of the gripper to put the sugar box in the basket? The current gripper pose is<g>七思节ɫ터𝓝给</g>.\\n Out:<cot><item_4>,<o>认ˆ态</o>,<t>交ペམ</t>,<stage_0>,<a>认ˆ್면ḳ收弘</a>,<q>,'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.tokenizer.decode(output_dict.sequences[0].cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "    def __init__(self, vla, processor, action_tokenizer):\n",
    "        self.vla = vla\n",
    "        self.processor = processor\n",
    "        self.action_tokenizer = action_tokenizer\n",
    "\n",
    "    def get_openvla_prompt(self, instruction: str):\n",
    "        SYSTEM_PROMPT = \"You are an assistant helping to control a robotic manipulator. The robot performs tasks by following a series of steps to interact with objects in its environment. The environment includes items like soup cans and baskets, and the robot uses a gripper to pick up and move these items.\\n\\nInstructions format:\\n- 'USER': Describes the task to be performed.\\n- 'ASSISTANT': Provides a detailed step-by-step plan for the robot to execute the task.\\n\\nThe 'ASSISTANT' response includes:\\n1. A logical step-by-step plan for the task.\\n2. The current positions of relevant objects and the gripper.\\n3. The current state of the gripper (whether it has grasped the object or not).\\n4. The next key pose of the gripper to achieve the task.\\n\\nExample:\\n\\nUSER: What action should the robot take to pick up the soup and place it in the basket?\\nASSISTANT: Let's think step by step. The plan is to move the gripper to the soup and pick it up, then move over the basket, and then place the soup in the basket. The soup is located at <object>ĉ‖호 </object>. The basket is located at <target>Ζ‖ご </target>. The gripper pose is <gripper>阳‖素군雅导弘 </gripper>. The gripper hasn't grasped the soup. So the current step is to move the gripper to the soup and pick it up. The next key pose of the gripper is <action>机‖素秀麻방弘 </action>. \\n <current conversation>\"\n",
    "        return f\"{SYSTEM_PROMPT} USER: What action should the robot take to {instruction.lower()}? ASSISTANT: Let's think step by step,\"\n",
    "\n",
    "    def get_instruct_prompt(self, gripper, instruction: str):\n",
    "        gripper = self.action_tokenizer(gripper)\n",
    "        prompt = (\n",
    "            f\"In: What should be the next key pose of the gripper to {instruction}? The current gripper pose is <g>{gripper} </g>.\\n \"\n",
    "            \"Out: <cot>\"\n",
    "        )\n",
    "        return prompt\n",
    "\n",
    "    def act(self, obs, instr, temperature = 1, deterministic = False):\n",
    "        gripper = np.concatenate([obs.gripper_pose,[obs.gripper_open]])\n",
    "        prompt = self.get_instruct_prompt(gripper,instr)\n",
    "        image = Image.fromarray(obs.front_rgb)\n",
    "        inputs = self.processor(prompt, image).to(self.vla.device, dtype=torch.bfloat16)\n",
    "        while True:\n",
    "            with torch.autocast(\"cuda\", dtype=torch.bfloat16):\n",
    "                output_dict = vla.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens = 28,\n",
    "                    do_sample=False,\n",
    "                    temperature=1,\n",
    "                    return_dict_in_generate=True,\n",
    "                    output_scores = True,\n",
    "                )\n",
    "            # output_dict = vla.generate(**inputs, max_new_tokens = 50, output_scores = True, return_dict_in_generate=True, do_sample = True, temperature = 0.5)\n",
    "            gripper_mask, item_mask, object_mask, target_mask, stage_mask, action_mask = action_tokenizer.get_mask(output_dict.sequences)\n",
    "            if action_mask.sum().item() != 7:\n",
    "                print(\"Action mask is not correct\")\n",
    "                continue\n",
    "            break\n",
    "        print(processor.tokenizer.decode(output_dict.sequences[0]))\n",
    "        output_logits = torch.stack(output_dict.scores, dim = 1)\n",
    "        action_logits = output_logits[action_mask[:,-output_logits.size(1):]][:,action_tokenizer.action_token_begin_idx:processor.tokenizer.vocab_size].view(1,-1,action_tokenizer.n_bins)\n",
    "        action = self.action_tokenizer.get_action(action_logits,temperature = temperature, deterministic = deterministic)\n",
    "        # get_action(self, logits: torch.tensor, temperature: float = 1, deterministic: bool = False)\n",
    "        q = output_logits[:,-1, 32016]\n",
    "\n",
    "        return action, q\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(vla,processor,action_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"\n",
    "    A simple FIFO experience replay buffer for DDPG agents.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, obs_dim, act_dim, size):\n",
    "        self.obs_buf = np.zeros(core.combined_shape(size, obs_dim), dtype=np.float32)\n",
    "        self.obs2_buf = np.zeros(core.combined_shape(size, obs_dim), dtype=np.float32)\n",
    "        self.act_buf = np.zeros(core.combined_shape(size, act_dim), dtype=np.float32)\n",
    "        self.rew_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.done_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.ptr, self.size, self.max_size = 0, 0, size\n",
    "\n",
    "    def store(self, obs, act, rew, next_obs, done):\n",
    "        self.obs_buf[self.ptr] = obs\n",
    "        self.obs2_buf[self.ptr] = next_obs\n",
    "        self.act_buf[self.ptr] = act\n",
    "        self.rew_buf[self.ptr] = rew\n",
    "        self.done_buf[self.ptr] = done\n",
    "        self.ptr = (self.ptr+1) % self.max_size\n",
    "        self.size = min(self.size+1, self.max_size)\n",
    "\n",
    "    def sample_batch(self, batch_size=32):\n",
    "        idxs = np.random.randint(0, self.size, size=batch_size)\n",
    "        batch = dict(obs=self.obs_buf[idxs],\n",
    "                     obs2=self.obs2_buf[idxs],\n",
    "                     act=self.act_buf[idxs],\n",
    "                     rew=self.rew_buf[idxs],\n",
    "                     done=self.done_buf[idxs])\n",
    "        return {k: torch.as_tensor(v, dtype=torch.float32) for k,v in batch.items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RLbench Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CoppeliaSim:loadinfo]   done.\n"
     ]
    }
   ],
   "source": [
    "env.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera = CameraConfig(image_size=(224, 224), depth=False, point_cloud=False, mask=False)\n",
    "obs_config = ObservationConfig(left_shoulder_camera=camera, right_shoulder_camera=camera, front_camera=camera, overhead_camera=camera)\n",
    "obs_config.front_camera.render_mode = RenderMode.OPENGL\n",
    "\n",
    "env = Environment(\n",
    "    action_mode=MoveArmThenGripper(\n",
    "        arm_action_mode=EndEffectorPoseViaPlanning(absolute_mode=True, collision_checking=False), gripper_action_mode=Discrete()),\n",
    "    obs_config=obs_config,\n",
    "    headless=False, shaped_rewards = True)\n",
    "env.launch()\n",
    "task = env.get_task(PickDescribedObject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'actor_critic1'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.vla.set_adapter(\"actor_critic\")\n",
    "agent.vla.active_adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8083"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<rlbench.backend.observation.Observation at 0x7b9e0397e310>,\n",
       " -0.8147093484236813,\n",
       " False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task = env.get_task(PickDescribedObject)\n",
    "task.sample_variation()\n",
    "descriptions, obs = task.reset()\n",
    "task.step(np.concatenate([task._task.get_waypoints()[1]._waypoint.get_pose(),[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<rlbench.backend.observation.Observation at 0x7b9e0397da90>, 0, False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task.step(np.concatenate([task._task.get_waypoints()[0]._waypoint.get_pose(),[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<rlbench.backend.observation.Observation at 0x7b9e0397e5d0>, 0, True)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task.step(np.concatenate([task._task.get_waypoints()[1]._waypoint.get_pose(),[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.038983672857284546, 0.03901934623718262]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task._task.robot.gripper.get_joint_positions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.99999928e-01, -3.24999958e-01,  1.20000005e+00,  4.37113883e-08,\n",
       "        1.00000000e+00,  7.54979013e-08, -3.30011808e-15])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task._task.get_waypoints()[1]._waypoint.get_pose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> In: What should be the next key pose of the gripper to put the chocolate dessert mix in the basket? The current gripper pose is<g>ड广শữḳ洲</g>.\n",
      " Out:<cot><item_0>,<o>创요関</o>,<t>交ペམ</t>,<stage_0>,<a>认马共면ḳ收弘</a>,<q>,\n",
      "-2.799954216043461 False\n",
      "Action mask is not correct\n",
      "Action mask is not correct\n",
      "Action mask is not correct\n",
      "Action mask is not correct\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m----> 9\u001b[0m         action, q \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdescriptions\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m         action_rotation \u001b[38;5;241m=\u001b[39m R\u001b[38;5;241m.\u001b[39mfrom_euler(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mxyz\u001b[39m\u001b[38;5;124m'\u001b[39m, action[\u001b[38;5;241m3\u001b[39m:\u001b[38;5;241m6\u001b[39m])\n\u001b[1;32m     11\u001b[0m         action_quaternion \u001b[38;5;241m=\u001b[39m action_rotation\u001b[38;5;241m.\u001b[39mas_quat()\n",
      "Cell \u001b[0;32mIn[19], line 26\u001b[0m, in \u001b[0;36mAgent.act\u001b[0;34m(self, obs, instr, temperature, deterministic)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautocast(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbfloat16):\n\u001b[0;32m---> 26\u001b[0m         output_dict \u001b[38;5;241m=\u001b[39m \u001b[43mvla\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m28\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;66;03m# output_dict = vla.generate(**inputs, max_new_tokens = 50, output_scores = True, return_dict_in_generate=True, do_sample = True, temperature = 0.5)\u001b[39;00m\n\u001b[1;32m     35\u001b[0m     gripper_mask, item_mask, object_mask, target_mask, stage_mask, action_mask \u001b[38;5;241m=\u001b[39m action_tokenizer\u001b[38;5;241m.\u001b[39mget_mask(output_dict\u001b[38;5;241m.\u001b[39msequences)\n",
      "File \u001b[0;32m~/anaconda3/envs/VLA-RL/lib/python3.11/site-packages/peft/peft_model.py:647\u001b[0m, in \u001b[0;36mPeftModel.generate\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    645\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_peft_forward_hooks(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    646\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_peft_forward_args}\n\u001b[0;32m--> 647\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_base_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/VLA-RL/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/VLA-RL/lib/python3.11/site-packages/transformers/generation/utils.py:1576\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1559\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assisted_decoding(\n\u001b[1;32m   1560\u001b[0m         input_ids,\n\u001b[1;32m   1561\u001b[0m         candidate_generator\u001b[38;5;241m=\u001b[39mcandidate_generator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1572\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1573\u001b[0m     )\n\u001b[1;32m   1574\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mGREEDY_SEARCH:\n\u001b[1;32m   1575\u001b[0m     \u001b[38;5;66;03m# 11. run greedy search\u001b[39;00m\n\u001b[0;32m-> 1576\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_greedy_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1577\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1578\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1579\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1580\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1581\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1582\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_logits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_logits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1583\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1584\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1585\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1586\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1587\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1589\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mCONTRASTIVE_SEARCH:\n\u001b[1;32m   1590\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/anaconda3/envs/VLA-RL/lib/python3.11/site-packages/transformers/generation/utils.py:2494\u001b[0m, in \u001b[0;36mGenerationMixin._greedy_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, output_logits, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2491\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2493\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2494\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2495\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2496\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2497\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2498\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2499\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2501\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2502\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/VLA-RL/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/VLA-RL/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/VLA-RL/lib/python3.11/site-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/ecot-openvla-7b-bridge/modeling_prismatic.py:366\u001b[0m, in \u001b[0;36mPrismaticForConditionalGeneration.forward\u001b[0;34m(self, input_ids, attention_mask, pixel_values, labels, inputs_embeds, past_key_values, use_cache, output_attentions, output_hidden_states, output_projector_features, return_dict)\u001b[0m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m past_key_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected key `past_key_values` provided during language-only forward!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    365\u001b[0m \u001b[38;5;66;03m# Visual Feature Extraction\u001b[39;00m\n\u001b[0;32m--> 366\u001b[0m patch_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvision_backbone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    368\u001b[0m \u001b[38;5;66;03m# Projection Logic =>> Update Attention Mask\u001b[39;00m\n\u001b[1;32m    369\u001b[0m projected_patch_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprojector(patch_features)\n",
      "File \u001b[0;32m~/anaconda3/envs/VLA-RL/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/VLA-RL/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/VLA-RL/lib/python3.11/site-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/ecot-openvla-7b-bridge/modeling_prismatic.py:117\u001b[0m, in \u001b[0;36mPrismaticVisionBackbone.forward\u001b[0;34m(self, pixel_values)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Run image (`pixel_values`) through featurizer; if channel-stacked, then dispatch and sequence stack.\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_fused_vision_backbone:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeaturizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;66;03m# Split `pixel_values :: [bsz, 2 * 3, resolution, resolution]` =>> featurize =>> channel stack\u001b[39;00m\n\u001b[1;32m    120\u001b[0m img, img_fused \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msplit(pixel_values, [\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/VLA-RL/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/VLA-RL/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/VLA-RL/lib/python3.11/site-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/ecot-openvla-7b-bridge/modeling_prismatic.py:43\u001b[0m, in \u001b[0;36munpack_tuple.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m---> 43\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m result\n",
      "File \u001b[0;32m~/anaconda3/envs/VLA-RL/lib/python3.11/site-packages/timm/models/vision_transformer.py:644\u001b[0m, in \u001b[0;36mVisionTransformer.get_intermediate_layers\u001b[0;34m(self, x, n, reshape, return_prefix_tokens, norm)\u001b[0m\n\u001b[1;32m    640\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\" Intermediate layer accessor (NOTE: This is a WIP experiment).\u001b[39;00m\n\u001b[1;32m    641\u001b[0m \u001b[38;5;124;03mInspired by DINO / DINOv2 interface\u001b[39;00m\n\u001b[1;32m    642\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    643\u001b[0m \u001b[38;5;66;03m# take last n blocks if n is an int, if in is a sequence, select by matching indices\u001b[39;00m\n\u001b[0;32m--> 644\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_intermediate_layers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    645\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m norm:\n\u001b[1;32m    646\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(out) \u001b[38;5;28;01mfor\u001b[39;00m out \u001b[38;5;129;01min\u001b[39;00m outputs]\n",
      "File \u001b[0;32m~/anaconda3/envs/VLA-RL/lib/python3.11/site-packages/timm/models/vision_transformer.py:626\u001b[0m, in \u001b[0;36mVisionTransformer._intermediate_layers\u001b[0;34m(self, x, n)\u001b[0m\n\u001b[1;32m    624\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_pre(x)\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks):\n\u001b[0;32m--> 626\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mblk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    627\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m take_indices:\n\u001b[1;32m    628\u001b[0m         outputs\u001b[38;5;241m.\u001b[39mappend(x)\n",
      "File \u001b[0;32m~/anaconda3/envs/VLA-RL/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/VLA-RL/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/VLA-RL/lib/python3.11/site-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/anaconda3/envs/VLA-RL/lib/python3.11/site-packages/timm/models/vision_transformer.py:157\u001b[0m, in \u001b[0;36mBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    156\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop_path1(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mls1(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(x))))\n\u001b[0;32m--> 157\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop_path2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mls2(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/anaconda3/envs/VLA-RL/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/VLA-RL/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/VLA-RL/lib/python3.11/site-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/anaconda3/envs/VLA-RL/lib/python3.11/site-packages/timm/layers/mlp.py:46\u001b[0m, in \u001b[0;36mMlp.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     44\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop1(x)\n\u001b[1;32m     45\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(x)\n\u001b[0;32m---> 46\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop2(x)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/anaconda3/envs/VLA-RL/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/VLA-RL/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/VLA-RL/lib/python3.11/site-packages/peft/tuners/lora/bnb.py:474\u001b[0m, in \u001b[0;36mLinear4bit.forward\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m    471\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(lora_A\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m    473\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_dora[active_adapter]:\n\u001b[0;32m--> 474\u001b[0m     output \u001b[38;5;241m=\u001b[39m lora_B(\u001b[43mlora_A\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;241m*\u001b[39m scaling\n\u001b[1;32m    475\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    476\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply_dora(x, lora_A, lora_B, scaling, active_adapter)\n",
      "File \u001b[0;32m~/anaconda3/envs/VLA-RL/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/VLA-RL/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/VLA-RL/lib/python3.11/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "task.sample_variation()\n",
    "descriptions, obs = task.reset()\n",
    "agent.vla.set_adapter(\"actor_critic\")\n",
    "agent.vla.active_adapter\n",
    "torch.cuda.empty_cache()\n",
    "# Image.fromarray(obs.front_rgb)\n",
    "while True:\n",
    "    try:\n",
    "        action, q = agent.act(obs, descriptions[0], temperature=1, deterministic=False)\n",
    "        action_rotation = R.from_euler('xyz', action[3:6])\n",
    "        action_quaternion = action_rotation.as_quat()\n",
    "        action = np.concatenate([action[0:3], action_quaternion, action[-1:]])\n",
    "        obs, reward, terminate = task.step(action)\n",
    "        print(reward, terminate)\n",
    "    except Exception as e:\n",
    "        # continue\n",
    "        print(e)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'reward' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mreward\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'reward' is not defined"
     ]
    }
   ],
   "source": [
    "reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> In: What should be the next key pose of the gripper to put the sugar box in the basket? The current gripper pose is<g>景ἱ߬ữ飛舞</g>.\n",
      " Out:<cot><item_4>,<o>认ˆ态</o>,<t>交ペམ</t>,<stage_0>,<a>认ˆ್면ḳ收弘</a>,<q>,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([ 0.0485    , -0.0525    ,  0.86981989, -2.71747765, -0.00785399,\n",
       "        -1.52367244,  1.        ]),\n",
       " tensor([2.7188], device='cuda:0'))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.act(obs, descriptions[0], temperature=10, deterministic=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VLA-RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
