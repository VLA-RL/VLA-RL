{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lawrence/anaconda3/envs/VLA-RL/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "from rlbench.action_modes.action_mode import MoveArmThenGripper\n",
    "from rlbench.action_modes.arm_action_modes import ArmActionMode, JointVelocity, JointPosition, EndEffectorPoseViaPlanning, EndEffectorPoseViaIK\n",
    "from rlbench.action_modes.gripper_action_modes import Discrete\n",
    "from rlbench.environment import Environment\n",
    "from rlbench.observation_config import ObservationConfig, CameraConfig\n",
    "from rlbench.tasks import ReachTarget, PickAndLift, StackBlocks, PushButton, StackBlocks, PickUpCup, PlaceHangerOnRack\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoModelForVision2Seq, AutoProcessor, BitsAndBytesConfig\n",
    "from PIL import Image\n",
    "import torch\n",
    "from transformers import BitsAndBytesConfig\n",
    "from pyquaternion import Quaternion\n",
    "from rlbench.backend.robot import Robot\n",
    "from scipy.spatial.transform import Rotation\n",
    "from rlbench.backend.scene import Scene\n",
    "from pathlib import Path\n",
    "import os, json\n",
    "\n",
    "from transformers import AutoModelForVision2Seq\n",
    "from peft import PeftModel\n",
    "import argparse\n",
    "import torch\n",
    "from vla.action_tokenizer import RLbenchActionTokenizer\n",
    "from vla.dataset import RLbenchDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_path = \"/media/lawrence/Work/checkpoints/openvla-7b\"\n",
    "adapter_path = \"adapter-tmp/openvla-7b+pick_up_cup+e3+b8+lr-2e-05+lora-r8+dropout-0.0+q-4bit\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = AutoProcessor.from_pretrained(base_model_path, trust_remote_code=True)\n",
    "action_tokenizer = RLbenchActionTokenizer(processor.tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:35<00:00, 11.84s/it]\n",
      "/home/lawrence/anaconda3/envs/VLA-RL/lib/python3.11/site-packages/peft/tuners/lora/bnb.py:325: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "quantization_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True, bnb_4bit_compute_dtype=torch.bfloat16, bnb_4bit_quant_type=\"nf4\", #llm_int8_skip_modules = ['projector'],\n",
    "        )\n",
    "base_model = AutoModelForVision2Seq.from_pretrained(\n",
    "        base_model_path,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        attn_implementation=\"sdpa\",\n",
    "        quantization_config=quantization_config,\n",
    "        low_cpu_mem_usage=True,\n",
    "        trust_remote_code=True,\n",
    "        device_map = \"auto\"\n",
    "    )\n",
    "vla = PeftModel.from_pretrained(base_model, adapter_path, offload_buffers=True)\n",
    "vla = vla.merge_and_unload()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vla.base_prompter import PurePromptBuilder\n",
    "vla_dataset = RLbenchDataset(\n",
    "    \"./datasets/pick_up_cup/data.pt\",\n",
    "    action_tokenizer,\n",
    "    processor.tokenizer,\n",
    "    image_transform=processor.image_processor.apply_transform,\n",
    "    prompt_builder_fn=PurePromptBuilder,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [1, 29871, 31697, 31698, 31798, 31898, 31948, 31997, 31999], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_action = torch.tensor([1,-0.5,0.5,-3.14,0,3.14,1])\n",
    "processor.tokenizer(action_tokenizer(test_action))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"In: What action should the robot take to <INSTRUCTION>?\\nOut:\"\n",
    "# prompt = prompt.replace(\"<INSTRUCTION>\", instr.lower())\n",
    "image = Image.fromarray(np.random.random([224,224]))\n",
    "inputs = processor(prompt, image).to(vla.device, dtype=torch.bfloat16)\n",
    "inputs['input_ids'] = torch.cat((inputs['input_ids'], torch.unsqueeze(torch.Tensor([29871]).long(), dim=0).to(inputs['input_ids'].device)), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_dict = vla.generate(**inputs, max_new_tokens = 7, output_scores = True, return_dict_in_generate=True, do_sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'▓ਿദ巴飛Ħ忠'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.tokenizer.decode(torch.tensor([31879, 31857, 31889, 31885, 31877, 31870, 31744]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "    def __init__(self, vla, processor, action_tokenizer):\n",
    "        self.vla = vla\n",
    "        self.processor = processor\n",
    "        self.action_tokenizer = action_tokenizer\n",
    "        \n",
    "    def act(self, obs, instr):\n",
    "        prompt = \"In: What action should the robot take to <INSTRUCTION>?\\nOut:\"\n",
    "        prompt = prompt.replace(\"<INSTRUCTION>\", instr.lower())\n",
    "        image = Image.fromarray(obs.front_rgb)\n",
    "        inputs = self.processor(prompt, image).to(self.vla.device, dtype=torch.bfloat16)\n",
    "        inputs['input_ids'] = torch.cat((inputs['input_ids'], torch.unsqueeze(torch.Tensor([29871]).long(), dim=0).to(inputs['input_ids'].device)), dim=1)        \n",
    "        action_dict = self.vla.generate(**inputs, max_new_tokens = 7, output_scores = True, return_dict_in_generate=True)\n",
    "        ation_score = torch.stack(action_dict['scores']).squeeze(1)[:,self.action_tokenizer.action_token_begin_idx:self.processor.tokenizer.vocab_size]\n",
    "        pred_action = self.action_tokenizer.decode_token_score_to_actions(ation_score, soft = True)\n",
    "        return pred_action\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(vla, processor, action_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera = CameraConfig(image_size=(224, 224), depth=False, point_cloud=False, mask=False)\n",
    "obs_config = ObservationConfig(left_shoulder_camera=camera, right_shoulder_camera=camera, front_camera=camera, overhead_camera=camera)\n",
    "\n",
    "env = Environment(\n",
    "    action_mode=MoveArmThenGripper(\n",
    "        arm_action_mode=EndEffectorPoseViaPlanning(absolute_mode=True, collision_checking=False), gripper_action_mode=Discrete()),\n",
    "    obs_config=obs_config,\n",
    "    headless=False)\n",
    "env.launch()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = env.get_task(PickUpCup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions, obs = task.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    1,   512, 29901,  1724,  3158,   881,   278, 19964,  2125,   304,\n",
       "           529,  1177, 10810, 29965,  9838, 29958, 29973,    13,  3744, 29901,\n",
       "         29871, 31912, 31858, 31847, 31899, 31833, 31859, 31761]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_dict['sequences']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.1250, -0.0350,  1.4250, -3.1102, -2.7332, -0.2827,  1.0000],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instr = descriptions[1]\n",
    "prompt = \"In: What action should the robot take to {<INSTRUCTION>}?\\nOut:\"\n",
    "prompt = prompt.replace(\"<INSTRUCTION>\", instr.lower())\n",
    "image = Image.fromarray(obs.front_rgb)\n",
    "\n",
    "inputs = processor(prompt, image).to(vla.device, dtype=torch.bfloat16)\n",
    "inputs['input_ids'] = torch.cat((inputs['input_ids'], torch.unsqueeze(torch.Tensor([29871]).long(), dim=0).to(inputs['input_ids'].device)), dim=1)        \n",
    "action_dict = vla.generate(**inputs, max_new_tokens = 7, output_scores = True, return_dict_in_generate=True)\n",
    "\n",
    "agent.act(obs, instr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0350,  0.1050,  1.4750,  2.2305, -0.8482, -2.4819,  1.0000],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.act(obs,descriptions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reset Episode\n",
      "grasp the red cup and lift it\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lawrence/VLA-RL/VLA-RL/vla/action_tokenizer.py:126: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x_pred = F.softmax(x_score) @ torch.tensor(self.x_bin_centers, dtype=torch.float32).to(device)\n",
      "/home/lawrence/VLA-RL/VLA-RL/vla/action_tokenizer.py:127: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  y_pred = F.softmax(y_score) @ torch.tensor(self.y_bin_centers, dtype=torch.float32).to(device)\n",
      "/home/lawrence/VLA-RL/VLA-RL/vla/action_tokenizer.py:128: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  z_pred = F.softmax(z_score) @ torch.tensor(self.z_bin_centers, dtype=torch.float32).to(device)\n",
      "/home/lawrence/VLA-RL/VLA-RL/vla/action_tokenizer.py:130: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  grip_pred = F.softmax(grip_score) @ torch.tensor([0,1], dtype=torch.float32).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.27572709  0.24583329  1.46532702 -0.20408997 -0.25716859 -0.85771209\n",
      "  0.39565334  0.99771273]\n",
      "0.0\n",
      "[ 0.20059341  0.05374869  1.33494139  0.23751288 -0.90787488 -0.24849725\n",
      "  0.23999988  0.715424  ]\n",
      "0.0\n",
      "[ 0.20427898  0.33727607  1.2896384  -0.72007487 -0.00627378 -0.58384206\n",
      "  0.37494169  0.99982733]\n",
      "0.0\n",
      "[ 0.22827733  0.29828966  1.36124277 -0.13839479 -0.44249046 -0.10972493\n",
      "  0.8792096   0.99659038]\n",
      "0.0\n",
      "[ 0.22979419  0.16954969  1.41112947 -0.23194927  0.4435576  -0.86251087\n",
      "  0.074372    0.92944044]\n",
      "0.0\n",
      "[ 0.18916911  0.2723802   1.37525153 -0.47196301 -0.3703273  -0.67230804\n",
      "  0.43371709  0.88991213]\n",
      "0.0\n",
      "[ 0.21629931  0.12004524  1.46467423  0.70475247  0.54476549  0.44603796\n",
      " -0.08720467  0.99576843]\n",
      "0.0\n",
      "[ 0.25534695  0.23317787  1.34824216  0.01521532  0.74741209 -0.10307417\n",
      "  0.65613975  0.99922061]\n",
      "0.0\n",
      "[ 0.19314648  0.23666552  1.42500782  0.28048186 -0.64344575  0.57425778\n",
      "  0.4213496   0.96138906]\n",
      "0.0\n",
      "[ 0.18919125  0.29094267  1.24881077 -0.86361804 -0.13638627 -0.39551703\n",
      " -0.28129869  0.76065069]\n",
      "0.0\n",
      "[ 0.21464063  0.09378454  1.27818573 -0.30251145 -0.25307119 -0.91848686\n",
      " -0.02869981  0.98473781]\n",
      "0.0\n",
      "[ 0.27068335  0.04814104  1.29918408 -0.74079511 -0.47391807 -0.20494132\n",
      "  0.42967816  0.86793381]\n",
      "0.0\n",
      "[ 0.24416523  0.16301644  1.26624739  0.10479916 -0.09676996 -0.98846661\n",
      "  0.05085738  0.99701905]\n",
      "0.0\n",
      "[ 0.24689159  0.41676629  1.31285548 -0.70685974  0.39017586 -0.53421528\n",
      " -0.25045187  0.94966936]\n",
      "0.0\n",
      "[ 0.23159222  0.03906785  1.30084801  0.07061788 -0.36922486  0.02546865\n",
      "  0.92630312  0.99939764]\n",
      "0.0\n",
      "[ 0.22322609  0.22786281  1.33561778 -0.62248238 -0.30077432 -0.22443876\n",
      "  0.68678799  0.19590156]\n",
      "0.0\n",
      "[ 0.25056076  0.14180167  1.29787052  0.3286937  -0.63209948  0.32626182\n",
      "  0.62125995  0.99477994]\n",
      "0.0\n",
      "[ 0.26281419  0.31551149  1.41257143 -0.44131479  0.71927768 -0.37717911\n",
      "  0.38159768  0.99944717]\n",
      "0.0\n",
      "[ 0.23137064  0.25162351  1.476951   -0.55685125 -0.05202212 -0.42355326\n",
      " -0.71261001  0.99971753]\n",
      "0.0\n",
      "[ 0.21462567  0.19030724  1.29600871 -0.40674315 -0.11075847 -0.66234547\n",
      "  0.61934728  0.97156399]\n",
      "0.0\n",
      "[ 0.20512429  0.22749388  1.3152802  -0.03853939  0.56428144 -0.47048812\n",
      "  0.67730503  0.99513251]\n",
      "0.0\n",
      "[ 0.17215779  0.32081461  1.33143473 -0.21020928 -0.81586162  0.28471437\n",
      "  0.45729596  0.94815457]\n",
      "A path could not be found. Most likely due to the target being inaccessible or a collison was detected.\n",
      "[ 0.17215779  0.32081461  1.33143473 -0.21020928 -0.81586162  0.28471437\n",
      "  0.45729596  0.94815457]\n",
      "A path could not be found. Most likely due to the target being inaccessible or a collison was detected.\n",
      "[ 0.17215779  0.32081461  1.33143473 -0.21020928 -0.81586162  0.28471437\n",
      "  0.45729596  0.94815457]\n",
      "A path could not be found. Most likely due to the target being inaccessible or a collison was detected.\n",
      "[ 0.17215779  0.32081461  1.33143473 -0.21020928 -0.81586162  0.28471437\n",
      "  0.45729596  0.94815457]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m     action \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate([action[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m3\u001b[39m], action_quaternion, action[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:]])\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28mprint\u001b[39m(action)\n\u001b[0;32m---> 17\u001b[0m     obs, reward, terminate \u001b[38;5;241m=\u001b[39m \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28mprint\u001b[39m(reward)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/envs/VLA-RL/lib/python3.11/site-packages/rlbench/task_environment.py:100\u001b[0m, in \u001b[0;36mTaskEnvironment.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset_called:\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     99\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCall \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreset\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m before calling \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m on a task.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 100\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_action_mode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_scene\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m success, terminate \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task\u001b[38;5;241m.\u001b[39msuccess()\n\u001b[1;32m    102\u001b[0m reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(success)\n",
      "File \u001b[0;32m~/anaconda3/envs/VLA-RL/lib/python3.11/site-packages/rlbench/action_modes/action_mode.py:41\u001b[0m, in \u001b[0;36mMoveArmThenGripper.action\u001b[0;34m(self, scene, action)\u001b[0m\n\u001b[1;32m     39\u001b[0m arm_action \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(action[:arm_act_size])\n\u001b[1;32m     40\u001b[0m ee_action \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(action[arm_act_size:])\n\u001b[0;32m---> 41\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marm_action_mode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscene\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marm_action\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgripper_action_mode\u001b[38;5;241m.\u001b[39maction(scene, ee_action)\n",
      "File \u001b[0;32m~/anaconda3/envs/VLA-RL/lib/python3.11/site-packages/rlbench/action_modes/arm_action_modes.py:256\u001b[0m, in \u001b[0;36mEndEffectorPoseViaPlanning.action\u001b[0;34m(self, scene, action)\u001b[0m\n\u001b[1;32m    253\u001b[0m         [s\u001b[38;5;241m.\u001b[39mset_collidable(\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m colliding_shapes]\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 256\u001b[0m     path \u001b[38;5;241m=\u001b[39m \u001b[43mscene\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrobot\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_path\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m        \u001b[49m\u001b[43maction\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquaternion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_collisions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_collision_checking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrelative_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrelative_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_configs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_time_ms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrials_per_goal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[43m        \u001b[49m\u001b[43malgorithm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mAlgos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mRRTConnect\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    267\u001b[0m     [s\u001b[38;5;241m.\u001b[39mset_collidable(\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m colliding_shapes]\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConfigurationPathError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/envs/VLA-RL/lib/python3.11/site-packages/pyrep/robots/arms/arm.py:453\u001b[0m, in \u001b[0;36mArm.get_path\u001b[0;34m(self, position, euler, quaternion, ignore_collisions, trials, max_configs, distance_threshold, max_time_ms, trials_per_goal, algorithm, relative_to)\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m  \u001b[38;5;66;03m# Allowed. Try again, but with non-linear.\u001b[39;00m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;66;03m# This time if an exception is thrown, we dont want to catch it.\u001b[39;00m\n\u001b[0;32m--> 453\u001b[0m p \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_nonlinear_path\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meuler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquaternion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_collisions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrials\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_configs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdistance_threshold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_time_ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrials_per_goal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malgorithm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrelative_to\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m p\n",
      "File \u001b[0;32m~/anaconda3/envs/VLA-RL/lib/python3.11/site-packages/pyrep/robots/arms/arm.py:388\u001b[0m, in \u001b[0;36mArm.get_nonlinear_path\u001b[0;34m(self, position, euler, quaternion, ignore_collisions, trials, max_configs, distance_threshold, max_time_ms, trials_per_goal, algorithm, relative_to)\u001b[0m\n\u001b[1;32m    385\u001b[0m handles \u001b[38;5;241m=\u001b[39m [j\u001b[38;5;241m.\u001b[39mget_handle() \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjoints]\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 388\u001b[0m     configs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msolve_ik_via_sampling\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meuler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquaternion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_collisions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_configs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdistance_threshold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_time_ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrelative_to\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConfigurationError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ConfigurationPathError(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCould not create path.\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/VLA-RL/lib/python3.11/site-packages/pyrep/robots/arms/arm.py:146\u001b[0m, in \u001b[0;36mArm.solve_ik_via_sampling\u001b[0;34m(self, position, euler, quaternion, ignore_collisions, trials, max_configs, distance_threshold, max_time_ms, relative_to)\u001b[0m\n\u001b[1;32m    144\u001b[0m valid_joint_positions \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(trials):\n\u001b[0;32m--> 146\u001b[0m     config \u001b[38;5;241m=\u001b[39m \u001b[43msim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimGetConfigForTipPose\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ik_group\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhandles\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdistance_threshold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmax_time_ms\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollision_pairs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjoint_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlow_limits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_limits\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(config) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    150\u001b[0m         valid_joint_positions\u001b[38;5;241m.\u001b[39mappend(config)\n",
      "File \u001b[0;32m~/anaconda3/envs/VLA-RL/lib/python3.11/site-packages/pyrep/backend/sim.py:1458\u001b[0m, in \u001b[0;36msimGetConfigForTipPose\u001b[0;34m(ikGroupHandle, jointHandles, thresholdDist, maxTimeInMs, metric, collisionPairs, jointOptions, lowLimits, ranges)\u001b[0m\n\u001b[1;32m   1456\u001b[0m metric \u001b[38;5;241m=\u001b[39m ffi\u001b[38;5;241m.\u001b[39mNULL \u001b[38;5;28;01mif\u001b[39;00m metric \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m metric\n\u001b[1;32m   1457\u001b[0m jointOptions \u001b[38;5;241m=\u001b[39m ffi\u001b[38;5;241m.\u001b[39mNULL \u001b[38;5;28;01mif\u001b[39;00m jointOptions \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m jointOptions\n\u001b[0;32m-> 1458\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimGetConfigForTipPose\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1459\u001b[0m \u001b[43m    \u001b[49m\u001b[43mikGroupHandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjointCnt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjointHandles\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthresholdDist\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1460\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaxTimeInMs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretConfigm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollisionPairCnt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollisionPairs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1461\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjointOptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlowLimits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mranges\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreserved\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1462\u001b[0m _check_return(ret)\n\u001b[1;32m   1463\u001b[0m _check_null_return(retConfigm)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "task = env.get_task(PickUpCup)\n",
    "training_steps = 1000\n",
    "episode_length = 100\n",
    "obs = None\n",
    "for i in range(training_steps):\n",
    "    if i % episode_length == 0:\n",
    "        print('Reset Episode')\n",
    "        descriptions, obs = task.reset()\n",
    "        print(descriptions[1])\n",
    "    try:\n",
    "        action = agent.act(obs,descriptions[0]).cpu().numpy()\n",
    "        action_rotation = Rotation.from_euler('xyz', action[3:6])\n",
    "        action_quaternion = action_rotation.as_quat()\n",
    "        # print(delta_quaternion)  # returns (qx, qy, qz, qw)\n",
    "        action = np.concatenate([action[0:3], action_quaternion, action[-1:]])\n",
    "        print(action)\n",
    "        obs, reward, terminate = task.step(action)\n",
    "        print(reward)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CoppeliaSim:loadinfo]   done.\n"
     ]
    }
   ],
   "source": [
    "env.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.02001152,  0.02458013, -0.01649398,  0.00402673, -0.01346854,\n",
       "        0.02819962,  0.99607843])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VLA-RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
