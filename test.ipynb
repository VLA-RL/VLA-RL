{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lawrence/anaconda3/envs/VLA-RL/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "from rlbench.action_modes.action_mode import MoveArmThenGripper\n",
    "from rlbench.action_modes.arm_action_modes import ArmActionMode, JointVelocity, JointPosition, EndEffectorPoseViaPlanning, EndEffectorPoseViaIK\n",
    "from rlbench.action_modes.gripper_action_modes import Discrete\n",
    "from rlbench.environment import Environment\n",
    "from rlbench.observation_config import ObservationConfig, CameraConfig\n",
    "from rlbench.tasks import ReachTarget, PickAndLift, StackBlocks, PushButton, StackBlocks, PickUpCup, PlaceHangerOnRack, PickDescribedObject\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoModelForVision2Seq, AutoProcessor, BitsAndBytesConfig\n",
    "from PIL import Image\n",
    "import torch\n",
    "from transformers import BitsAndBytesConfig\n",
    "from pyquaternion import Quaternion\n",
    "from rlbench.backend.robot import Robot\n",
    "from scipy.spatial.transform import Rotation\n",
    "from rlbench.backend.scene import Scene\n",
    "from pathlib import Path\n",
    "import os, json\n",
    "\n",
    "from transformers import AutoModelForVision2Seq\n",
    "from peft import PeftModel\n",
    "import argparse\n",
    "import torch\n",
    "from vla.action_tokenizer import RLbenchPoseTokenizer\n",
    "from vla.dataset import RLbenchDataset\n",
    "from peft import LoraConfig, PeftModel, get_peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_path = \"/media/lawrence/Work/checkpoints/ecot-openvla-7b-bridge\"\n",
    "adapter_path = \"adapter-tmp/Weighted_loss_4+pick_described_object+e1+b8+lr-1e-05+lora-r16+dropout-0.0+q-4bit\"\n",
    "save_path = \"/media/lawrence/Work/checkpoints/vla-rl-ecot\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_statistics: tuple = (np.array([-0.20173775, -0.36754665,  0.81396234, -3.14153998, -0.38798628, -3.14158631,  0. ]), np.array([0.41802976, 0.45118147, 1.47966564, 3.14159215, 0.30391057, 3.14157801, 1.])) # Min-Max normalization statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "processor = AutoProcessor.from_pretrained(base_model_path, trust_remote_code=True)\n",
    "action_tokenizer = RLbenchPoseTokenizer(processor.tokenizer,dataset_statistics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_model = AutoModelForVision2Seq.from_pretrained(\n",
    "#         base_model_path,\n",
    "#         torch_dtype=torch.bfloat16,\n",
    "#         attn_implementation=\"sdpa\",\n",
    "#         # quantization_config=quantization_config,\n",
    "#         low_cpu_mem_usage=True,\n",
    "#         trust_remote_code=True,\n",
    "#         device_map = \"cpu\"\n",
    "#     )\n",
    "# vla = PeftModel.from_pretrained(base_model, adapter_path)\n",
    "# vla = vla.merge_and_unload()\n",
    "# vla.save_pretrained(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:30<00:00, 10.22s/it]\n"
     ]
    }
   ],
   "source": [
    "quantization_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True, bnb_4bit_compute_dtype=torch.bfloat16, bnb_4bit_quant_type=\"nf4\", #llm_int8_skip_modules = ['projector'],\n",
    "        )\n",
    "vla = AutoModelForVision2Seq.from_pretrained(\n",
    "        save_path,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        attn_implementation=\"sdpa\",\n",
    "        quantization_config=quantization_config,\n",
    "        low_cpu_mem_usage=True,\n",
    "        trust_remote_code=True,\n",
    "        device_map = \"cuda\"\n",
    "    )\n",
    "# vla.load_adapter(adapter_path)\n",
    "# vla.enable_adapters()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "    def __init__(self, vla, processor, action_tokenizer):\n",
    "        self.vla = vla\n",
    "        self.processor = processor\n",
    "        self.action_tokenizer = action_tokenizer\n",
    "        \n",
    "    def act(self, obs, instr):\n",
    "        prompt = \"In: What action should the robot take to <INSTRUCTION>?\\nOut:\"\n",
    "        prompt = prompt.replace(\"<INSTRUCTION>\", instr.lower())\n",
    "        image = Image.fromarray(obs.front_rgb)\n",
    "        inputs = self.processor(prompt, image).to(self.vla.device, dtype=torch.bfloat16)\n",
    "        # inputs['input_ids'] = torch.cat((inputs['input_ids'], torch.unsqueeze(torch.Tensor([29871]).long(), dim=0).to(inputs['input_ids'].device)), dim=1)        \n",
    "        action_dict = self.vla.generate(**inputs, max_new_tokens = 1024, output_scores = True, return_dict_in_generate=True)\n",
    "        ation_score = torch.stack(action_dict['scores']).squeeze(1)[:,self.action_tokenizer.action_token_begin_idx:self.processor.tokenizer.vocab_size]\n",
    "        pred_action = self.action_tokenizer.decode_token_score_to_actions(ation_score, soft = True)\n",
    "        return pred_action\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(vla, processor, action_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera = CameraConfig(image_size=(224, 224), depth=False, point_cloud=False, mask=False)\n",
    "obs_config = ObservationConfig(left_shoulder_camera=camera, right_shoulder_camera=camera, front_camera=camera, overhead_camera=camera)\n",
    "\n",
    "env = Environment(\n",
    "    action_mode=MoveArmThenGripper(\n",
    "        arm_action_mode=EndEffectorPoseViaPlanning(absolute_mode=True, collision_checking=False), gripper_action_mode=Discrete()),\n",
    "    obs_config=obs_config,\n",
    "    headless=True)\n",
    "env.launch()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = env.get_task(PickDescribedObject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions, obs = task.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = (\n",
    "    \"A chat between a curious user and an artificial intelligence assistant. \"\n",
    "    \"The assistant gives helpful, detailed, and polite answers to the user's questions.\"\n",
    ")\n",
    "\n",
    "def get_openvla_prompt(instruction: str) -> str:\n",
    "    return f\"{SYSTEM_PROMPT} USER: What action should the robot take to {instruction.lower()}? ASSISTANT: \"\n",
    "\n",
    "INSTRUCTION = \"place the watermelon on the towel\"\n",
    "\n",
    "def get_instruction_prompt(instruction:str) -> str:\n",
    "    return f\"In: \"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pick up the chocolate jello and place in the basket'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "instr = descriptions[1]\n",
    "prompt = get_openvla_prompt(instr)\n",
    "\n",
    "image = Image.fromarray(obs.front_rgb)\n",
    "\n",
    "inputs = processor(prompt, image).to(vla.device, dtype=torch.bfloat16)\n",
    "# inputs['input_ids'] = torch.cat((inputs['input_ids'], torch.unsqueeze(torch.Tensor([29871]).long(), dim=0).to(inputs['input_ids'].device)), dim=1)        \n",
    "action_dict = vla.generate(**inputs, max_new_tokens = 1024, output_scores = True, return_dict_in_generate=True)\n",
    "\n",
    "# agent.act(obs, instr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    1,   319, 13563,  1546,   263, 12758,  1404,   322,   385, 23116,\n",
       "        21082, 20255, 29889,   450, 20255,  4076,  8444, 29892, 13173, 29892,\n",
       "          322,  1248,   568,  6089,   304,   278,  1404, 29915, 29879,  5155,\n",
       "        29889,  3148,  1001, 29901,  1724,  3158,   881,   278, 19964,  2125,\n",
       "          304,  5839,   701,   278,   521,   542, 23167,   432,  3156,   322,\n",
       "         2058,   297,   278, 25972, 29973,   319,  1799,  9047, 13566, 29901,\n",
       "          521,   542, 23167,   432,  3156, 21521,  1660, 29901, 32005, 16999,\n",
       "        12064,  5195, 29909,  3094,  4214, 29901,   521,   542, 23167,   432,\n",
       "         3156,   338,   297,   278,   330,   374,  2496, 29892,   541,   451,\n",
       "          297,   278, 25972, 29892,   577,  4337,   304,   278, 25972, 16999,\n",
       "        12064, 29901,  4337,  1492,   402,  3960, 29925, 13171,   349,  3267,\n",
       "        22122, 29901,   518, 29896, 29896, 29946, 29892, 29871, 29929, 29946,\n",
       "        29962,   478,  3235,  8979,  1307,   438, 29933, 17637, 29903, 29901,\n",
       "          322,   263, 18345, 11915,  4933,   518, 29929, 29929, 29892, 29871,\n",
       "        29896, 29892, 29871, 29896, 29945, 29945, 29892, 29871, 29896, 29946,\n",
       "        29945,  1402,   263, 13328, 28704,   518, 29945, 29906, 29892, 29871,\n",
       "        29896, 29941, 29929, 29892, 29871, 29955, 29900, 29892, 29871, 29896,\n",
       "        29955, 29896,  1402,   263,  4796, 27278,  1591,   518, 29896, 29945,\n",
       "        29892, 29871, 29896, 29906, 29941, 29892, 29871, 29906, 29941, 29929,\n",
       "        29892, 29871, 29906, 29945, 29900,  1402,   263, 19964,   518, 29929,\n",
       "        29941, 29892, 29871, 29900, 29892, 29871, 29896, 29945, 29929, 29892,\n",
       "        29871, 29896, 29946, 29955, 29962,   319,  9838, 29901, 29871, 31860,\n",
       "        31919, 31888, 31938, 31932, 31920, 31872,     2], device='cuda:0')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_dict.sequences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<s> A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: What action should the robot take to pick up the chocolate jello and place in the basket? ASSISTANT: chocolate jello POSE:<object> MOVE REASONING: chocolate jello is in the gripper, but not in the basket, so move to the basket MOVE: move right GRIPPER POSITION: [114, 94] VISIBLE OBJECTS: and a grey metal machine [99, 1, 155, 145], a yellow cube [52, 139, 70, 171], a white wooden table [15, 123, 239, 250], a robot [93, 0, 159, 147] ACTION: ھპ项ほ唐ċŸ</s>\""
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "processor.tokenizer.decode(action_dict.sequences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0, device='cuda:0')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(action_dict.sequences == 32002).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pick up the chocolate jello and place in the basket'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'RLbenchPoseTokenizer' object has no attribute 'decode_token_score_to_actions'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdescriptions\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[19], line 15\u001b[0m, in \u001b[0;36mAgent.act\u001b[0;34m(self, obs, instr)\u001b[0m\n\u001b[1;32m     13\u001b[0m action_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvla\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, max_new_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1024\u001b[39m, output_scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, return_dict_in_generate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     14\u001b[0m ation_score \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(action_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscores\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)[:,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_tokenizer\u001b[38;5;241m.\u001b[39maction_token_begin_idx:\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessor\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mvocab_size]\n\u001b[0;32m---> 15\u001b[0m pred_action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode_token_score_to_actions\u001b[49m(ation_score, soft \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pred_action\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'RLbenchPoseTokenizer' object has no attribute 'decode_token_score_to_actions'"
     ]
    }
   ],
   "source": [
    "agent.act(obs,descriptions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reset Episode\n",
      "grasp the red cup and lift it\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lawrence/VLA-RL/VLA-RL/vla/action_tokenizer.py:126: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  x_pred = F.softmax(x_score) @ torch.tensor(self.x_bin_centers, dtype=torch.float32).to(device)\n",
      "/home/lawrence/VLA-RL/VLA-RL/vla/action_tokenizer.py:127: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  y_pred = F.softmax(y_score) @ torch.tensor(self.y_bin_centers, dtype=torch.float32).to(device)\n",
      "/home/lawrence/VLA-RL/VLA-RL/vla/action_tokenizer.py:128: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  z_pred = F.softmax(z_score) @ torch.tensor(self.z_bin_centers, dtype=torch.float32).to(device)\n",
      "/home/lawrence/VLA-RL/VLA-RL/vla/action_tokenizer.py:130: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  grip_pred = F.softmax(grip_score) @ torch.tensor([0,1], dtype=torch.float32).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.27572709  0.24583329  1.46532702 -0.20408997 -0.25716859 -0.85771209\n",
      "  0.39565334  0.99771273]\n",
      "0.0\n",
      "[ 0.20059341  0.05374869  1.33494139  0.23751288 -0.90787488 -0.24849725\n",
      "  0.23999988  0.715424  ]\n",
      "0.0\n",
      "[ 0.20427898  0.33727607  1.2896384  -0.72007487 -0.00627378 -0.58384206\n",
      "  0.37494169  0.99982733]\n",
      "0.0\n",
      "[ 0.22827733  0.29828966  1.36124277 -0.13839479 -0.44249046 -0.10972493\n",
      "  0.8792096   0.99659038]\n",
      "0.0\n",
      "[ 0.22979419  0.16954969  1.41112947 -0.23194927  0.4435576  -0.86251087\n",
      "  0.074372    0.92944044]\n",
      "0.0\n",
      "[ 0.18916911  0.2723802   1.37525153 -0.47196301 -0.3703273  -0.67230804\n",
      "  0.43371709  0.88991213]\n",
      "0.0\n",
      "[ 0.21629931  0.12004524  1.46467423  0.70475247  0.54476549  0.44603796\n",
      " -0.08720467  0.99576843]\n",
      "0.0\n",
      "[ 0.25534695  0.23317787  1.34824216  0.01521532  0.74741209 -0.10307417\n",
      "  0.65613975  0.99922061]\n",
      "0.0\n",
      "[ 0.19314648  0.23666552  1.42500782  0.28048186 -0.64344575  0.57425778\n",
      "  0.4213496   0.96138906]\n",
      "0.0\n",
      "[ 0.18919125  0.29094267  1.24881077 -0.86361804 -0.13638627 -0.39551703\n",
      " -0.28129869  0.76065069]\n",
      "0.0\n",
      "[ 0.21464063  0.09378454  1.27818573 -0.30251145 -0.25307119 -0.91848686\n",
      " -0.02869981  0.98473781]\n",
      "0.0\n",
      "[ 0.27068335  0.04814104  1.29918408 -0.74079511 -0.47391807 -0.20494132\n",
      "  0.42967816  0.86793381]\n",
      "0.0\n",
      "[ 0.24416523  0.16301644  1.26624739  0.10479916 -0.09676996 -0.98846661\n",
      "  0.05085738  0.99701905]\n",
      "0.0\n",
      "[ 0.24689159  0.41676629  1.31285548 -0.70685974  0.39017586 -0.53421528\n",
      " -0.25045187  0.94966936]\n",
      "0.0\n",
      "[ 0.23159222  0.03906785  1.30084801  0.07061788 -0.36922486  0.02546865\n",
      "  0.92630312  0.99939764]\n",
      "0.0\n",
      "[ 0.22322609  0.22786281  1.33561778 -0.62248238 -0.30077432 -0.22443876\n",
      "  0.68678799  0.19590156]\n",
      "0.0\n",
      "[ 0.25056076  0.14180167  1.29787052  0.3286937  -0.63209948  0.32626182\n",
      "  0.62125995  0.99477994]\n",
      "0.0\n",
      "[ 0.26281419  0.31551149  1.41257143 -0.44131479  0.71927768 -0.37717911\n",
      "  0.38159768  0.99944717]\n",
      "0.0\n",
      "[ 0.23137064  0.25162351  1.476951   -0.55685125 -0.05202212 -0.42355326\n",
      " -0.71261001  0.99971753]\n",
      "0.0\n",
      "[ 0.21462567  0.19030724  1.29600871 -0.40674315 -0.11075847 -0.66234547\n",
      "  0.61934728  0.97156399]\n",
      "0.0\n",
      "[ 0.20512429  0.22749388  1.3152802  -0.03853939  0.56428144 -0.47048812\n",
      "  0.67730503  0.99513251]\n",
      "0.0\n",
      "[ 0.17215779  0.32081461  1.33143473 -0.21020928 -0.81586162  0.28471437\n",
      "  0.45729596  0.94815457]\n",
      "A path could not be found. Most likely due to the target being inaccessible or a collison was detected.\n",
      "[ 0.17215779  0.32081461  1.33143473 -0.21020928 -0.81586162  0.28471437\n",
      "  0.45729596  0.94815457]\n",
      "A path could not be found. Most likely due to the target being inaccessible or a collison was detected.\n",
      "[ 0.17215779  0.32081461  1.33143473 -0.21020928 -0.81586162  0.28471437\n",
      "  0.45729596  0.94815457]\n",
      "A path could not be found. Most likely due to the target being inaccessible or a collison was detected.\n",
      "[ 0.17215779  0.32081461  1.33143473 -0.21020928 -0.81586162  0.28471437\n",
      "  0.45729596  0.94815457]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m     action \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate([action[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m3\u001b[39m], action_quaternion, action[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:]])\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28mprint\u001b[39m(action)\n\u001b[0;32m---> 17\u001b[0m     obs, reward, terminate \u001b[38;5;241m=\u001b[39m \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28mprint\u001b[39m(reward)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/envs/VLA-RL/lib/python3.11/site-packages/rlbench/task_environment.py:100\u001b[0m, in \u001b[0;36mTaskEnvironment.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset_called:\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     99\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCall \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreset\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m before calling \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m on a task.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 100\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_action_mode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_scene\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m success, terminate \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task\u001b[38;5;241m.\u001b[39msuccess()\n\u001b[1;32m    102\u001b[0m reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(success)\n",
      "File \u001b[0;32m~/anaconda3/envs/VLA-RL/lib/python3.11/site-packages/rlbench/action_modes/action_mode.py:41\u001b[0m, in \u001b[0;36mMoveArmThenGripper.action\u001b[0;34m(self, scene, action)\u001b[0m\n\u001b[1;32m     39\u001b[0m arm_action \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(action[:arm_act_size])\n\u001b[1;32m     40\u001b[0m ee_action \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(action[arm_act_size:])\n\u001b[0;32m---> 41\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marm_action_mode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscene\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marm_action\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgripper_action_mode\u001b[38;5;241m.\u001b[39maction(scene, ee_action)\n",
      "File \u001b[0;32m~/anaconda3/envs/VLA-RL/lib/python3.11/site-packages/rlbench/action_modes/arm_action_modes.py:256\u001b[0m, in \u001b[0;36mEndEffectorPoseViaPlanning.action\u001b[0;34m(self, scene, action)\u001b[0m\n\u001b[1;32m    253\u001b[0m         [s\u001b[38;5;241m.\u001b[39mset_collidable(\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m colliding_shapes]\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 256\u001b[0m     path \u001b[38;5;241m=\u001b[39m \u001b[43mscene\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrobot\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_path\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m        \u001b[49m\u001b[43maction\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquaternion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_collisions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_collision_checking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrelative_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrelative_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_configs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_time_ms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrials_per_goal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[43m        \u001b[49m\u001b[43malgorithm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mAlgos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mRRTConnect\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    267\u001b[0m     [s\u001b[38;5;241m.\u001b[39mset_collidable(\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m colliding_shapes]\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConfigurationPathError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/envs/VLA-RL/lib/python3.11/site-packages/pyrep/robots/arms/arm.py:453\u001b[0m, in \u001b[0;36mArm.get_path\u001b[0;34m(self, position, euler, quaternion, ignore_collisions, trials, max_configs, distance_threshold, max_time_ms, trials_per_goal, algorithm, relative_to)\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m  \u001b[38;5;66;03m# Allowed. Try again, but with non-linear.\u001b[39;00m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;66;03m# This time if an exception is thrown, we dont want to catch it.\u001b[39;00m\n\u001b[0;32m--> 453\u001b[0m p \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_nonlinear_path\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meuler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquaternion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_collisions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrials\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_configs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdistance_threshold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_time_ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrials_per_goal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malgorithm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrelative_to\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m p\n",
      "File \u001b[0;32m~/anaconda3/envs/VLA-RL/lib/python3.11/site-packages/pyrep/robots/arms/arm.py:388\u001b[0m, in \u001b[0;36mArm.get_nonlinear_path\u001b[0;34m(self, position, euler, quaternion, ignore_collisions, trials, max_configs, distance_threshold, max_time_ms, trials_per_goal, algorithm, relative_to)\u001b[0m\n\u001b[1;32m    385\u001b[0m handles \u001b[38;5;241m=\u001b[39m [j\u001b[38;5;241m.\u001b[39mget_handle() \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjoints]\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 388\u001b[0m     configs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msolve_ik_via_sampling\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meuler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquaternion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_collisions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_configs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdistance_threshold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_time_ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrelative_to\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConfigurationError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ConfigurationPathError(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCould not create path.\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/VLA-RL/lib/python3.11/site-packages/pyrep/robots/arms/arm.py:146\u001b[0m, in \u001b[0;36mArm.solve_ik_via_sampling\u001b[0;34m(self, position, euler, quaternion, ignore_collisions, trials, max_configs, distance_threshold, max_time_ms, relative_to)\u001b[0m\n\u001b[1;32m    144\u001b[0m valid_joint_positions \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(trials):\n\u001b[0;32m--> 146\u001b[0m     config \u001b[38;5;241m=\u001b[39m \u001b[43msim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimGetConfigForTipPose\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ik_group\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhandles\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdistance_threshold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmax_time_ms\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollision_pairs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjoint_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlow_limits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_limits\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(config) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    150\u001b[0m         valid_joint_positions\u001b[38;5;241m.\u001b[39mappend(config)\n",
      "File \u001b[0;32m~/anaconda3/envs/VLA-RL/lib/python3.11/site-packages/pyrep/backend/sim.py:1458\u001b[0m, in \u001b[0;36msimGetConfigForTipPose\u001b[0;34m(ikGroupHandle, jointHandles, thresholdDist, maxTimeInMs, metric, collisionPairs, jointOptions, lowLimits, ranges)\u001b[0m\n\u001b[1;32m   1456\u001b[0m metric \u001b[38;5;241m=\u001b[39m ffi\u001b[38;5;241m.\u001b[39mNULL \u001b[38;5;28;01mif\u001b[39;00m metric \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m metric\n\u001b[1;32m   1457\u001b[0m jointOptions \u001b[38;5;241m=\u001b[39m ffi\u001b[38;5;241m.\u001b[39mNULL \u001b[38;5;28;01mif\u001b[39;00m jointOptions \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m jointOptions\n\u001b[0;32m-> 1458\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimGetConfigForTipPose\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1459\u001b[0m \u001b[43m    \u001b[49m\u001b[43mikGroupHandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjointCnt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjointHandles\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthresholdDist\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1460\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaxTimeInMs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretConfigm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollisionPairCnt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollisionPairs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1461\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjointOptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlowLimits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mranges\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreserved\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1462\u001b[0m _check_return(ret)\n\u001b[1;32m   1463\u001b[0m _check_null_return(retConfigm)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "task = env.get_task(PickUpCup)\n",
    "training_steps = 1000\n",
    "episode_length = 100\n",
    "obs = None\n",
    "for i in range(training_steps):\n",
    "    if i % episode_length == 0:\n",
    "        print('Reset Episode')\n",
    "        descriptions, obs = task.reset()\n",
    "        print(descriptions[1])\n",
    "    try:\n",
    "        action = agent.act(obs,descriptions[0]).cpu().numpy()\n",
    "        action_rotation = Rotation.from_euler('xyz', action[3:6])\n",
    "        action_quaternion = action_rotation.as_quat()\n",
    "        # print(delta_quaternion)  # returns (qx, qy, qz, qw)\n",
    "        action = np.concatenate([action[0:3], action_quaternion, action[-1:]])\n",
    "        print(action)\n",
    "        obs, reward, terminate = task.step(action)\n",
    "        print(reward)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CoppeliaSim:loadinfo]   done.\n"
     ]
    }
   ],
   "source": [
    "env.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.02001152,  0.02458013, -0.01649398,  0.00402673, -0.01346854,\n",
       "        0.02819962,  0.99607843])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VLA-RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
