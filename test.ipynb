{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-02 22:24:51.400979: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-08-02 22:24:51.401082: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-08-02 22:24:51.462933: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-08-02 22:24:51.579911: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-02 22:24:52.945930: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "sys.path.append('.')\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "import draccus\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import tqdm\n",
    "import wandb\n",
    "from accelerate import PartialState\n",
    "from peft import LoraConfig, PeftModel, get_peft_model, prepare_model_for_kbit_training\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModelForVision2Seq, AutoProcessor, BitsAndBytesConfig\n",
    "from transformers.modeling_outputs import CausalLMOutputWithPast\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "from vla.base_prompter import PurePromptBuilder\n",
    "from vla.utils import PaddedCollatorForPosePrediction, runningLoss\n",
    "from vla.action_tokenizer import RLbenchPoseTokenizer\n",
    "from vla.dataset import RLbenchCotDataset\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "from rlbench.action_modes.action_mode import MoveArmThenGripper\n",
    "from rlbench.action_modes.arm_action_modes import ArmActionMode, JointVelocity, JointPosition, EndEffectorPoseViaPlanning, EndEffectorPoseViaIK\n",
    "\n",
    "\n",
    "from rlbench.action_modes.gripper_action_modes import Discrete\n",
    "from rlbench.environment import Environment\n",
    "from rlbench.observation_config import ObservationConfig, CameraConfig\n",
    "# from rlbench.tasks.pick_described_object import PickDescribedObject\n",
    "from rlbench.tasks import PutGroceriesInCupboard, PickAndLift, StackBlocks, PlaceHangerOnRack, PickDescribedObject, TakeLidOffSaucepan, SetTheTable, PutGroceriesInCupboard\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "from pyrep.const import RenderMode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_path = \"/media/lawrence/Work/checkpoints/ecot-openvla-7b-bridge\"\n",
    "adapter_path = \"adapter-tmp/0+weighted+pick_described_object+e1+b8+lr-0.0005+lora-r16+dropout-0.0+q-4bit\"\n",
    "adapter_path1 = \"adapter-tmp/weighted_loss_cot_1+nll+pick_described_object2+e1+b8+lr-0.0001+lora-r16+dropout-0.0+q-4bit\"\n",
    "# save_path = \"/media/lawrence/Work/checkpoints/vla-rl-ecot\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_statistics: tuple = (np.array([-0.2, -0.35,  0.75199986, -np.pi/2, -np.pi/2, -np.pi/2,  0. ]), np.array([0.5, 0.35, 1.3, np.pi/2, 0, np.pi/2, 1.]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = AutoProcessor.from_pretrained(base_model_path, trust_remote_code=True)\n",
    "add_tokens = ['<g>', '</g>'] + [f'<item_{i}>' for i in np.arange(5)] + ['<o>', '</o>', '<t>', '</t>'] + [f'<stage_{i}>' for i in np.arange(2)] + ['<a>', '</a>']\n",
    "processor.tokenizer.add_tokens(add_tokens)\n",
    "action_tokenizer = RLbenchPoseTokenizer(processor.tokenizer,dataset_statistics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:35<00:00, 11.71s/it]\n"
     ]
    }
   ],
   "source": [
    "quantization_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True, bnb_4bit_compute_dtype=torch.bfloat16, bnb_4bit_quant_type=\"nf4\", #llm_int8_skip_modules = ['projector'],\n",
    "        )\n",
    "vla = AutoModelForVision2Seq.from_pretrained(\n",
    "        base_model_path,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        attn_implementation=\"sdpa\",\n",
    "        quantization_config=quantization_config,\n",
    "        low_cpu_mem_usage=True,\n",
    "        trust_remote_code=True,\n",
    "        device_map = \"cuda\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vla.load_adapter(adapter_path,adapter_name = \"adapter\")\n",
    "vla.load_adapter(adapter_path1,adapter_name = \"adapter1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vla.set_adapter(\"adapter\")\n",
    "vla.enable_adapters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera = CameraConfig(image_size=(224, 224), depth=False, point_cloud=False, mask=False)\n",
    "obs_config = ObservationConfig(left_shoulder_camera=camera, right_shoulder_camera=camera, front_camera=camera, overhead_camera=camera)\n",
    "obs_config.front_camera.render_mode = RenderMode.OPENGL\n",
    "\n",
    "env = Environment(\n",
    "    action_mode=MoveArmThenGripper(\n",
    "        arm_action_mode=EndEffectorPoseViaPlanning(absolute_mode=True, collision_checking=False), gripper_action_mode=Discrete()),\n",
    "    obs_config=obs_config,\n",
    "    headless=False)\n",
    "env.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = env.get_task(PickDescribedObject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(task, variation_num):\n",
    "    obs = task._scene.get_observation()\n",
    "    img = Image.fromarray(obs.front_rgb,'RGB')\n",
    "    gripper_pose = obs.gripper_pose\n",
    "    gripper_open = obs.gripper_open\n",
    "    object_pos = task._task.get_graspable_objects()[variation_num].get_position()\n",
    "    target_pos = task._task.dropin_box.get_position()\n",
    "    return img, gripper_pose, gripper_open, object_pos, target_pos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_instruct_prompt(gripper, instruction: str):\n",
    "    prompt = f\"\"\"In: What the next key pose of gripper should the robot take to {instruction}? Current pose is <g>{gripper} </g>, let's think step by step.\n",
    "    Out: \"\"\"        \n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not find a valid joint configuration for desired end effector pose.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['put the chocolate jello powder in the basket',\n",
       " 'pick up the chocolate jello powder and place it in the basket',\n",
       " 'store the chocolate jello powder in the basket',\n",
       " 'move the chocolate jello powder to the basket',\n",
       " 'place the chocolate jello powder in the basket']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# task = env.get_task(PickDescribedObject)\n",
    "descriptions, obs = task.reset()\n",
    "img = Image.fromarray(obs.front_rgb)\n",
    "\n",
    "descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> In: What the next key pose of gripper should the robot take to pick up the chocolate jello and place in the basket? Let's think step by step. \n",
      "    Out:<g>編민克ỹΈശ给</g>,<item_0>,<o>任민関</o>,<t>交ペམ</t>,<stage_0>,<a>任민共면ḳ彦弘</a></s>\n",
      "<s> In: What the next key pose of gripper should the robot take to pick up the chocolate jello and place in the basket? Let's think step by step. \n",
      "    Out:<g>編민克ỹΈശ给</g>,<item_0>,<o>任민関</o>,<t>交ペམ</t>,<stage_0>,<a>任민共면ḳ彦弘</a></s>\n"
     ]
    }
   ],
   "source": [
    "task = env.get_task(PickDescribedObject)\n",
    "descriptions, obs = task.reset()\n",
    "instr = descriptions[1]\n",
    "prompt = get_instruct_prompt(instr)\n",
    "image = Image.fromarray(obs.front_rgb)\n",
    "inputs = processor(prompt, image).to(vla.device, dtype=torch.bfloat16)\n",
    "while True:\n",
    "    output_dict = vla.generate(**inputs, max_new_tokens = 50, output_scores = True, return_dict_in_generate=True, do_sample = False)\n",
    "    gripper_mask, item_mask, object_mask, target_mask, stage_mask, action_mask = action_tokenizer.get_mask(output_dict.sequences)\n",
    "    print(processor.tokenizer.decode(output_dict.sequences[0]))\n",
    "    if action_mask.sum().item() != 7:\n",
    "        print(\"Action mask is not correct\")\n",
    "        continue\n",
    "    break\n",
    "print(processor.tokenizer.decode(output_dict.sequences[0]))\n",
    "output_logits = torch.stack(output_dict.scores, dim = 1)\n",
    "action_logits = output_logits[action_mask[:,-output_logits.size(1):]][:,action_tokenizer.action_token_begin_idx:processor.tokenizer.vocab_size].view(1,-1,action_tokenizer.n_bins)\n",
    "action = action_tokenizer.get_action(action_logits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "    def __init__(self, vla, processor, action_tokenizer):\n",
    "        self.vla = vla\n",
    "        self.processor = processor\n",
    "        self.action_tokenizer = action_tokenizer\n",
    "\n",
    "    def get_openvla_prompt(self, instruction: str):\n",
    "        SYSTEM_PROMPT = \"You are an assistant helping to control a robotic manipulator. The robot performs tasks by following a series of steps to interact with objects in its environment. The environment includes items like soup cans and baskets, and the robot uses a gripper to pick up and move these items.\\n\\nInstructions format:\\n- 'USER': Describes the task to be performed.\\n- 'ASSISTANT': Provides a detailed step-by-step plan for the robot to execute the task.\\n\\nThe 'ASSISTANT' response includes:\\n1. A logical step-by-step plan for the task.\\n2. The current positions of relevant objects and the gripper.\\n3. The current state of the gripper (whether it has grasped the object or not).\\n4. The next key pose of the gripper to achieve the task.\\n\\nExample:\\n\\nUSER: What action should the robot take to pick up the soup and place it in the basket?\\nASSISTANT: Let's think step by step. The plan is to move the gripper to the soup and pick it up, then move over the basket, and then place the soup in the basket. The soup is located at <object>ĉ‖호 </object>. The basket is located at <target>Ζ‖ご </target>. The gripper pose is <gripper>阳‖素군雅导弘 </gripper>. The gripper hasn't grasped the soup. So the current step is to move the gripper to the soup and pick it up. The next key pose of the gripper is <action>机‖素秀麻방弘 </action>. \\n <current conversation>\"\n",
    "        return f\"{SYSTEM_PROMPT} USER: What action should the robot take to {instruction.lower()}? ASSISTANT: Let's think step by step,\"\n",
    "\n",
    "    def get_instruct_prompt(self, instruction: str):\n",
    "        prompt = f\"\"\"In: What the next key pose of gripper should the robot take to {instruction}? Let's think step by step. \n",
    "        Out: <g>\"\"\"        \n",
    "        return prompt\n",
    "\n",
    "    def act(self, obs, instr):\n",
    "        prompt = self.get_instruct_prompt(instr)\n",
    "        image = Image.fromarray(obs.front_rgb)\n",
    "        inputs = self.processor(prompt, image).to(self.vla.device, dtype=torch.bfloat16)\n",
    "        while True:\n",
    "            output_dict = vla.generate(**inputs, max_new_tokens = 50, output_scores = True, return_dict_in_generate=True, do_sample = True, temperature = 0.5)\n",
    "            gripper_mask, item_mask, object_mask, target_mask, stage_mask, action_mask = action_tokenizer.get_mask(output_dict.sequences)\n",
    "            if action_mask.sum().item() != 7:\n",
    "                print(\"Action mask is not correct\")\n",
    "                continue\n",
    "            break\n",
    "        print(processor.tokenizer.decode(output_dict.sequences[0]))\n",
    "        output_logits = torch.stack(output_dict.scores, dim = 1)\n",
    "        action_logits = output_logits[action_mask[:,-output_logits.size(1):]][:,action_tokenizer.action_token_begin_idx:processor.tokenizer.vocab_size].view(1,-1,action_tokenizer.n_bins)\n",
    "        action = action_tokenizer.get_action(action_logits)\n",
    "\n",
    "        return action\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(vla, processor, action_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomize_pose(task):\n",
    "    np.random.seed()\n",
    "    while True:\n",
    "        try:\n",
    "            pos_lower_bound = np.array([-0.2, -0.35, task._scene._workspace_minz])\n",
    "            pos_upper_bound = np.array([task._scene._workspace_maxx, 0.35, 1.3])\n",
    "            rot_lower_bound = np.array([-np.pi/2, 0, -np.pi/2])\n",
    "            rot_upper_bound = np.array([np.pi/2, np.pi/2, np.pi/2])\n",
    "            pos = np.random.uniform(pos_lower_bound, pos_upper_bound)\n",
    "            euler = np.random.uniform(rot_lower_bound, rot_upper_bound)\n",
    "            euler[0] = np.clip(np.random.normal(0, np.pi/6),-np.pi/2,np.pi/2)\n",
    "            euler[1] = -np.clip(abs(np.random.normal(0,np.pi/6)),0, np.pi/2)\n",
    "            trans = lambda rx: rx - np.pi if rx > 0 else rx + np.pi \n",
    "            euler[0] = trans(euler[0])\n",
    "            quat = R.from_euler('xyz', euler).as_quat()\n",
    "            joint_position = task._scene.robot.arm.solve_ik_via_sampling(position=pos, euler=euler,trials=10)\n",
    "            task.step(np.concatenate([pos, quat, [1]]))\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            continue\n",
    "    return task._scene.get_observation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = env.get_task(PickDescribedObject)\n",
    "task.set_variation(2)\n",
    "training_steps = 1000\n",
    "episode_length = 100\n",
    "for i in range(training_steps):\n",
    "    if i % episode_length == 0:\n",
    "        print('Reset Episode')\n",
    "        descriptions, obs = task.reset()\n",
    "        obs = randomize_pose(task)\n",
    "        print(descriptions[1])   \n",
    "    plt.imshow(obs.front_rgb)     \n",
    "    plt.show()    \n",
    "    action = agent.act(obs,descriptions[1])\n",
    "    action_rotation = R.from_euler('xyz', action[3:6])\n",
    "    action_quaternion = action_rotation.as_quat()\n",
    "    # print(delta_quaternion)  # returns (qx, qy, qz, qw)\n",
    "    action = np.concatenate([action[0:3], action_quaternion, action[-1:]])\n",
    "    print(action)\n",
    "    obs, reward, terminate = task.step(action)\n",
    "    print(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reset Episode\n",
      "pick up the chocolate jello and place in the basket\n",
      "Could not find a valid joint configuration for desired end effector pose.\n",
      "Reset Episode\n",
      "pick up the chocolate jello and place in the basket\n",
      "Could not find a valid joint configuration for desired end effector pose.\n",
      "Reset Episode\n",
      "pick up the chocolate jello and place in the basket\n",
      "Could not find a valid joint configuration for desired end effector pose.\n",
      "Reset Episode\n",
      "pick up the chocolate jello and place in the basket\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lawrence/anaconda3/envs/VLA-RL/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/lawrence/anaconda3/envs/VLA-RL/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:509: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> In: You are an assistant helping to control a robotic manipulator. The robot performs tasks by following a series of steps to interact with objects in its environment. What the next key pose of gripper should the robot take to pick up the chocolate jello and place in the basket? Out: Let's think step by step, First, move the gripper to the chocolate jello and pick it up. Next, move over the basket, and finally, place the chocolate jello in the basket. The chocolate jello is located at <object>Ġ瀬합ữrivialŸ</s>\n",
      "Expected `angles` to be at most 2-dimensional with width equal to number of axes specified, got (0,) for shape\n",
      "Reset Episode\n",
      "pick up the chocolate jello and place in the basket\n",
      "<s> In: You are an assistant helping to control a robotic manipulator. The robot performs tasks by following a series of steps to interact with objects in its environment. What the next key pose of gripper should the robot take to pick up the chocolate jello and place in the basket? Out: Let's think step by step, First, move the gripper to the chocolate jello and pick it up. Next, move over the basket, and finally, place the chocolate jello in the basket. The chocolate jello is located at <object>Ġ唐háZygote터忠</s>\n",
      "Expected `angles` to be at most 2-dimensional with width equal to number of axes specified, got (0,) for shape\n",
      "Reset Episode\n",
      "pick up the chocolate jello and place in the basket\n",
      "Could not find a valid joint configuration for desired end effector pose.\n",
      "Reset Episode\n",
      "pick up the chocolate jello and place in the basket\n",
      "<s> In: You are an assistant helping to control a robotic manipulator. The robot performs tasks by following a series of steps to interact with objects in its environment. What the next key pose of gripper should the robot take to pick up the chocolate jello and place in the basket? Out: Let's think step by step, First, move the gripper to the chocolate jello and pick it up. Next, move over the basket, and finally, place the chocolate jello in the basket. The chocolate jello is located at <object>Ġ居회编żyn忠</s>\n",
      "Expected `angles` to be at most 2-dimensional with width equal to number of axes specified, got (0,) for shape\n",
      "Reset Episode\n",
      "pick up the chocolate jello and place in the basket\n",
      "<s> In: You are an assistant helping to control a robotic manipulator. The robot performs tasks by following a series of steps to interact with objects in its environment. What the next key pose of gripper should the robot take to pick up the chocolate jello and place in the basket? Out: Let's think step by step, First, move the gripper to the chocolate jello and pick it up. Next, move over the basket, and finally, place the chocolate jello in the basket. The chocolate jello is located at <object>泰败嘉ữoba忠</s>\n",
      "Expected `angles` to be at most 2-dimensional with width equal to number of axes specified, got (0,) for shape\n",
      "Reset Episode\n",
      "pick up the chocolate jello and place in the basket\n",
      "<s> In: You are an assistant helping to control a robotic manipulator. The robot performs tasks by following a series of steps to interact with objects in its environment. What the next key pose of gripper should the robot take to pick up the chocolate jello and place in the basket? Out: Let's think step by step, First, move the gripper to the chocolate jello and pick it up. Next, move over the basket, and finally, place the chocolate jello in the basket. The chocolate jello is located at <object>ordnetȚഞữ position, and the basket is at <target>Ζ away from the jello. The gripper is at aggio, and its orientation is  CLOSE_GRIPPER. The basket is at weitory from the jello, but the chocolate jello needs to be picked up before the basket can be reached. The gripper needs to be fully closed around the chocolate jello before the gripper can pick it up. The chocolate jello is at  Bittezendecuit銀❯忠</s>\n",
      "Expected `angles` to be at most 2-dimensional with width equal to number of axes specified, got (0,) for shape\n",
      "Reset Episode\n",
      "pick up the chocolate jello and place in the basket\n",
      "<s> In: You are an assistant helping to control a robotic manipulator. The robot performs tasks by following a series of steps to interact with objects in its environment. What the next key pose of gripper should the robot take to pick up the chocolate jello and place in the basket? Out: Let's think step by step, First, move the gripper to the chocolate jello and pick it up. Next, move over the basket, and finally, place the chocolate jello in the basket. The chocolate jello is located at <object>Ġ才ữữb忠</s>\n",
      "Expected `angles` to be at most 2-dimensional with width equal to number of axes specified, got (0,) for shape\n",
      "Reset Episode\n",
      "pick up the chocolate jello and place in the basket\n",
      "Could not find a valid joint configuration for desired end effector pose.\n",
      "Reset Episode\n",
      "pick up the chocolate jello and place in the basket\n",
      "Could not find a valid joint configuration for desired end effector pose.\n",
      "Reset Episode\n",
      "pick up the chocolate jello and place in the basket\n",
      "Could not find a valid joint configuration for desired end effector pose.\n",
      "Reset Episode\n",
      "pick up the chocolate jello and place in the basket\n",
      "<s> In: You are an assistant helping to control a robotic manipulator. The robot performs tasks by following a series of steps to interact with objects in its environment. What the next key pose of gripper should the robot take to pick up the chocolate jello and place in the basket? Out: Let's think step by step, First, move the gripper to the chocolate jello and pick it up. Next, move over the basket, and finally, place the chocolate jello in the basket. The chocolate jello is located at <object>Ġ居회్lop忠</s>\n",
      "Expected `angles` to be at most 2-dimensional with width equal to number of axes specified, got (0,) for shape\n",
      "Reset Episode\n",
      "pick up the chocolate jello and place in the basket\n",
      "Could not find a valid joint configuration for desired end effector pose.\n",
      "Reset Episode\n",
      "pick up the chocolate jello and place in the basket\n",
      "Could not find a valid joint configuration for desired end effector pose.\n",
      "Reset Episode\n",
      "pick up the chocolate jello and place in the basket\n",
      "<s> In: You are an assistant helping to control a robotic manipulator. The robot performs tasks by following a series of steps to interact with objects in its environment. What the next key pose of gripper should the robot take to pick up the chocolate jello and place in the basket? Out: Let's think step by step, First, move the gripper to the chocolate jello and pick it up. Next, move over the basket, and finally, place the chocolate jello in the basket. The chocolate jello is located at <object>Ġ瀬합ữALSE忠</s>\n",
      "Expected `angles` to be at most 2-dimensional with width equal to number of axes specified, got (0,) for shape\n",
      "Reset Episode\n",
      "pick up the chocolate jello and place in the basket\n",
      "<s> In: You are an assistant helping to control a robotic manipulator. The robot performs tasks by following a series of steps to interact with objects in its environment. What the next key pose of gripper should the robot take to pick up the chocolate jello and place in the basket? Out: Let's think step by step, First, move the gripper to the chocolate jello and pick it up. Next, move over the basket, and finally, place the chocolate jello in the basket. The chocolate jello is located at <object>zin�reten</object>, the basket is at <target>Ζirtualized by the manipulator arm. The gripper is at <gripper>NLSP坂ರ拳忠</s>\n",
      "Expected `angles` to be at most 2-dimensional with width equal to number of axes specified, got (0,) for shape\n",
      "Reset Episode\n",
      "pick up the chocolate jello and place in the basket\n",
      "Could not find a valid joint configuration for desired end effector pose.\n",
      "Reset Episode\n",
      "pick up the chocolate jello and place in the basket\n",
      "Could not find a valid joint configuration for desired end effector pose.\n",
      "Reset Episode\n",
      "pick up the chocolate jello and place in the basket\n",
      "Could not find a valid joint configuration for desired end effector pose.\n",
      "Reset Episode\n",
      "pick up the chocolate jello and place in the basket\n",
      "Could not find a valid joint configuration for desired end effector pose.\n",
      "Reset Episode\n",
      "pick up the chocolate jello and place in the basket\n",
      "Could not find a valid joint configuration for desired end effector pose.\n",
      "Reset Episode\n",
      "pick up the chocolate jello and place in the basket\n",
      "<s> In: You are an assistant helping to control a robotic manipulator. The robot performs tasks by following a series of steps to interact with objects in its environment. What the next key pose of gripper should the robot take to pick up the chocolate jello and place in the basket? Out: Let's think step by step, Move the gripper to the chocolate jello and place in the basket. The chocolate is on the table, the basket is also on the table, and the arm is above the chocolate. The first step is moving the arm right, to move the gripper above the chocolate. The next key pose of the gripper needs to be  preparation for grasping the chocolate. Hence, the first step is moving the arm right. and then the arm up, because the chocolate is on the table. The next step is moving the arm down, because the chocolate is on the table. The next step is moving the arm down, because the chocolate is on the table. The next step is moving the arm down, because the chocolate is on the table. The next step is moving the arm down. The next step is moving the arm down. The next step is moving the arm down.\n",
      "Expected `angles` to be at most 2-dimensional with width equal to number of axes specified, got (0,) for shape\n",
      "Reset Episode\n",
      "pick up the chocolate jello and place in the basket\n",
      "<s> In: You are an assistant helping to control a robotic manipulator. The robot performs tasks by following a series of steps to interact with objects in its environment. What the next key pose of gripper should the robot take to pick up the chocolate jello and place in the basket? Out: Let's think step by step, First, move the gripper to the chocolate jello and pick it up. Next, move over the basket, and finally, place the chocolate jello in the basket. The chocolate jello is located at <object>zin4, 171, 199, 234] ACTION: 巴孝微么衛Ÿ忠</s>\n",
      "Expected `angles` to be at most 2-dimensional with width equal to number of axes specified, got (0,) for shape\n",
      "Reset Episode\n",
      "pick up the chocolate jello and place in the basket\n"
     ]
    }
   ],
   "source": [
    "task = env.get_task(PickDescribedObject)\n",
    "lower_bound = np.array([task._scene._workspace_minx, task._scene._workspace_miny, task._scene._workspace_minz])\n",
    "upper_bound = np.array([task._scene._workspace_maxx, task._scene._workspace_maxy, task._scene._workspace_maxz])\n",
    "training_steps = 1000\n",
    "episode_length = 100\n",
    "obs = None\n",
    "while 1:\n",
    "    try:\n",
    "        for i in range(training_steps):\n",
    "            if i % episode_length == 0:\n",
    "                print('Reset Episode')\n",
    "                descriptions, obs = task.reset()\n",
    "                plt.imshow(obs.front_rgb)\n",
    "                print(descriptions[1])            \n",
    "                pos = np.random.uniform(lower_bound, upper_bound)\n",
    "                rot = np.random.uniform(-np.pi, np.pi, 3)\n",
    "                joint_position = task._scene.robot.arm.solve_ik_via_sampling(position=pos, euler=rot)\n",
    "                task._scene.robot.arm.set_joint_positions(joint_position[0], disable_dynamics=True)\n",
    "            action = agent.act(obs,descriptions[1])\n",
    "            action_rotation = R.from_euler('xyz', action[3:6])\n",
    "            action_quaternion = action_rotation.as_quat()\n",
    "            # print(delta_quaternion)  # returns (qx, qy, qz, qw)\n",
    "            action = np.concatenate([action[0:3], action_quaternion, action[-1:]])\n",
    "            print(action)\n",
    "            obs, reward, terminate = task.step(action)\n",
    "            print(reward)\n",
    "    except Exception as e:\n",
    "        print(e)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VLA-RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
