{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lawrence/anaconda3/envs/VLA-RL/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "sys.path.append('.')\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "import draccus\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import tqdm\n",
    "import wandb\n",
    "from accelerate import PartialState\n",
    "from peft import LoraConfig, PeftModel, get_peft_model, prepare_model_for_kbit_training\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModelForVision2Seq, AutoProcessor, BitsAndBytesConfig\n",
    "from transformers.modeling_outputs import CausalLMOutputWithPast\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "from vla.base_prompter import PurePromptBuilder\n",
    "from vla.utils import PaddedCollatorForPosePrediction, runningLoss\n",
    "from vla.action_tokenizer import RLbenchPoseTokenizer\n",
    "from vla.dataset import RLbenchCotDataset\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"/media/lawrence/Work/checkpoints/vla-rl-ecot\"\n",
    "base_model_path = \"/media/lawrence/Work/checkpoints/ecot-openvla-7b-bridge\"\n",
    "adapter_path = \"adapter-tmp/nll_loss_cot+pick_described_object+e5+b8+lr-5e-05+lora-r16+dropout-0.0+q-4bit\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_statistics: tuple = (np.array([-0.20173775, -0.36754665,  0.81396234, -3.14153998, -0.38798628, -3.14158631,  0. ]), np.array([0.41802976, 0.45118147, 1.47966564, 3.14159215, 0.30391057, 3.14157801, 1.])) # Min-Max normalization statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "processor = AutoProcessor.from_pretrained(base_model_path, trust_remote_code=True)\n",
    "action_tokenizer = RLbenchPoseTokenizer(processor.tokenizer,dataset_statistics)\n",
    "test_data_path: Path = Path(f\"./datasets/pick_described_object/test_data1.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:12<00:00,  4.27s/it]\n"
     ]
    }
   ],
   "source": [
    "quantization_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True, bnb_4bit_compute_dtype=torch.bfloat16, bnb_4bit_quant_type=\"nf4\", #llm_int8_skip_modules = ['projector'],\n",
    "        )\n",
    "vla = AutoModelForVision2Seq.from_pretrained(\n",
    "        base_model_path,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        attn_implementation=\"sdpa\",\n",
    "        quantization_config=quantization_config,\n",
    "        low_cpu_mem_usage=True,\n",
    "        trust_remote_code=True,\n",
    "        device_map = \"cuda\"\n",
    "    )\n",
    "vla.load_adapter(adapter_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset = RLbenchCotDataset(\n",
    "    test_data_path,\n",
    "    action_tokenizer,\n",
    "    processor.tokenizer,\n",
    "    image_transform=processor.image_processor.apply_transform,\n",
    "    prompt_builder_fn=PurePromptBuilder,\n",
    ")\n",
    "collator = PaddedCollatorForPosePrediction(\n",
    "    processor.tokenizer.model_max_length, processor.tokenizer.pad_token_id, padding_side=\"right\"\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    testset,\n",
    "    batch_size=2,\n",
    "    sampler=None,\n",
    "    collate_fn=collator,\n",
    "    num_workers=1,  # Important =>> Set to 0 if using RLDS; TFDS rolls its own parallelism!\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device_id = vla.device.type\n",
    "test_nll_loss = []\n",
    "vla.eval()\n",
    "for test_idx, batch in enumerate(test_dataloader):\n",
    "    with torch.no_grad(), torch.autocast(\"cuda\", dtype=torch.bfloat16):\n",
    "        output: CausalLMOutputWithPast = vla(\n",
    "            input_ids=batch[\"input_ids\"].to(device_id),\n",
    "            attention_mask=batch[\"attention_mask\"].to(device_id),\n",
    "            pixel_values=batch[\"pixel_values\"].to(torch.bfloat16).to(device_id),\n",
    "            labels=batch[\"labels\"],\n",
    "        )\n",
    "        test_nll_loss_ = output.loss\n",
    "        test_nll_loss.append(test_nll_loss_)\n",
    "    if test_idx == 30:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(0.7865, device='cuda:0'),\n",
       " tensor(0.8690, device='cuda:0'),\n",
       " tensor(0.7530, device='cuda:0'),\n",
       " tensor(0.6961, device='cuda:0'),\n",
       " tensor(0.3100, device='cuda:0'),\n",
       " tensor(0.2857, device='cuda:0'),\n",
       " tensor(0.3724, device='cuda:0'),\n",
       " tensor(0.3880, device='cuda:0'),\n",
       " tensor(0.3730, device='cuda:0'),\n",
       " tensor(0.2115, device='cuda:0'),\n",
       " tensor(0.6261, device='cuda:0'),\n",
       " tensor(0.6178, device='cuda:0'),\n",
       " tensor(0.6179, device='cuda:0'),\n",
       " tensor(0.6286, device='cuda:0'),\n",
       " tensor(0.4834, device='cuda:0'),\n",
       " tensor(0.3462, device='cuda:0'),\n",
       " tensor(0.3569, device='cuda:0'),\n",
       " tensor(0.3540, device='cuda:0'),\n",
       " tensor(0.2164, device='cuda:0'),\n",
       " tensor(0.2830, device='cuda:0'),\n",
       " tensor(0.5385, device='cuda:0'),\n",
       " tensor(0.5433, device='cuda:0'),\n",
       " tensor(0.4642, device='cuda:0'),\n",
       " tensor(0.4272, device='cuda:0'),\n",
       " tensor(0.3361, device='cuda:0'),\n",
       " tensor(0.2896, device='cuda:0'),\n",
       " tensor(0.3263, device='cuda:0'),\n",
       " tensor(0.3334, device='cuda:0'),\n",
       " tensor(0.0704, device='cuda:0'),\n",
       " tensor(0.5370, device='cuda:0'),\n",
       " tensor(0.5520, device='cuda:0')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_nll_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VLA-RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
