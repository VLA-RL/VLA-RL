{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lawrence/anaconda3/envs/VLA-RL/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import tqdm\n",
    "import wandb\n",
    "from accelerate import PartialState\n",
    "from peft import LoraConfig, PeftModel, get_peft_model, prepare_model_for_kbit_training, PeftConfig\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import transformers\n",
    "from transformers import AutoModel\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModelForVision2Seq, AutoProcessor, BitsAndBytesConfig\n",
    "from transformers.modeling_outputs import CausalLMOutputWithPast\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "from vla.base_prompter import PurePromptBuilder\n",
    "from vla.utils import PaddedCollatorForPosePrediction, runningLoss\n",
    "from vla.action_tokenizer import RLbenchPoseTokenizer\n",
    "from vla.dataset import RLbenchCotDataset\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from typing import Callable, Dict, Sequence, Tuple\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "from rlbench.action_modes.action_mode import MoveArmThenGripper\n",
    "from rlbench.action_modes.arm_action_modes import ArmActionMode, JointVelocity, JointPosition, EndEffectorPoseViaPlanning, EndEffectorPoseViaIK\n",
    "\n",
    "\n",
    "from rlbench.action_modes.gripper_action_modes import Discrete\n",
    "from rlbench.environment import Environment\n",
    "from rlbench.observation_config import ObservationConfig, CameraConfig\n",
    "# from rlbench.tasks.pick_described_object import PickDescribedObject\n",
    "from rlbench.tasks import PutGroceriesInCupboard, PickAndLift, StackBlocks, PlaceHangerOnRack, PickDescribedObject, TakeLidOffSaucepan, SetTheTable, PutGroceriesInCupboard\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "from pyrep.const import RenderMode\n",
    "\n",
    "from torch.utils.data import DataLoader, random_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCriticModel(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, dropout_rate, device='cuda'):\n",
    "        super(ActorCriticModel, self).__init__()\n",
    "        self.device = device\n",
    "\n",
    "        # Define the Actor as a nested class\n",
    "        class Actor(nn.Module):\n",
    "            def __init__(self):\n",
    "                super(Actor, self).__init__()\n",
    "                self.net = nn.Sequential(\n",
    "                    nn.Linear(state_dim, 4096),\n",
    "                    nn.Linear(4096, 4096),\n",
    "                    nn.LeakyReLU(),\n",
    "                    nn.Dropout(dropout_rate),\n",
    "                    nn.Linear(4096, 2048),\n",
    "                    nn.LeakyReLU(),\n",
    "                    nn.Dropout(dropout_rate),\n",
    "                    nn.Linear(2048, 2048),\n",
    "                    nn.LeakyReLU(),\n",
    "                    nn.Dropout(dropout_rate),\n",
    "                    nn.Linear(2048, 1024),\n",
    "                    nn.LeakyReLU(),\n",
    "                    nn.Dropout(dropout_rate),\n",
    "                    nn.Linear(1024, action_dim),\n",
    "                )\n",
    "\n",
    "            def forward(self, state):\n",
    "                action_probs = self.net(state)\n",
    "                # last 2 logits for gripper open/close\n",
    "                pos_logits = action_probs[:, :300]\n",
    "                pos_logits = pos_logits.reshape(-1, 3, 100)\n",
    "                pos_logprob = F.log_softmax(pos_logits, dim=-1)\n",
    "                rot_logits = action_probs[:, 300:600]\n",
    "                rot_logits = rot_logits.reshape(-1, 3, 100)\n",
    "                rot_logprob = F.log_softmax(rot_logits, dim=-1)\n",
    "                open_logits = action_probs[:, 600:].reshape(-1, 1, 2)\n",
    "                open_logprob = F.log_softmax(open_logits, dim=-1)\n",
    "                return pos_logprob, rot_logprob, open_logprob\n",
    "\n",
    "        # Define the Critic as a nested class\n",
    "        class Critic(nn.Module):\n",
    "            def __init__(self):\n",
    "                super(Critic, self).__init__()\n",
    "                self.net = nn.Sequential(\n",
    "                    nn.Linear(state_dim, 4096),\n",
    "                    nn.Linear(4096, 2048),\n",
    "                    nn.LeakyReLU(),\n",
    "                    nn.Dropout(dropout_rate),\n",
    "                    nn.Linear(2048, 1024),\n",
    "                    nn.LeakyReLU(),\n",
    "                    nn.Dropout(dropout_rate),\n",
    "                    nn.Linear(1024, 512),\n",
    "                    nn.LeakyReLU(),\n",
    "                    nn.Dropout(dropout_rate),\n",
    "                    nn.Linear(512, 256),\n",
    "                    nn.LeakyReLU(),\n",
    "                    nn.Dropout(dropout_rate),\n",
    "                    nn.Linear(256, 1),\n",
    "                    # nn.Tanh()\n",
    "                )\n",
    "\n",
    "            def forward(self, state):\n",
    "                state_value = self.net(state)\n",
    "                return state_value\n",
    "\n",
    "        # Initialize actor and critic\n",
    "        self.actor = Actor()\n",
    "        self.critic = Critic()\n",
    "\n",
    "        self.init_weight()\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, state):\n",
    "        # Actor forward pass\n",
    "        pos_logprob, rot_logprob, open_logprob = self.actor(state)\n",
    "\n",
    "        # Critic forward pass\n",
    "        state_value = self.critic(state)\n",
    "\n",
    "        return pos_logprob, rot_logprob, open_logprob, state_value\n",
    "\n",
    "    def init_weight(self):\n",
    "        for layer in self.actor.net.children():\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                nn.init.xavier_normal_(layer.weight)\n",
    "                nn.init.constant_(layer.bias, 0)\n",
    "\n",
    "        for layer in self.critic.net.children():\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                nn.init.xavier_normal_(layer.weight)\n",
    "                nn.init.constant_(layer.bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"datasets/pick_described_object_replay1/data.pt\"\n",
    "data = torch.load(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['images', 'instructions', 'grippers', 'items', 'objects', 'targets', 'stages', 'actions', 'rewards', 'dones', 'next_images', 'next_grippers'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lawrence/anaconda3/envs/VLA-RL/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of ViTModel were not initialized from the model checkpoint at facebook/deit-tiny-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "bert_model = BertModel.from_pretrained(\"bert-base-uncased\").to(device)\n",
    "#  theaiinstitute/theia-base-patch16-224-cdiv\n",
    "# theaiinstitute/theia-tiny-patch16-224-cdiv\n",
    "image_encoder = AutoModel.from_pretrained(\"theaiinstitute/theia-tiny-patch16-224-cddsv\", trust_remote_code=True,).to(device)\n",
    "\n",
    "\n",
    "# Function to encode the descriptor using BERT\n",
    "def encode_descriptor(descriptor):\n",
    "    inputs = tokenizer(descriptor, return_tensors=\"pt\", padding=True, truncation=True, max_length=10)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = bert_model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()\n",
    "\n",
    "def encode_image(image):\n",
    "    assert image.shape == (224, 224, 3)\n",
    "    with torch.no_grad():\n",
    "        image_encoding = image_encoder.forward_feature(image)\n",
    "    return image_encoding.flatten().cpu().numpy()\n",
    "\n",
    "def pose_processor(pose):\n",
    "    #from quat to euler\n",
    "    pos = pose[:3]\n",
    "    pos_lower_bound = np.array([-0.2, -0.35, 0.752])\n",
    "    pos_upper_bound = np.array([0.5, 0.35, 1.3])\n",
    "    pos = np.clip(pos, pos_lower_bound, pos_upper_bound-1e-8)\n",
    "    pos_bins = np.linspace(pos_lower_bound, pos_upper_bound, 101)\n",
    "    x_idx = np.digitize(pos[0], pos_bins[:,0]) -1 \n",
    "    y_idx = np.digitize(pos[1], pos_bins[:,1]) -1\n",
    "    z_idx = np.digitize(pos[2], pos_bins[:,2]) -1\n",
    "    x = np.zeros(100)\n",
    "    x[x_idx] = 1\n",
    "    y = np.zeros(100)\n",
    "    y[y_idx] = 1\n",
    "    z = np.zeros(100)\n",
    "    z[z_idx] = 1\n",
    "    pos = np.concatenate([x,y,z])\n",
    "\n",
    "    euler = pose[3:6]\n",
    "    euler[0] = euler[0] + np.pi if euler[0] < 0 else euler[0] - np.pi\n",
    "    euler_lower_bound = np.array([-np.pi/4, -np.pi/4, -np.pi/2])\n",
    "    euler_upper_bound = np.array([np.pi/4, np.pi/4, np.pi/2])\n",
    "    euler = np.clip(euler, euler_lower_bound, euler_upper_bound-1e-8)\n",
    "    euler_bins = np.linspace(euler_lower_bound, euler_upper_bound, 101)\n",
    "    rx_idx = np.digitize(euler[0], euler_bins[:,0]) -1\n",
    "    ry_idx = np.digitize(euler[1], euler_bins[:,1]) -1\n",
    "    rz_idx = np.digitize(euler[2], euler_bins[:,2]) -1\n",
    "    rx = np.zeros(100)\n",
    "    rx[rx_idx] = 1\n",
    "    ry = np.zeros(100)\n",
    "    ry[ry_idx] = 1\n",
    "    rz = np.zeros(100)\n",
    "    rz[rz_idx] = 1\n",
    "    \n",
    "    rot = np.concatenate([rx,ry,rz])\n",
    "    open_state = np.zeros(2)\n",
    "    open_state[int(pose[-1])] = 1\n",
    "\n",
    "    return np.concatenate([pos, rot, open_state])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ACDataLoader(torch.utils.data.Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data['images'])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = np.array(Image.open(self.data['images'][idx]))\n",
    "        img_encoding = encode_image(img)\n",
    "        descriptor = self.data['instructions'][idx]\n",
    "        descriptor_encoding = encode_descriptor(descriptor)\n",
    "        pose = self.data['grippers'][idx]\n",
    "        pose_encoding = pose_processor(pose)\n",
    "        state = torch.tensor(np.concatenate([img_encoding, descriptor_encoding, pose_encoding]))\n",
    "        action = torch.tensor(self.data['actions'][idx])\n",
    "        return dict(states = state.float(), actions = action.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = ACDataLoader(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.9 * len(trainset))\n",
    "test_size = len(trainset) - train_size\n",
    "train_dataset, test_dataset = random_split(trainset, [train_size, test_size])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Collator:\n",
    "    def __call__(self, instances: Sequence[Dict[str, torch.Tensor]]) -> Dict[str, torch.Tensor]:\n",
    "        states, actions = tuple([instance[key] for instance in instances] for key in (\"states\", \"actions\"))\n",
    "        states = torch.stack(states)\n",
    "        actions = torch.stack(actions)\n",
    "        output = dict(\n",
    "            states = states,\n",
    "            actions = actions\n",
    "        )\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "collator = Collator()\n",
    "train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=16,\n",
    "        shuffle=False,\n",
    "        # sampler=sampler,\n",
    "        collate_fn=collator,\n",
    "        num_workers=0, \n",
    "    )\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=False,\n",
    "    # sampler=sampler,\n",
    "    collate_fn=collator,\n",
    "    num_workers=0, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_encoding_dim = 768  # BERT base model output dimension\n",
    "image_encoding_dim = 37632\n",
    "action_dim =  6*100 + 2 # 6 for endeffector pose + 1 for gripper\n",
    "\n",
    "state_dim = image_encoding_dim + action_dim + bert_encoding_dim\n",
    "dropout_rate = 0.2\n",
    "\n",
    "acmodel = ActorCriticModel(state_dim, action_dim, dropout_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-4\n",
    "weight_decay = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(acmodel.actor.parameters(), lr=learning_rate,weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.1510e-01, -2.2684e-01,  9.0183e-01,  3.1356e+00,  3.5455e-03,\n",
       "         -1.8505e+00,  0.0000e+00],\n",
       "        [-2.0000e-01, -3.2500e-01,  1.2000e+00,  3.1416e+00, -1.3101e-14,\n",
       "          3.1416e+00,  1.0000e+00],\n",
       "        [ 6.0139e-03,  2.0162e-02,  1.1981e+00, -2.8774e+00,  5.0894e-01,\n",
       "          1.1299e+00,  0.0000e+00],\n",
       "        [ 9.3510e-02,  4.2717e-02,  9.8968e-01,  2.9928e+00,  2.7490e-01,\n",
       "          1.3478e+00,  0.0000e+00],\n",
       "        [-2.0000e-01, -3.2500e-01,  1.2000e+00,  3.1416e+00, -1.3101e-14,\n",
       "          3.1416e+00,  1.0000e+00],\n",
       "        [ 2.5556e-01,  3.2610e-01,  8.2499e-01,  3.1416e+00,  2.6340e-05,\n",
       "          2.8547e+00,  0.0000e+00],\n",
       "        [-1.8882e-02,  2.3190e-01,  8.6805e-01, -2.8813e+00,  7.9399e-01,\n",
       "          2.8683e-01,  0.0000e+00],\n",
       "        [-2.0000e-01, -3.2500e-01,  1.2000e+00,  3.1416e+00, -1.3101e-14,\n",
       "          3.1416e+00,  1.0000e+00],\n",
       "        [-2.0126e-03,  3.3283e-01,  1.2427e+00,  2.6184e+00,  4.5629e-01,\n",
       "         -5.4830e-01,  0.0000e+00],\n",
       "        [-2.0000e-01, -3.2500e-01,  1.2000e+00,  3.1416e+00, -1.3101e-14,\n",
       "          3.1416e+00,  1.0000e+00],\n",
       "        [ 9.7494e-02,  1.7227e-01,  8.2499e-01,  3.1412e+00, -1.1721e-04,\n",
       "          1.6644e+00,  0.0000e+00],\n",
       "        [ 7.9346e-02,  2.9489e-01,  8.2500e-01, -3.1416e+00,  1.2783e-05,\n",
       "         -2.0605e+00,  0.0000e+00],\n",
       "        [ 3.9638e-01,  1.2313e-01,  1.2736e+00, -2.4395e+00,  3.3169e-01,\n",
       "         -6.7229e-01,  0.0000e+00],\n",
       "        [-1.5408e-01, -1.8605e-01,  1.1205e+00,  3.0379e+00,  1.0020e+00,\n",
       "          4.2665e-01,  1.0000e+00],\n",
       "        [ 4.7976e-03,  6.1867e-02,  9.3915e-01,  1.5708e+00,  3.1503e-01,\n",
       "         -9.7311e-01,  0.0000e+00],\n",
       "        [-2.0000e-01, -3.2500e-01,  1.2000e+00,  3.1416e+00, -1.3101e-14,\n",
       "          3.1416e+00,  1.0000e+00]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['actions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action_loss(pos_logprob,rot_logprob,open_logprob, action_gt):\n",
    "    pos_gt = action_gt[:,:3]\n",
    "    pos_lower_bound = np.array([-0.2, -0.35, 0.752])\n",
    "    pos_upper_bound = np.array([0.5, 0.35, 1.3])\n",
    "    pos_bins = np.linspace(pos_lower_bound, pos_upper_bound, 101)\n",
    "    pos_bin_centers = (pos_bins[:-1] + pos_bins[1:]) / 2\n",
    "    pos_pred = (pos_logprob.exp()@torch.tensor(pos_bin_centers).to(torch.float32).to(device))[:,range(3),range(3)]\n",
    "    assert pos_pred.shape == pos_gt.shape, f\"{pos_pred.shape} != {pos_gt.shape}\"\n",
    "    pos_loss = F.mse_loss(pos_pred,pos_gt.to(device))\n",
    "\n",
    "    euler_gt = action_gt[:,3:6]\n",
    "    torch.where(euler_gt[:,0:1]>0, euler_gt[:,0:1] - torch.pi, euler_gt[:,0:1] + torch.pi) \n",
    "    euler_lower_bound = np.array([-np.pi/4, -np.pi/4, -np.pi/2])\n",
    "    euler_upper_bound = np.array([np.pi/4, np.pi/4, np.pi/2])\n",
    "    euler_bins = np.linspace(euler_lower_bound, euler_upper_bound, 101)\n",
    "    euler_bin_centers = (euler_bins[:-1] + euler_bins[1:]) / 2\n",
    "    euler_pred = (rot_logprob.exp()@torch.tensor(euler_bin_centers).to(torch.float32).to(device))[:,range(3),range(3)]\n",
    "    euler_loss = F.mse_loss(euler_pred,euler_gt.to(device))\n",
    "\n",
    "    open_gt = F.one_hot(action_gt[:,6:7].to(torch.int64).view(-1), num_classes=2).to(device)\n",
    "    open_loss = F.cross_entropy(open_logprob.exp().squeeze(1).to(torch.float32),open_gt.to(device).to(torch.float32))\n",
    "    \n",
    "    return (pos_loss * 0.6 + euler_loss * 0.3 + open_loss * 0.1).to(torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "euler_lower_bound = np.array([-np.pi/4, -np.pi/4, -np.pi/2])\n",
    "euler_upper_bound = np.array([np.pi/4, np.pi/4, np.pi/2])\n",
    "euler_bins = np.linspace(euler_lower_bound, euler_upper_bound, 101)\n",
    "euler_bin_centers = (euler_bins[:-1] + euler_bins[1:]) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/99 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.2314, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "avg_loss 1.1630859375\n",
      "tensor(1.1963, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(1.2852, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(1.1416, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(1.3369, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(1.6396, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(0.9854, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(1.3174, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(1.2568, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(1.2686, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(1.3750, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(1.1191, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(1.1426, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(1.0791, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(1.2139, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(1.0977, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(1.3154, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(1.3418, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(0.9551, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(1.0352, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(1.2627, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "avg_loss 1.1630859375\n",
      "tensor(1.1816, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(1.2188, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(1.0898, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(1.0977, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(0.9814, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(1.2363, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(1.1572, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(1.0254, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(1.1504, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(1.0947, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(1.0693, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(0.9839, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(1.1777, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(1.0635, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(1.0850, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(1.3252, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(1.1494, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(1.1992, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(1.1211, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(1.2314, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "avg_loss 1.1630859375\n",
      "tensor(1.0244, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(1.0469, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(0.9487, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(1.2725, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(1.2012, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(1.2344, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(0.9834, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(1.3486, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(1.2344, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(1.2891, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(1.2451, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(1.0322, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(1.1396, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(1.2510, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(1.0869, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(1.1689, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(1.2500, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(1.2646, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(1.4004, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(1.1729, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "avg_loss 1.1630859375\n",
      "tensor(1.2529, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(1.1797, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(1.2168, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(1.0947, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(1.2783, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(1.3359, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(1.0391, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(1.2012, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(1.2295, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(1.1416, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(1.1328, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(1.1846, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(1.4854, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(1.0615, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(1.1416, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(1.2764, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(1.4141, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(0.9941, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(1.2822, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(0.8057, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "avg_loss 1.1630859375\n",
      "tensor(1.1533, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(1.0166, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(1.1113, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(1.1738, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(1.1641, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(1.3896, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(1.1807, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(1.0010, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(1.2285, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(1.1475, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(1.2051, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(1.2910, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(0.9976, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(1.1699, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(1.3486, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(1.0830, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/99 [00:50<1:22:25, 50.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.5107, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(1.2939, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(1.2314, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "avg_loss 1.162109375\n",
      "tensor(1.1963, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(1.2852, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(1.1416, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(1.3369, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(1.6396, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(0.9854, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(1.3174, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(1.2568, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(1.2686, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(1.3750, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(1.1191, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(1.1416, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(1.0791, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(1.2139, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(1.0977, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(1.3057, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(1.3418, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(0.9551, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(1.0361, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(1.2627, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "avg_loss 1.162109375\n",
      "tensor(1.1816, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(1.2188, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(1.0898, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(1.0957, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(0.9897, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(1.2344, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(1.1631, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(1.0254, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(1.1436, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(1.0947, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(1.0713, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(0.9810, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(1.1777, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(1.0635, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(1.0850, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(1.3252, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(1.1494, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(1.1992, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(1.1211, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(1.2314, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "avg_loss 1.1630859375\n",
      "tensor(1.0234, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(1.0469, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(0.9487, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(1.2725, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(1.2012, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(1.2324, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(0.9834, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(1.3486, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(1.2344, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(1.2891, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(1.2451, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(1.0322, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(1.1396, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(1.2510, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(1.0869, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(1.1689, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(1.2500, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(1.2646, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(1.4004, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(1.1729, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "avg_loss 1.162109375\n",
      "tensor(1.2529, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(1.1797, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(1.2168, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(1.0947, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(1.2783, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(1.3359, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(1.0391, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(1.2021, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(1.2295, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(1.1416, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n",
      "tensor(1.1328, device='cuda:0', dtype=torch.float16, grad_fn=<ToCopyBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# scaler = torch.cuda.amp.GradScaler()\n",
    "acmodel.actor.train()\n",
    "epochs = 5\n",
    "total_action_loss = []\n",
    "with tqdm.tqdm(total=epochs*train_dataloader.__len__() , leave=False) as progress:\n",
    "    for epoch in range(epochs):\n",
    "        for step_idx, batch in enumerate(train_dataloader):\n",
    "            # with torch.autocast(\"cuda\"):\n",
    "            inputs = batch['states'].to(torch.float32).to(device)\n",
    "            outputs = acmodel.actor(inputs)\n",
    "            pos_logprob,rot_logprob,open_logprob = outputs\n",
    "            action_loss =get_action_loss(pos_logprob,rot_logprob,open_logprob, batch['actions'])\n",
    "            optimizer.zero_grad()\n",
    "            action_loss.backward()\n",
    "            optimizer.step()\n",
    "            print(action_loss)\n",
    "            if step_idx % 20 == 0:\n",
    "                action_losses = []\n",
    "                for batch in test_dataloader:\n",
    "                    with torch.no_grad():\n",
    "                        inputs = batch['states'].to(torch.float32).to(device)\n",
    "                        outputs = acmodel.actor(inputs)\n",
    "                        pos_logprob,rot_logprob,open_logprob = outputs\n",
    "                        action_loss =get_action_loss(pos_logprob,rot_logprob,open_logprob, batch['actions'])\n",
    "                        action_losses.append(action_loss.cpu().numpy())\n",
    "                avg_loss = np.mean(action_losses)\n",
    "                total_action_loss.append(avg_loss)\n",
    "                print(f\"avg_loss {avg_loss}\")\n",
    "        progress.update()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_logprob.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VLA-RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
